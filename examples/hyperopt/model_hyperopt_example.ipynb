{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimization with Hyperopt\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Hyperopt package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hyperopt in /usr/local/lib/python3.6/dist-packages (0.2.2)\n",
      "Requirement already satisfied: networkx==2.2 in /usr/local/lib/python3.6/dist-packages (from hyperopt) (2.2)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from hyperopt) (0.18.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from hyperopt) (1.3.1)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from hyperopt) (4.36.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from hyperopt) (1.17.3)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from hyperopt) (1.11.0)\n",
      "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.6/dist-packages (from hyperopt) (1.2.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx==2.2->hyperopt) (4.4.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install hyperopt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "from ludwig.api import LudwigModel\n",
    "from ludwig.utils.data_utils import load_json\n",
    "from ludwig.visualize import learning_curves\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from hyperopt.pyll.stochastic import rng_from_seed\n",
    "import logging\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean out old results\n",
    "try:\n",
    "    shutil.rmtree('./results')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    shutil.rmtree('./visualizations')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    file_list = glob.glob('./data/*.json')\n",
    "    file_list.append(glob.glob('./data/*.hdf5'))\n",
    "    os.remove(file_list)\n",
    "except:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(891, 12)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_df = pd.read_csv('./data/train.csv')\n",
    "raw_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, vald_df = train_test_split(raw_df, test_size = 0.2, random_state=13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define objective function for minimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_model(params):\n",
    "    \n",
    "    print(params)\n",
    "    \n",
    "    model_definition['training']['learning_rate'] = params['learning_rate']\n",
    "    model_definition['training']['batch_size'] = params['batch_size']\n",
    "    model_definition['output_features'][0]['num_fc_layers'] = np.int(params['output_fc_num_layers'])\n",
    "    model_definition['output_features'][0]['fc_size'] = np.int(params['output_fc_size'])\n",
    "    \n",
    "    # Define Ludwig model object that drive model training\n",
    "    model = LudwigModel(model_definition,\n",
    "                        logging_level=logging.WARN)\n",
    "\n",
    "    # initiate model training\n",
    "    train_stats = model.train(data_train_df = train_df,\n",
    "                            data_validation_df = vald_df,\n",
    "                            experiment_name='experiment_name',\n",
    "                            model_name='model_name',\n",
    "                            skip_save_training_description=True, \n",
    "                            skip_save_training_statistics=True, \n",
    "                            skip_save_model=True, \n",
    "                            skip_save_progress=True,                          \n",
    "                            skip_save_log=True,\n",
    "                            skip_save_processed_input=True,\n",
    "                            random_seed=42)\n",
    "\n",
    "\n",
    "\n",
    "    model.close()\n",
    "    \n",
    "    validation_losses = train_stats['validation']['Survived']['loss']\n",
    "    \n",
    "    last_epoch = len(validation_losses)\n",
    "    \n",
    "    return {'loss': validation_losses[last_epoch - 1], 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_definition = {\n",
    "    'input_features': [\n",
    "        {'name': 'Pclass', 'type': 'category'},\n",
    "        {'name': 'Sex', 'type': 'category'},\n",
    "        {'name': 'Age', 'type': 'numerical', \n",
    "            'preprocessing': {'missing_value_strategy': 'fill_with_mean', 'normalization': 'zscore'}},\n",
    "        {'name': 'SibSp', 'type': 'numerical', \n",
    "            'preprocessing': {'missing_value_strategy': 'fill_with_mean', 'normalization': 'minmax'}},\n",
    "        {'name': 'Parch', 'type': 'numerical',\n",
    "            'preprocessing': {'missing_value_strategy': 'fill_with_mean', 'normalization': 'minmax'}},\n",
    "        {'name': 'Fare', 'type': 'numerical',\n",
    "            'preprocessing': {'missing_value_strategy': 'fill_with_mean', 'normalization': 'zscore'}},\n",
    "        {'name': 'Embarked', 'type': 'category'}\n",
    "    ],\n",
    "    \n",
    "    'output_features':[\n",
    "        {'name': 'Survived', 'type': 'binary'}\n",
    "    ],\n",
    "    \n",
    "    'training': {}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define hyperparameter search space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED=13\n",
    "\n",
    "space = {'learning_rate':  hp.uniform('learning_rate', 0.0001, 0.01, rng=rng_from_seed(SEED)),\n",
    "         'batch_size': 2 ** (3 + hp.randint('batch_size_exponent',5, rng=rng_from_seed(SEED))),\n",
    "         'output_fc_num_layers': hp.quniform('output_fc_num_layers',1,4,1, rng=rng_from_seed(SEED)),\n",
    "         'output_fc_size': hp.quniform('output_fc_size',5,100,1, rng=rng_from_seed(SEED))\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 64, 'learning_rate': 0.0014278776196269983, 'output_fc_num_layers': 1.0, 'output_fc_size': 38.0}\n",
      "  0%|          | 0/30 [00:00<?, ?it/s, best loss: ?]WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /opt/project/ludwig/features/binary_feature.py:194: calling weighted_cross_entropy_with_logits (from tensorflow.python.ops.nn_impl) with targets is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "targets is deprecated, use labels instead\n",
      "WARNING:tensorflow:From /opt/project/ludwig/utils/tf_utils.py:78: The name tf.GPUOptions is deprecated. Please use tf.compat.v1.GPUOptions instead.\n",
      "\n",
      "{'batch_size': 128, 'learning_rate': 0.004827655322986053, 'output_fc_num_layers': 2.0, 'output_fc_size': 5.0}\n",
      "{'batch_size': 16, 'learning_rate': 0.006582161965921761, 'output_fc_num_layers': 4.0, 'output_fc_size': 34.0}\n",
      "{'batch_size': 32, 'learning_rate': 0.00028175320126028273, 'output_fc_num_layers': 3.0, 'output_fc_size': 33.0}\n",
      "{'batch_size': 16, 'learning_rate': 0.0018049283628271554, 'output_fc_num_layers': 2.0, 'output_fc_size': 9.0}\n",
      "{'batch_size': 8, 'learning_rate': 0.0018401902444361475, 'output_fc_num_layers': 2.0, 'output_fc_size': 21.0}\n",
      "{'batch_size': 16, 'learning_rate': 0.008365035429481069, 'output_fc_num_layers': 2.0, 'output_fc_size': 12.0}\n",
      "{'batch_size': 64, 'learning_rate': 0.0043562431290752645, 'output_fc_num_layers': 2.0, 'output_fc_size': 35.0}\n",
      "{'batch_size': 16, 'learning_rate': 0.007062192972370033, 'output_fc_num_layers': 4.0, 'output_fc_size': 91.0}\n",
      "{'batch_size': 16, 'learning_rate': 0.008091359403893206, 'output_fc_num_layers': 2.0, 'output_fc_size': 16.0}\n",
      "{'batch_size': 64, 'learning_rate': 0.001763527425700674, 'output_fc_num_layers': 2.0, 'output_fc_size': 33.0}\n",
      "{'batch_size': 16, 'learning_rate': 0.0010154492924543564, 'output_fc_num_layers': 1.0, 'output_fc_size': 38.0}\n",
      "{'batch_size': 16, 'learning_rate': 0.00422693581789172, 'output_fc_num_layers': 4.0, 'output_fc_size': 50.0}\n",
      "{'batch_size': 16, 'learning_rate': 0.002350840823498986, 'output_fc_num_layers': 3.0, 'output_fc_size': 89.0}\n",
      "{'batch_size': 16, 'learning_rate': 0.0001742271736189628, 'output_fc_num_layers': 2.0, 'output_fc_size': 50.0}\n",
      "{'batch_size': 128, 'learning_rate': 0.009849703096785902, 'output_fc_num_layers': 4.0, 'output_fc_size': 84.0}\n",
      "{'batch_size': 64, 'learning_rate': 0.003957768518218805, 'output_fc_num_layers': 3.0, 'output_fc_size': 8.0}\n",
      "{'batch_size': 16, 'learning_rate': 0.004982278737156768, 'output_fc_num_layers': 1.0, 'output_fc_size': 98.0}\n",
      "{'batch_size': 32, 'learning_rate': 0.009011403312895752, 'output_fc_num_layers': 3.0, 'output_fc_size': 27.0}\n",
      "{'batch_size': 64, 'learning_rate': 0.0006512792412728838, 'output_fc_num_layers': 1.0, 'output_fc_size': 30.0}\n",
      "{'batch_size': 8, 'learning_rate': 0.0029103819803094214, 'output_fc_num_layers': 1.0, 'output_fc_size': 68.0}\n",
      "{'batch_size': 64, 'learning_rate': 0.0031973882838613405, 'output_fc_num_layers': 1.0, 'output_fc_size': 60.0}\n",
      "{'batch_size': 64, 'learning_rate': 0.0008370677731604487, 'output_fc_num_layers': 1.0, 'output_fc_size': 43.0}\n",
      "{'batch_size': 32, 'learning_rate': 0.0012212814529741997, 'output_fc_num_layers': 1.0, 'output_fc_size': 73.0}\n",
      "{'batch_size': 128, 'learning_rate': 0.006039303757188343, 'output_fc_num_layers': 1.0, 'output_fc_size': 44.0}\n",
      "{'batch_size': 8, 'learning_rate': 0.0033128526027089494, 'output_fc_num_layers': 1.0, 'output_fc_size': 59.0}\n",
      "{'batch_size': 64, 'learning_rate': 0.00011525955535967702, 'output_fc_num_layers': 1.0, 'output_fc_size': 42.0}\n",
      "{'batch_size': 64, 'learning_rate': 0.00582991061128906, 'output_fc_num_layers': 1.0, 'output_fc_size': 19.0}\n",
      "{'batch_size': 128, 'learning_rate': 0.002378623478191065, 'output_fc_num_layers': 1.0, 'output_fc_size': 58.0}\n",
      "{'batch_size': 32, 'learning_rate': 0.001059405935261645, 'output_fc_num_layers': 2.0, 'output_fc_size': 26.0}\n",
      "100%|██████████| 30/30 [01:18<00:00,  2.63s/it, best loss: 0.39512621490649]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "best = fmin(fn=score_model,\n",
    "    space=space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimal hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size: 64 , learning_rate: 0.0014278776196269983 , fc_num_layers: 1 , fc_size: 38\n"
     ]
    }
   ],
   "source": [
    "print('batch size:', 2 ** np.int(3 + best['batch_size_exponent']),\n",
    "      ', learning_rate:', best['learning_rate'],\n",
    "      ', fc_num_layers:', np.int(best['output_fc_num_layers']),\n",
    "      ', fc_size:', np.int(best['output_fc_size']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with optimal hyperparameters on the whole data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean out old results\n",
    "try:\n",
    "    shutil.rmtree('./results')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    shutil.rmtree('./visualizations')\n",
    "except:\n",
    "    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set optimal hyperparameters for training    \n",
    "model_definition['training']['learning_rate'] = best['learning_rate']\n",
    "model_definition['training']['batch_size'] = 2** (3 + best['batch_size_exponent'])\n",
    "model_definition['output_features'][0]['num_fc_layers'] = np.int(best['output_fc_num_layers'])\n",
    "model_definition['output_features'][0]['fc_size'] = np.int(best['output_fc_size'])\n",
    "\n",
    "\n",
    "# Define Ludwig model object that drive model training\n",
    "model = LudwigModel(model_definition,\n",
    "                    logging_level=logging.WARN)\n",
    "\n",
    "# initiate model training\n",
    "train_stats = model.train(data_train_df = raw_df,\n",
    "                        experiment_name='experiment_name',\n",
    "                        model_name='model_name',                        \n",
    "                        random_seed=42)\n",
    "\n",
    "\n",
    "\n",
    "model.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
