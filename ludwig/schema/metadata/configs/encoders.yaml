ALBERT:
    type:
        short_description:
            Similar to BERT with lower memory footprint and faster training.
        long_description:
            The albert encoder loads a pretrained ALBERT (default albert-base-v2) model using the Hugging
            Face transformers package. Albert is similar to BERT, with significantly lower memory usage and
            somewhat faster training time.
    attention_probs_dropout_prob:
        default_value_reasoning:
            Dropout can cause training to become less stable.
            Consider start with a dropout-free baseline, and add dropout gradually
            in subsequent experiments.
        description_implications:
            "Dropout is a computationally cheap regularization\
            \ method where during training, some neurons are randomly ignored or \u201C\
            dropped out\u201D. Increasing dropout has the effect of making the training\
            \ process more noisy and lowering overall network capacity, but it can\
            \ be an effective regularization method to reduce overfitting and improve\
            \ generalization."
        example_value:
            - 0.2
        expected_impact: 3
        literature_references:
            - https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html
        related_parameters:
            - hidden_dropout_prob, classifier_dropout_prob
        suggested_values: 0.05 - 0.8
        suggested_values_reasoning:
            Tuning dropout is really something to be done
            when all of the big choices about architecture have been settled. Consider
            starting with 0.5 and adjusting the dropout depending on observed model
            performance.
        ui_display_name: attention_probs_dropout_prob
    bos_token_id:
        default_value_reasoning: Default value used in pre-trained HF encoder.
        ui_display_name: Beginning-of-Sentence Token Id
    classifier_dropout_prob:
        default_value_reasoning: Huggingface default.
        description_implications:
            "Dropout is a computationally cheap regularization\
            \ method where during training, some neurons are randomly ignored or \u201C\
            dropped out\u201D. Increasing dropout has the effect of making the training\
            \ process more noisy and lowering overall network capacity, but it can\
            \ be an effective regularization method to reduce overfitting and improve\
            \ generalization."
        example_value:
            - 0.2
        expected_impact: 3
        literature_references:
            - https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html
        related_parameters:
            - hidden_dropout_prob, attention_probs_dropout_prob
        suggested_values: 0.05 - 0.8
        suggested_values_reasoning:
            Tuning dropout is really something to be done
            when all of the big choices about architecture have been settled. Consider
            starting with 0.5 and adjusting the dropout depending on observed model
            performance.
        ui_display_name: classifier_dropout_prob
    embedding_size:
        default_value_reasoning: Not too big, not too small.
        description_implications:
            'An embedding is a relatively low-dimensional space
            that is used to translate high-dimensional vectors like words, which can
            have a large vocbulary size. Ideally, after an embedding is trained, it
            captures some of the semantics of the input by placing semantically similar
            inputs close together in the embedding space.


            In most cases, the embedding size is chosen empirically, by trial and
            error. From https://www.amazon.com/dp/1098115783, "one rule of thumb is
            to use the fourth root of the total number of unique categorical elements
            while another is that the embedding dimension should be approximately
            1.6 times the square root of the number of unique elements in the category,
            and no less than 600."


            Increasing the embedding size may cause the model to train more slowly,
            but the higher dimensionality can also improve overall quality.'
        expected_impact: 2
        literature_references:
            - https://developers.google.com/machine-learning/crash-course/embeddings/video-lecture
        suggested_values: 1.6 * sqrt(vocab_size)
        suggested_values_reasoning:
            Rule of thumb suggested by a deep learning textbook.
            Try models with smaller or larger embedding sizes to observe relative
            impact.
        ui_display_name: Embedding Size
    eos_token_id:
        default_value_reasoning: Default value used in pre-trained HF encoder.
        ui_display_name: End-of-Sentence Token Id
    hidden_act:
        default_value_reasoning: Taken from huggingface.
        description_implications:
            Changing this activation function will only affect
            the feed-forward layers of the transformer.
        example_value:
            - relu
        expected_impact: 2
        literature_references:
            - "[Hugging face docs for ALBERT config](https://huggingface.co/docs/transformers/model_doc/albert#transformers.AlbertConfig.hidden_act)\n\
              \r\n[Relevant StackOverflow discussion](https://ai.stackexchange.com/questions/30341/why-does-a-transformer-not-use-an-activation-function-following-the-multi-head-a)"
        suggested_values: gelu
        suggested_values_reasoning: Taken from huggingface defaults.
        ui_display_name: Hidden Layer Activation
    hidden_dropout_prob:
        default_value_reasoning:
            Dropout can cause training to become less stable.
            Consider start with a dropout-free baseline, and add dropout gradually
            in subsequent experiments.
        description_implications:
            "Dropout is a computationally cheap regularization\
            \ method where during training, some neurons are randomly ignored or \u201C\
            dropped out\u201D. Increasing dropout has the effect of making the training\
            \ process more noisy and lowering overall network capacity, but it can\
            \ be an effective regularization method to reduce overfitting and improve\
            \ generalization."
        example_value:
            - 0.2
        expected_impact: 3
        literature_references:
            - https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html
        related_parameters:
            - "attention_probs_dropout_prob,

              classifier_dropout_prob"
        suggested_values: 0.05 - 0.8
        suggested_values_reasoning:
            Tuning dropout is really something to be done
            when all of the big choices about architecture have been settled. Consider
            starting with 0.5 and adjusting the dropout depending on observed model
            performance.
        ui_display_name: hidden_dropout_prob
    hidden_size:
        default_value_reasoning: Huggingface default.
        description_implications:
            Increasing the hidden size makes the model larger
            and slower to train, increases the model's capacity to capture more complexity.
            It also increases the chance of overfitting.
        expected_impact: 2
        suggested_values: 10 - 2048
        suggested_values_reasoning:
            Increasing the hidden size makes sense if the
            model is underfitting. It's useful to train both smaller and larger models
            to see how model capacity affects performance. This should only be explored
            after the architecture of the model has been settled.
        ui_display_name: Hidden Size
    initializer_range:
        description_implications:
            There is an ideal value for this variable that doesn't
            lead to the outputs of these matrices to vanish or explode
        example_value:
            - 0.02
        expected_impact: 3
        other_information: Must be greater than 0
        related_parameters:
            - weights_initializer
        suggested_values: 0.01-0.05
        suggested_values_reasoning:
            Large values will likely lead to very large outputs.
            Small values will lead to vanishing outputs.
        ui_display_name: null
    inner_group_num:
        ui_display_name: null
    intermediate_size:
        ui_display_name: null
    layer_norm_eps:
        ui_display_name: null
    max_position_embeddings:
        default_value_reasoning: Taken from huggingface.
        description_implications:
            "An embedding is a relatively low-dimensional space
            that is used to translate high-dimensional vectors like words or positions,
            which can have a large vocbulary size. Ideally, after an embedding is
            trained, it captures some of the semantics of the input by placing semantically
            similar inputs close together in the embedding space.


            Increasing the embedding size may cause the model to train more slowly,
            but the higher dimensionality can also improve overall quality."
        expected_impact: 2
        suggested_values: 512
        suggested_values_reasoning:
            Out of the box value based on published literature.
            Try models with smaller or larger embedding sizes to observe relative
            impact.
        ui_display_name: Max Position Embeddings
    max_sequence_length:
        default_value_reasoning:
            Sets the maximum sequence length of the expected
            inputs, so input/output shapes are computed accurately.
        expected_impact: 1
        ui_display_name: null
    num_attention_heads:
        ui_display_name: null
    num_hidden_groups:
        ui_display_name: null
    num_hidden_layers:
        ui_display_name: null
    pad_token_id:
        ui_display_name: null
    position_embedding_type:
        ui_display_name: null
    pretrained_kwargs:
        default_value_reasoning: These arguments typically don't need to be specified.
        expected_impact: 1
        related_parameters:
            - pretrained_model_name_or_path
        suggested_values: Default
        ui_display_name: null
    pretrained_model_name_or_path:
        default_value_reasoning:
            The default model is the canonical model for this
            model architecture, and is therefore a good starting point for most use
            cases.
        description_implications:
            "There are two factors to consider when choosing\
            \ a pre-trained model: (1) size, and (2) task similarity. \n\nThe larger\
            \ the model, the more subtle its comprehension of inputs can become. However,\
            \ larger models are also more compute and memory-intensive to train.\n\
            \nModels pretrained on highly-related source tasks are more likely to\
            \ be successful on the target task. Consider searching the HuggingFace\
            \ model repository for models trained on similar tasks."
        expected_impact: 3
        literature_references:
            - https://arxiv.org/abs/1909.11942
        related_parameters:
            - use_pretrained, trainable, pretrained_kwargs
        suggested_values: albert-large-v2, albert-base-chinese
        suggested_values_reasoning:
            "If you would like better performance and are
            not compute/memory-constrained, increasing model capacity can potentially
            provide a richer representation than the default. The suggested value
            upsizes the model while maintaining the same model architecture.


            Language models trained on general corpora typically generalize well.
            Consider deviating from the default only if the text in the dataset originates
            from another domain (e.g. languages other than English)."
        ui_display_name: Pretrained model
    reduce_output:
        ui_display_name: null
    saved_weights_in_checkpoint:
        default_value_reasoning:
            The weights of the encoder are not necessarily saved
            in the checkpoint. The user has to save them first.
        description_implications:
            The memory footprint for some of these encoders
            can be large.
        expected_impact: 1
        related_parameters:
            - skip_save_model
        suggested_values:
            - false
        suggested_values_reasoning:
            Some of these encoders are large, so it might
            be better to load them as needed, especially if 1. they're not used frequently
            2. the user doesn't have a lot of storage.
        ui_display_name: null
    trainable:
        expected_impact: 2
        ui_display_name: null
    type_vocab_size:
        ui_display_name: null
    use_pretrained:
        ui_display_name: null
    vocab:
        default_value_reasoning:
            Computed and passed along internally according to
            preprocessing settings.
        example_value:
            - a
            - b
            - c
        expected_impact: 2
        internal_only: true
        ui_display_name: Not Displayed
    vocab_size:
        internal_only: true
        ui_display_name: Not displayed
AutoTransformer:
    type:
        short_description:
            Automatically retrieves the architecture from the provided model name/path.
        long_description:
            The auto_transformer encoder automatically instantiates the model architecture for the specified
            pretrained_model_name_or_path. Unlike the other HF encoders available, auto_transformer does not
            provide a default value for pretrained_model_name_or_path, this is its only mandatory parameter.
            See the Hugging Face AutoModels documentation for more details.
        literature_references:
            - https://huggingface.co/docs/transformers/model_doc/auto
    max_sequence_length:
        default_value_reasoning:
            Sets the maximum sequence length of the expected
            inputs, so input/output shapes are computed accurately.
        expected_impact: 1
        ui_display_name: null
    pretrained_kwargs:
        ui_display_name: null
    pretrained_model_name_or_path:
        ui_display_name: null
    reduce_output:
        ui_display_name: null
    trainable:
        expected_impact: 2
        ui_display_name: null
    vocab:
        default_value_reasoning:
            Computed and passed along internally according to
            preprocessing settings.
        example_value:
            - a
            - b
            - c
        expected_impact: 2
        internal_only: true
        ui_display_name: Not Displayed
    vocab_size:
        internal_only: true
        ui_display_name: Not displayed
BERT:
    type:
        short_description:
            Bidirectional transformer great for language modeling.
        long_description:
            The bert encoder loads a pretrained BERT (default bert-base-uncased) model using the Hugging
            Face transformers package. BERT is a bidirectional transformer pretrained using a combination of
            masked language modeling objective and next sentence prediction on a large corpus comprising the
            Toronto Book Corpus and Wikipedia.
        literature_references:
            - https://arxiv.org/abs/1810.04805
    attention_probs_dropout_prob:
        default_value_reasoning: Huggingface default.
        description_implications:
            "Dropout is a computationally cheap regularization\
            \ method where during training, some neurons are randomly ignored or \u201C\
            dropped out\u201D. Increasing dropout has the effect of making the training\
            \ process more noisy and lowering overall network capacity, but it can\
            \ be an effective regularization method to reduce overfitting and improve\
            \ generalization."
        example_value:
            - 0.2
        expected_impact: 3
        literature_references:
            - https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html
        related_parameters:
            - hidden_dropout_prob, classifier_dropout
        suggested_values: 0.05 - 0.8
        suggested_values_reasoning:
            Tuning dropout is really something to be done
            when all of the big choices about architecture have been settled. Consider
            starting with 0.5 and adjusting the dropout depending on observed model
            performance.
        ui_display_name: attention_probs_dropout_prob
    classifier_dropout:
        default_value_reasoning: Huggingface default.
        description_implications:
            "Dropout is a computationally cheap regularization\
            \ method where during training, some neurons are randomly ignored or \u201C\
            dropped out\u201D. Increasing dropout has the effect of making the training\
            \ process more noisy and lowering overall network capacity, but it can\
            \ be an effective regularization method to reduce overfitting and improve\
            \ generalization."
        example_value:
            - 0.2
        expected_impact: 3
        literature_references:
            - https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html
        related_parameters:
            - hidden_dropout_prob, attention_probs_dropout_prob
        suggested_values: 0.05 - 0.8
        suggested_values_reasoning:
            Tuning dropout is really something to be done
            when all of the big choices about architecture have been settled. Consider
            starting with 0.5 and adjusting the dropout depending on observed model
            performance.
        ui_display_name: classifier_dropout
    gradient_checkpointing:
        ui_display_name: null
    hidden_act:
        default_value_reasoning: Taken from huggingface.
        description_implications:
            Changing this activation function will only affect
            the feed-forward layers of the transformer.
        example_value:
            - relu
        expected_impact: 2
        literature_references:
            - "[Huggingface docs for BERT config](https://huggingface.co/docs/transformers/model_doc/bert#transformers.BertConfig.hidden_act)\n\
              \r\n[Relevant StackOverflow discussion](https://ai.stackexchange.com/questions/30341/why-does-a-transformer-not-use-an-activation-function-following-the-multi-head-a)"
        suggested_values: gelu
        suggested_values_reasoning: Taken from huggingface defaults.
        ui_display_name: Hidden Layer Activation
    hidden_dropout_prob:
        default_value_reasoning: Huggingface default.
        description_implications:
            "Dropout is a computationally cheap regularization\
            \ method where during training, some neurons are randomly ignored or \u201C\
            dropped out\u201D. Increasing dropout has the effect of making the training\
            \ process more noisy and lowering overall network capacity, but it can\
            \ be an effective regularization method to reduce overfitting and improve\
            \ generalization."
        example_value:
            - 0.2
        expected_impact: 3
        literature_references:
            - https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html
        related_parameters:
            - attention_probs_dropout_prob, classifier_dropout
        suggested_values: 0.05 - 0.8
        suggested_values_reasoning:
            Tuning dropout is really something to be done
            when all of the big choices about architecture have been settled. Consider
            starting with 0.5 and adjusting the dropout depending on observed model
            performance.
        ui_display_name: hidden_dropout_prob
    hidden_size:
        default_value_reasoning: Huggingface default.
        description_implications:
            Increasing the hidden size makes the model larger
            and slower to train, increases the model's capacity to capture more complexity.
            It also increases the chance of overfitting.
        expected_impact: 2
        suggested_values: 10 - 2048
        suggested_values_reasoning:
            Increasing the hidden size makes sense if the
            model is underfitting. It's useful to train both smaller and larger models
            to see how model capacity affects performance. This should only be explored
            after the architecture of the model has been settled.
        ui_display_name: Hidden Size
    initializer_range:
        description_implications:
            There is an ideal value for this variable that doesn't
            lead to the outputs of these matrices to vanish or explode
        example_value:
            - 0.02
        expected_impact: 3
        other_information: Must be greater than 0
        related_parameters:
            - weights_initializer
        suggested_values: 0.01-0.05
        suggested_values_reasoning:
            Large values will likely lead to very large outputs.
            Small values will lead to vanishing outputs.
        ui_display_name: null
    intermediate_size:
        ui_display_name: null
    layer_norm_eps:
        ui_display_name: null
    max_position_embeddings:
        default_value_reasoning: Taken from huggingface.
        description_implications:
            "An embedding is a relatively low-dimensional space
            that is used to translate high-dimensional vectors like words or positions,
            which can have a large vocbulary size. Ideally, after an embedding is
            trained, it captures some of the semantics of the input by placing semantically
            similar inputs close together in the embedding space.


            Increasing the embedding size may cause the model to train more slowly,
            but the higher dimensionality can also improve overall quality."
        expected_impact: 2
        suggested_values: 512
        suggested_values_reasoning:
            Out of the box value based on published literature.
            Try models with smaller or larger embedding sizes to observe relative
            impact.
        ui_display_name: Max Position Embeddings
    max_sequence_length:
        default_value_reasoning:
            Sets the maximum sequence length of the expected
            inputs, so input/output shapes are computed accurately.
        expected_impact: 1
        ui_display_name: null
    num_attention_heads:
        ui_display_name: null
    num_hidden_layers:
        ui_display_name: null
    pad_token_id:
        ui_display_name: null
    position_embedding_type:
        ui_display_name: null
    pretrained_kwargs:
        ui_display_name: null
    pretrained_model_name_or_path:
        ui_display_name: null
    reduce_output:
        ui_display_name: null
    saved_weights_in_checkpoint:
        default_value_reasoning:
            The weights of the encoder are not necessarily saved
            in the checkpoint. The user has to save them first.
        description_implications:
            The memory footprint for some of these encoders
            can be large.
        expected_impact: 1
        related_parameters:
            - skip_save_model
        suggested_values:
            - false
        suggested_values_reasoning:
            Some of these encoders are large, so it might
            be better to load them as needed, especially if 1. they're not used frequently
            2. the user doesn't have a lot of storage.
        ui_display_name: null
    trainable:
        expected_impact: 2
        ui_display_name: null
    type_vocab_size:
        ui_display_name: null
    use_pretrained:
        ui_display_name: null
    vocab:
        default_value_reasoning:
            Computed and passed along internally according to
            preprocessing settings.
        example_value:
            - a
            - b
            - c
        expected_impact: 2
        internal_only: true
        ui_display_name: Not Displayed
    vocab_size:
        internal_only: true
        ui_display_name: Not displayed
BagEmbedWeighted:
    type:
        short_description:
            Transforms feature to vector, maps to sparse or dense embeddings, then aggregates.
        long_description:
            The embed weighted encoder first transforms the element frequency vector to sparse integer
            lists, which are then mapped to either dense or sparse embeddings (one-hot encodings). Lastly,
            embeddings are aggregated as a weighted sum where each embedding is multiplied by its respective
            element's frequency. Inputs are of size b while outputs are of size b x h where b is the batch
            size and h is the dimensionality of the embeddings.
    activation:
        default_value_reasoning:
            The Rectified Linear Units (ReLU) function is the
            standard activation function used for adding non-linearity. It is simple,
            fast, and empirically works well (https://arxiv.org/abs/1803.08375).
        description_implications:
            Changing the activation functions has an impact
            on the computational load of the model and might require further hypterparameter
            tuning
        expected_impact: 1
        suggested_values:
            The default value will work well in the majority of the
            cases
        ui_display_name: Activation
    bias_initializer:
        default_value_reasoning:
            It is possible and common to initialize the biases
            to be zero, since the asymmetry breaking is provided by the small random
            numbers in the weights.
        description_implications:
            It's rare to see any performance gains from choosing
            a different bias initialization. Some practitioners like to use a small
            constant value such as 0.01 for all biases to ensure that all ReLU units
            are activated in the beginning and have some effect on the gradient. However,
            it's still an open question as to whether this provides consistent improvement.
        expected_impact: 1
        literature_references:
            - https://cs231n.github.io/neural-networks-2/
        related_parameters:
            - weights_initializer
        suggested_values: zeros
        suggested_values_reasoning:
            It is possible and common to initialize the biases
            to be zero, since the asymmetry breaking is provided by the small random
            numbers in the weights. For ReLU non-linearities, some people like to
            use small constant value such as 0.01 for all biases because this ensures
            that all ReLU units fire in the beginning and therefore obtain and propagate
            some gradient. However, it is not clear if this provides a consistent
            improvement (in fact some results seem to indicate that this performs
            worse) and it is more common to simply use 0 bias initialization.
        ui_display_name: Bias Initializer
    dropout:
        default_value_reasoning:
            Dropout can cause training to become less stable.
            Consider start with a dropout-free baseline, and add dropout gradually
            in subsequent experiments.
        description_implications:
            "Dropout is a computationally cheap regularization\
            \ method where during training, some neurons are randomly ignored or \u201C\
            dropped out\u201D. Increasing dropout has the effect of making the training\
            \ process more noisy and lowering overall network capacity, but it can\
            \ be an effective regularization method to reduce overfitting and improve\
            \ generalization."
        example_value:
            - 0.2
        expected_impact: 3
        literature_references:
            - https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html
        suggested_values: 0.05 - 0.8
        suggested_values_reasoning:
            Tuning dropout is really something to be done
            when all of the big choices about architecture have been settled. Consider
            starting with 0.5 and adjusting the dropout depending on observed model
            performance.
        ui_display_name: Dropout
    embedding_size:
        default_value_reasoning: Not too big, not too small.
        description_implications:
            'An embedding is a relatively low-dimensional space
            that is used to translate high-dimensional vectors like words, which can
            have a large vocbulary size. Ideally, after an embedding is trained, it
            captures some of the semantics of the input by placing semantically similar
            inputs close together in the embedding space.


            In most cases, the embedding size is chosen empirically, by trial and
            error. From https://www.amazon.com/dp/1098115783, "one rule of thumb is
            to use the fourth root of the total number of unique categorical elements
            while another is that the embedding dimension should be approximately
            1.6 times the square root of the number of unique elements in the category,
            and no less than 600."


            Increasing the embedding size may cause the model to train more slowly,
            but the higher dimensionality can also improve overall quality.'
        expected_impact: 2
        literature_references:
            - https://developers.google.com/machine-learning/crash-course/embeddings/video-lecture
        suggested_values: 1.6 * sqrt(vocab_size)
        suggested_values_reasoning:
            Rule of thumb suggested by a deep learning textbook.
            Try models with smaller or larger embedding sizes to observe relative
            impact.
        ui_display_name: Embedding Size
    embeddings_on_cpu:
        default_value_reasoning:
            By default embeddings matrices are stored on GPU
            memory if a GPU is used, as it allows for faster access.
        description_implications:
            By default embeddings matrices are stored on GPU
            memory if a GPU is used, as it allows for faster access. However, in some
            cases when the vocabulary size is very large, the full embedding matrix
            may be really big and unwieldy to have in GPU memory. This parameter forces
            the placement of the embedding matrix in regular memory and the CPU is
            used to access them. This may slow down training due to additional data
            transfer between CPU and GPU memory, but can lead to healthier GPU memory
            resource usage.
        expected_impact: 1
        suggested_values:
            - false
        suggested_values_reasoning:
            If GPU memory is not a constraint, having embeddings
            stored and accessed within the GPU is faster.
        ui_display_name: Embeddings on CPU
    embeddings_trainable:
        default_value_reasoning:
            If trained from scratch, embedding vectors are typically
            learned alongside the rest of the model.
        description_implications:
            Typically this value is only set to False if pre-trained
            embeddings are uploaded. Even then, it is reasonable to leave it as True
            in order to fine-tune the embeddings.
        expected_impact: 1
        related_parameters:
            - embedding_size, representation, pretrained_embeddings
        ui_display_name: (under Embeddings header) Trainable?
    fc_layers:
        default_value_reasoning:
            By default the stack is built by using num_fc_layers,
            output_size, use_bias, weights_initializer, bias_initializer, norm, norm_params,
            activation, dropout. When a list of dictionaries is provided, the stack
            is built following the parameters of each dict for building each layer.
        description_implications:
            The more layers that are specified the deeper and
            higher capacity the model will be. This makes it possible to potentially
            achieve better performance when a big anough amount of data is provided,
            but also makes the model more computationally expensive and potentially
            more prone to overfitting.
        example_value:
            - dropout: 0.1
              output_size: 128
            - norm: layer
              output_size: 64
        expected_impact: 1
        related_parameters:
            - output_size
            - use_bias
            - weights_initializer
            - bias_initializer
            - norm
            - norm_params
            - activation
            - dropout
        suggested_values_reasoning:
            It is easier to define a stack of fully connected
            layers by just specifying num_fc_layers, output_size and the other individual
            parameters. It will create a stack of layers with identical properties.
            Use this parameter only if you need a fine grained level of control of
            each individual layer in the stack.
        ui_display_name: Fully Connected Layers
    force_embedding_size:
        default_value_reasoning:
            It is not often the case that the user has a strict
            need for using an embedding size that should be larger than the vocabulary
            size.
        description_implications:
            Should only be True if the user has a strict need
            for using an embedding size that should be larger than the vocabulary
            size. For example, there may be size requirements across multiple features
            imposed by downstream modules like the ComparatorCombiner.
        expected_impact: 1
        related_parameters:
            - embedding_size
        suggested_values:
            - false
        suggested_values_reasoning: True for advanced usage only.
        ui_display_name: Force Embedding Size
    norm:
        default_value_reasoning:
            While batch normalization and layer normalization
            usually lead to improvements, it can be useful to start with fewer bells
            and whistles.
        description_implications:
            Normalization helps stabilize the learning process
            and can have a regularizing effect that can help with generalization.
            It's often suggested that with normalization, you can use a higher learning
            rate.
        example_value:
            - batch
        expected_impact: 3
        literature_references:
            - https://machinelearningmastery.com/batch-normalization-for-training-of-deep-neural-networks/
        related_parameters:
            - norm_params
        suggested_values: '"batch" or "layer"'
        suggested_values_reasoning:
            Normalization tries to solve "internal covariate
            shift" that comes from the changing distributions of the inputs to layers
            deep in the network when weights are updated. For example, batch normalization
            standardizes the inputs to a layer for each mini-batch. Try out different
            normalizations to see if that helps with training stability
        ui_display_name: Normalization Type
    norm_params:
        default_value_reasoning:
            The default parameters that come with Torch's implementation
            of these normalization types are a trusted starting point.
        description_implications:
            There are a variety of ways a certain set of parameters
            specificed could influence performance here. Broadly speaking the different
            values passed in here allow for different levels of smoothness to be observed
            in the learning curves. Since setting this parameters depends on the type
            of `norm` set, see [BatchNorm2d](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html)
            for more information on the parameters to set for batch normalization,
            and see [LayerNorm](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html)
            for more information on the parameters to set for layer normalization.
        example_value:
            - affine: false
              momentum: 0.2
              num_features: 100
        expected_impact: 1
        literature_references:
            - "For BatchNorm2d: https://arxiv.org/abs/1502.03167


              For LayerNorm: https://arxiv.org/abs/1607.06450"
        related_parameters:
            - "`norm`"
        suggested_values: Depends on the type of `norm` set.
        suggested_values_reasoning: "NO"
        ui_display_name: Normalization Parameters
    num_fc_layers:
        default_value_reasoning:
            The encoder already has learnable parameters.Sometimes
            the default is 1 for modules where the FC stack is used for shape management,
            or the only source of learnable parameters.
        description_implications:
            Increasing num_fc_layers will increase the capacity
            of the model. The model will be slower to train, and there's a higher
            risk of overfitting.
        example_value:
            - 1
        expected_impact: 1
        other_information:
            Not all modules that have fc_layers also have an accompanying
            num_fc_layers parameter. Where both are present, fc_layers takes precedent
            over num_fc_layers. Specifying num_fc_layers alone uses fully connected
            layers that are configured by the defaults in FCStack.
        related_parameters:
            - fc_layers
        suggested_values: 0-1
        suggested_values_reasoning:
            The full model likely contains many learnable
            parameters. Consider starting with very few, or without any additional
            fully connected layers and add them if you observe evidence of limited
            model capacity. Sometimes the default is 1 for modules where the FC stack
            is used for shape management, or the only source of learnable parameters.
        ui_display_name: Number of Fully Connected Layers
    output_size:
        default_value_reasoning: A modest value, not too small, not too large.
        description_implications:
            If there are fully connected layers in this module,
            increasing the output size of each fully connected layer will increase
            the capacity of the model. However, the model may be slower to train,
            and there's a higher risk of overfitting. If it seems like the model could
            use even more capacity, consider increasing the number of fully connected
            layers, or explore other architectures.
        expected_impact: 2
        other_information:
            If num_fc_layers=0 and fc_layers=None, and there are no
            fully connected layers defined on the module, then this parameter may
            have no effect on the module's final output shape.
        related_parameters:
            - num_fc_layers, fc_layers
        suggested_values: 10 - 1024
        suggested_values_reasoning:
            Increasing the output size increases the capacity
            of the model. If this seems to have a positive effect, then it could be
            worth increasing the number of layers, or trying a different architecture
            with a larger capacity.
        ui_display_name: Output Size
    pretrained_embeddings:
        default_value_reasoning:
            Embeddings are commonly trained from scratch, or
            incorporated as part of a pre-trained model package.
        description_implications:
            If pretrained embeddings are specified, then the
            model may have a head start in its representation of various input entities.
        example_value:
            - ~/Downloads/glove.6B.100d.txt
        expected_impact: 2
        related_parameters:
            - embedding_size, embeddings_trainable
        ui_display_name: Pretrained embeddings path
    representation:
        default_value_reasoning:
            Trainable, randomly initialized embedding vectors
            often lead to more subtle representations of input entities than one-hot
            vectors.
        description_implications:
            If set to sparse, the representations for input
            entities are fixed as one-hot vectors. This leads to less flexible representations
            for input entities, but could lead to faster training since there are
            less learnable parameters.
        expected_impact: 1
        other_information: ""
        related_parameters:
            - embedding_size, embeddings_trainable, pretrained_embeddings
        ui_display_name: Representation approach
    use_bias:
        default_value_reasoning:
            "Bias terms may improve model accuracy, and don't
            have much impact in terms of memory or training speed. For most models
            it is reasonable to use bias terms.


            Batch Normalization, however, adds a trainable shift parameter which is
            added to the activation. When Batch Normalization is used in a layer,
            bias terms are redundant and may be removed."
        description_implications:
            Bias terms may improve model accuracy, and don't
            have much impact in terms of memory or training speed. For most models
            it is reasonable to leave this parameter set to True.
        example_value:
            - true
        expected_impact: 1
        other_information:
            If fc_layers is not specified, or use_bias is not specified
            for individual layers, the value of use_bias will be used as the default
            for all layers.
        related_parameters:
            - bias_initializer, fc_layers
        suggested_values:
            - true
        ui_display_name: Use Bias
    vocab:
        default_value_reasoning:
            Computed and passed along internally according to
            preprocessing settings.
        example_value:
            - a
            - b
            - c
        expected_impact: 2
        internal_only: true
        ui_display_name: Not Displayed
    weights_initializer:
        default_value_reasoning: Taken from published [literature](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf).
        description_implications:
            The method you choose to initialize layer weights
            during training can have a big impact on performance as well as the reproducibility
            of your final model between runs. As an example, if you were to randomly
            initialize weights you would risk non-reproducibility (and possibly general
            training performance), but sticking with constant values for initialization
            might significantly increase the time needed for model convergence. Generally,
            choosing one of the probabilistic approaches strikes a balance between
            the two extremes, and the literature kicked off by the landmark [*Xavier
            et al.* paper](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)
            provides a few good options. See this nice discussion from [Weights and
            Biases](https://wandb.ai/site/articles/the-effects-of-weight-initialization-on-neural-nets#:~:text=Studies%20have%20shown%20that%20initializing,net%20train%20better%20and%20faster.)
            for more information.
        expected_impact: 3
        literature_references:
            - "Weights and Biases blog post: https://wandb.ai/site/articles/the-effects-of-weight-initialization-on-neural-nets#:~:text=Studies%20have%20shown%20that%20initializing,net%20train%20better%20and%20faster.


              Xavier et al. paper: http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf"
        suggested_values: xavier_uniform
        suggested_values_reasoning:
            Changing the weights initialization scheme is
            something to consider if a model is having trouble with convergence, or
            otherwise it is something to experiment with after other factors are considered.
            The default choice (`xavier_uniform`) is a suitable starting point for
            most tasks.
        ui_display_name: Layer Weights Initializer
CTRL:
    type:
        short_description:
            Language model trained to condition on control codes that govern style, content and task-specific behavior.
        long_description:
            The ctrl encoder loads a pretrained CTRL (default ctrl) model using the Hugging Face
            transformers package. CTRL is a causal (unidirectional) transformer pre-trained using language
            modeling on a very large corpus of ~140 GB of text data with the first token reserved as a
            control code (such as Links, Books, Wikipedia etc.).
        literature_references:
            - https://arxiv.org/abs/1909.05858
    attn_pdrop:
        ui_display_name: null
    dff:
        ui_display_name: null
    embd_pdrop:
        ui_display_name: null
    initializer_range:
        description_implications:
            There is an ideal value for this variable that doesn't
            lead to the outputs of these matrices to vanish or explode
        example_value:
            - 0.02
        expected_impact: 3
        other_information: Must be greater than 0
        related_parameters:
            - weights_initializer
        suggested_values: 0.01-0.05
        suggested_values_reasoning:
            Large values will likely lead to very large outputs.
            Small values will lead to vanishing outputs.
        ui_display_name: null
    layer_norm_epsilon:
        ui_display_name: null
    max_sequence_length:
        default_value_reasoning:
            Sets the maximum sequence length of the expected
            inputs, so input/output shapes are computed accurately.
        expected_impact: 1
        ui_display_name: null
    n_ctx:
        ui_display_name: null
    n_embd:
        ui_display_name: null
    n_head:
        ui_display_name: null
    n_layer:
        ui_display_name: null
    n_positions:
        ui_display_name: null
    pretrained_kwargs:
        ui_display_name: null
    pretrained_model_name_or_path:
        ui_display_name: null
    reduce_output:
        ui_display_name: null
    resid_pdrop:
        ui_display_name: null
    saved_weights_in_checkpoint:
        default_value_reasoning:
            The weights of the encoder are not necessarily saved
            in the checkpoint. The user has to save them first.
        description_implications:
            The memory footprint for some of these encoders
            can be large.
        expected_impact: 1
        related_parameters:
            - skip_save_model
        suggested_values:
            - false
        suggested_values_reasoning:
            Some of these encoders are large, so it might
            be better to load them as needed, especially if 1. they're not used frequently
            2. the user doesn't have a lot of storage.
        ui_display_name: null
    trainable:
        expected_impact: 2
        ui_display_name: null
    use_pretrained:
        ui_display_name: null
    vocab:
        default_value_reasoning:
            Computed and passed along internally according to
            preprocessing settings.
        example_value:
            - a
            - b
            - c
        expected_impact: 2
        internal_only: true
        ui_display_name: Not Displayed
    vocab_size:
        internal_only: true
        ui_display_name: Not displayed
CamemBERT:
    type:
        short_description:
            Language model trained on large French text corpus.
        long_description:
            The camembert encoder loads a pretrained CamemBERT (default jplu/tf-camembert-base) model using
            the Hugging Face transformers package. CamemBERT is pre-trained on 138GB of French text.
        literature_references:
            - https://arxiv.org/abs/1911.03894
    attention_probs_dropout_prob:
        default_value_reasoning: Huggingface default.
        description_implications:
            "Dropout is a computationally cheap regularization\
            \ method where during training, some neurons are randomly ignored or \u201C\
            dropped out\u201D. Increasing dropout has the effect of making the training\
            \ process more noisy and lowering overall network capacity, but it can\
            \ be an effective regularization method to reduce overfitting and improve\
            \ generalization."
        example_value:
            - 0.2
        expected_impact: 3
        literature_references:
            - https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html
        related_parameters:
            - classifier_dropout, hidden_dropout_prob
        suggested_values: 0.05 - 0.8
        suggested_values_reasoning:
            Tuning dropout is really something to be done
            when all of the big choices about architecture have been settled. Consider
            starting with 0.5 and adjusting the dropout depending on observed model
            performance.
        ui_display_name: attention_probs_dropout_prob
    classifier_dropout:
        default_value_reasoning: Huggingface default.
        description_implications:
            "Dropout is a computationally cheap regularization\
            \ method where during training, some neurons are randomly ignored or \u201C\
            dropped out\u201D. Increasing dropout has the effect of making the training\
            \ process more noisy and lowering overall network capacity, but it can\
            \ be an effective regularization method to reduce overfitting and improve\
            \ generalization."
        example_value:
            - 0.2
        expected_impact: 3
        literature_references:
            - https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html
        related_parameters:
            - attention_probs_dropout_prob, hidden_dropout_prob
        suggested_values: 0.05 - 0.8
        suggested_values_reasoning:
            Tuning dropout is really something to be done
            when all of the big choices about architecture have been settled. Consider
            starting with 0.5 and adjusting the dropout depending on observed model
            performance.
        ui_display_name: classifier_dropout
    gradient_checkpointing:
        ui_display_name: null
    hidden_act:
        default_value_reasoning: Taken from huggingface.
        description_implications:
            Changing this activation function will only affect
            the feed-forward layers of the transformer.
        example_value:
            - relu
        expected_impact: 2
        literature_references:
            - "[Relevant StackOverflow discussion](https://ai.stackexchange.com/questions/30341/why-does-a-transformer-not-use-an-activation-function-following-the-multi-head-a)"
        suggested_values: gelu
        suggested_values_reasoning: Taken from huggingface defaults.
        ui_display_name: Hidden Layer Activation
    hidden_dropout_prob:
        default_value_reasoning: Huggingface default.
        description_implications:
            "Dropout is a computationally cheap regularization\
            \ method where during training, some neurons are randomly ignored or \u201C\
            dropped out\u201D. Increasing dropout has the effect of making the training\
            \ process more noisy and lowering overall network capacity, but it can\
            \ be an effective regularization method to reduce overfitting and improve\
            \ generalization."
        example_value:
            - 0.2
        expected_impact: 3
        literature_references:
            - https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html
        related_parameters:
            - "attention_probs_dropout_prob, \nclassifier_dropout"
        suggested_values: 0.05 - 0.8
        suggested_values_reasoning:
            Tuning dropout is really something to be done
            when all of the big choices about architecture have been settled. Consider
            starting with 0.5 and adjusting the dropout depending on observed model
            performance.
        ui_display_name: hidden_dropout_prob
    hidden_size:
        default_value_reasoning: Huggingface default.
        description_implications:
            Increasing the hidden size makes the model larger
            and slower to train, increases the model's capacity to capture more complexity.
            It also increases the chance of overfitting.
        expected_impact: 2
        suggested_values: 10 - 2048
        suggested_values_reasoning:
            Increasing the hidden size makes sense if the
            model is underfitting. It's useful to train both smaller and larger models
            to see how model capacity affects performance. This should only be explored
            after the architecture of the model has been settled.
        ui_display_name: Hidden Size
    initializer_range:
        description_implications:
            There is an ideal value for this variable that doesn't
            lead to the outputs of these matrices to vanish or explode
        example_value:
            - 0.02
        expected_impact: 3
        other_information: Must be greater than 0
        related_parameters:
            - weights_initializer
        suggested_values: 0.01-0.05
        suggested_values_reasoning:
            Large values will likely lead to very large outputs.
            Small values will lead to vanishing outputs.
        ui_display_name: null
    intermediate_size:
        ui_display_name: null
    layer_norm_eps:
        ui_display_name: null
    max_position_embeddings:
        default_value_reasoning: Taken from huggingface.
        description_implications:
            "An embedding is a relatively low-dimensional space
            that is used to translate high-dimensional vectors like words or positions,
            which can have a large vocbulary size. Ideally, after an embedding is
            trained, it captures some of the semantics of the input by placing semantically
            similar inputs close together in the embedding space.


            Increasing the embedding size may cause the model to train more slowly,
            but the higher dimensionality can also improve overall quality."
        expected_impact: 2
        suggested_values: 512
        suggested_values_reasoning:
            Out of the box value based on published literature.
            Try models with smaller or larger embedding sizes to observe relative
            impact.
        ui_display_name: Max Position Embeddings
    max_sequence_length:
        default_value_reasoning:
            Sets the maximum sequence length of the expected
            inputs, so input/output shapes are computed accurately.
        expected_impact: 1
        ui_display_name: null
    num_attention_heads:
        ui_display_name: null
    num_hidden_layers:
        ui_display_name: null
    pad_token_id:
        ui_display_name: null
    position_embedding_type:
        ui_display_name: null
    pretrained_kwargs:
        ui_display_name: null
    pretrained_model_name_or_path:
        ui_display_name: null
    reduce_output:
        ui_display_name: null
    saved_weights_in_checkpoint:
        default_value_reasoning:
            The weights of the encoder are not necessarily saved
            in the checkpoint. The user has to save them first.
        description_implications:
            The memory footprint for some of these encoders
            can be large.
        expected_impact: 1
        related_parameters:
            - skip_save_model
        suggested_values:
            - false
        suggested_values_reasoning:
            Some of these encoders are large, so it might
            be better to load them as needed, especially if 1. they're not used frequently
            2. the user doesn't have a lot of storage.
        ui_display_name: null
    trainable:
        expected_impact: 2
        ui_display_name: null
    type_vocab_size:
        ui_display_name: null
    use_pretrained:
        ui_display_name: null
    vocab:
        default_value_reasoning:
            Computed and passed along internally according to
            preprocessing settings.
        example_value:
            - a
            - b
            - c
        expected_impact: 2
        internal_only: true
        ui_display_name: Not Displayed
    vocab_size:
        internal_only: true
        ui_display_name: Not displayed
CategoricalEmbed:
    type:
        short_description:
            Maps the categorical feature to a dense embedding.
        long_description:
            The dense encoder maps to a dense embedding and is returned as outputs of size `b x h`,
            where `b` is the batch size and `h` is the dimensionality of the embeddings.
    dropout:
        default_value_reasoning:
            Dropout can cause training to become less stable.
            Consider start with a dropout-free baseline, and add dropout gradually
            in subsequent experiments.
        description_implications:
            "Dropout is a computationally cheap regularization\
            \ method where during training, some neurons are randomly ignored or \u201C\
            dropped out\u201D. Increasing dropout has the effect of making the training\
            \ process more noisy and lowering overall network capacity, but it can\
            \ be an effective regularization method to reduce overfitting and improve\
            \ generalization."
        example_value:
            - 0.2
        expected_impact: 3
        literature_references:
            - https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html
        suggested_values: 0.05 - 0.8
        suggested_values_reasoning:
            Tuning dropout is really something to be done
            when all of the big choices about architecture have been settled. Consider
            starting with 0.5 and adjusting the dropout depending on observed model
            performance.
        ui_display_name: Dropout
    embedding_initializer:
        default_value_reasoning:
            According to https://arxiv.org/abs/1711.09160, choice
            of embedding initialization is not important as long as the variance is
            kept reasonably low.
        description_implications:
            According to https://arxiv.org/abs/1711.09160, choice
            of embedding initialization is not important as long as the variance is
            kept reasonably low.
        example_value:
            - kaiming
        expected_impact: 1
        literature_references:
            - https://arxiv.org/abs/1711.09160
        suggested_values: kaiming
        suggested_values_reasoning: https://discuss.huggingface.co/t/state-of-the-art-technique-for-initializing-embedding-matrix/326
        ui_display_name: Embedding Initialization
    embedding_size:
        default_value_reasoning: Not too big, not too small.
        description_implications:
            'An embedding is a relatively low-dimensional space
            that is used to translate high-dimensional vectors like words, which can
            have a large vocbulary size. Ideally, after an embedding is trained, it
            captures some of the semantics of the input by placing semantically similar
            inputs close together in the embedding space.


            In most cases, the embedding size is chosen empirically, by trial and
            error. From https://www.amazon.com/dp/1098115783, "one rule of thumb is
            to use the fourth root of the total number of unique categorical elements
            while another is that the embedding dimension should be approximately
            1.6 times the square root of the number of unique elements in the category,
            and no less than 600."


            Increasing the embedding size may cause the model to train more slowly,
            but the higher dimensionality can also improve overall quality.'
        expected_impact: 2
        literature_references:
            - https://developers.google.com/machine-learning/crash-course/embeddings/video-lecture
        suggested_values: 1.6 * sqrt(vocab_size)
        suggested_values_reasoning:
            Rule of thumb suggested by a deep learning textbook.
            Try models with smaller or larger embedding sizes to observe relative
            impact.
        ui_display_name: Embedding Size
    embeddings_on_cpu:
        default_value_reasoning:
            By default embeddings matrices are stored on GPU
            memory if a GPU is used, as it allows for faster access.
        description_implications:
            By default embeddings matrices are stored on GPU
            memory if a GPU is used, as it allows for faster access. However, in some
            cases when the vocabulary size is very large, the full embedding matrix
            may be really big and unwieldy to have in GPU memory. This parameter forces
            the placement of the embedding matrix in regular memory and the CPU is
            used to access them. This may slow down training due to additional data
            transfer between CPU and GPU memory, but can lead to healthier GPU memory
            resource usage.
        expected_impact: 1
        suggested_values:
            - false
        suggested_values_reasoning:
            If GPU memory is not a constraint, having embeddings
            stored and accessed within the GPU is faster.
        ui_display_name: Embeddings on CPU
    embeddings_trainable:
        ui_display_name: null
    pretrained_embeddings:
        ui_display_name: null
    vocab:
        default_value_reasoning:
            Computed and passed along internally according to
            preprocessing settings.
        example_value:
            - a
            - b
            - c
        expected_impact: 2
        internal_only: true
        ui_display_name: Not Displayed
CategoricalSparse:
    type:
        short_description:
            Maps the categorical feature to a sparse embedding.
        long_description:
            The sparse encoder maps to a sparse embedding (one-hot encodings) and is returned as outputs of
            size `b x h`, where `b` is the batch size and `h` is the dimensionality of the embeddings.
    dropout:
        default_value_reasoning:
            Dropout can cause training to become less stable.
            Consider start with a dropout-free baseline, and add dropout gradually
            in subsequent experiments.
        description_implications:
            "Dropout is a computationally cheap regularization\
            \ method where during training, some neurons are randomly ignored or \u201C\
            dropped out\u201D. Increasing dropout has the effect of making the training\
            \ process more noisy and lowering overall network capacity, but it can\
            \ be an effective regularization method to reduce overfitting and improve\
            \ generalization."
        example_value:
            - 0.2
        expected_impact: 3
        literature_references:
            - https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html
        suggested_values: 0.05 - 0.8
        suggested_values_reasoning:
            Tuning dropout is really something to be done
            when all of the big choices about architecture have been settled. Consider
            starting with 0.5 and adjusting the dropout depending on observed model
            performance.
        ui_display_name: Dropout
    embedding_initializer:
        default_value_reasoning:
            According to https://arxiv.org/abs/1711.09160, choice
            of embedding initialization is not important as long as the variance is
            kept reasonably low.
        description_implications:
            According to https://arxiv.org/abs/1711.09160, choice
            of embedding initialization is not important as long as the variance is
            kept reasonably low.
        example_value:
            - kaiming
        expected_impact: 1
        literature_references:
            - https://arxiv.org/abs/1711.09161
        suggested_values: kaiming
        suggested_values_reasoning: https://discuss.huggingface.co/t/state-of-the-art-technique-for-initializing-embedding-matrix/327
        ui_display_name: Embedding Initialization
    embedding_size:
        default_value_reasoning: Not too big, not too small.
        description_implications:
            'An embedding is a relatively low-dimensional space
            that is used to translate high-dimensional vectors like words, which can
            have a large vocbulary size. Ideally, after an embedding is trained, it
            captures some of the semantics of the input by placing semantically similar
            inputs close together in the embedding space.


            In most cases, the embedding size is chosen empirically, by trial and
            error. From https://www.amazon.com/dp/1098115783, "one rule of thumb is
            to use the fourth root of the total number of unique categorical elements
            while another is that the embedding dimension should be approximately
            1.6 times the square root of the number of unique elements in the category,
            and no less than 600."


            Increasing the embedding size may cause the model to train more slowly,
            but the higher dimensionality can also improve overall quality.'
        expected_impact: 2
        literature_references:
            - https://developers.google.com/machine-learning/crash-course/embeddings/video-lecture
        suggested_values: 1.6 * sqrt(vocab_size)
        suggested_values_reasoning:
            Rule of thumb suggested by a deep learning textbook.
            Try models with smaller or larger embedding sizes to observe relative
            impact.
        ui_display_name: Embedding Size
    embeddings_on_cpu:
        default_value_reasoning:
            By default embeddings matrices are stored on GPU
            memory if a GPU is used, as it allows for faster access.
        description_implications:
            By default embeddings matrices are stored on GPU
            memory if a GPU is used, as it allows for faster access. However, in some
            cases when the vocabulary size is very large, the full embedding matrix
            may be really big and unwieldy to have in GPU memory. This parameter forces
            the placement of the embedding matrix in regular memory and the CPU is
            used to access them. This may slow down training due to additional data
            transfer between CPU and GPU memory, but can lead to healthier GPU memory
            resource usage.
        expected_impact: 1
        suggested_values:
            - false
        suggested_values_reasoning:
            If GPU memory is not a constraint, having embeddings
            stored and accessed within the GPU is faster.
        ui_display_name: Embeddings on CPU
    embeddings_trainable:
        ui_display_name: null
    pretrained_embeddings:
        ui_display_name: null
    vocab:
        default_value_reasoning:
            Computed and passed along internally according to
            preprocessing settings.
        example_value:
            - a
            - b
            - c
        expected_impact: 2
        internal_only: true
        ui_display_name: Not Displayed
DateEmbed:
    type:
        short_description:
            Embeds the date elements passes them through fully connected layers.
        long_description:
            The Embed encoder passes the year through a fully connected layer of one neuron and embeds all
            other elements for the date, concatenates them and passes the concatenated representation
            through fully connected layers.
    activation:
        default_value_reasoning:
            The Rectified Linear Units (ReLU) function is the
            standard activation function used for adding non-linearity. It is simple,
            fast, and empirically works well (https://arxiv.org/abs/1803.08375).
        description_implications:
            Changing the activation functions has an impact
            on the computational load of the model and might require further hypterparameter
            tuning
        expected_impact: 1
        suggested_values:
            The default value will work well in the majority of the
            cases
        ui_display_name: Activation
    bias_initializer:
        default_value_reasoning:
            It is possible and common to initialize the biases
            to be zero, since the asymmetry breaking is provided by the small random
            numbers in the weights.
        description_implications:
            It's rare to see any performance gains from choosing
            a different bias initialization. Some practitioners like to use a small
            constant value such as 0.01 for all biases to ensure that all ReLU units
            are activated in the beginning and have some effect on the gradient. However,
            it's still an open question as to whether this provides consistent improvement.
        expected_impact: 1
        literature_references:
            - https://cs231n.github.io/neural-networks-2/
        related_parameters:
            - weights_initializer
        suggested_values: zeros
        suggested_values_reasoning:
            It is possible and common to initialize the biases
            to be zero, since the asymmetry breaking is provided by the small random
            numbers in the weights. For ReLU non-linearities, some people like to
            use small constant value such as 0.01 for all biases because this ensures
            that all ReLU units fire in the beginning and therefore obtain and propagate
            some gradient. However, it is not clear if this provides a consistent
            improvement (in fact some results seem to indicate that this performs
            worse) and it is more common to simply use 0 bias initialization.
        ui_display_name: Bias Initializer
    dropout:
        default_value_reasoning:
            Dropout can cause training to become less stable.
            Consider start with a dropout-free baseline, and add dropout gradually
            in subsequent experiments.
        description_implications:
            "Dropout is a computationally cheap regularization\
            \ method where during training, some neurons are randomly ignored or \u201C\
            dropped out\u201D. Increasing dropout has the effect of making the training\
            \ process more noisy and lowering overall network capacity, but it can\
            \ be an effective regularization method to reduce overfitting and improve\
            \ generalization."
        example_value:
            - 0.2
        expected_impact: 3
        literature_references:
            - https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html
        suggested_values: 0.05 - 0.8
        suggested_values_reasoning:
            Tuning dropout is really something to be done
            when all of the big choices about architecture have been settled. Consider
            starting with 0.5 and adjusting the dropout depending on observed model
            performance.
        ui_display_name: Dropout
    embedding_size:
        default_value_reasoning: Not too big, not too small.
        description_implications:
            'An embedding is a relatively low-dimensional space
            that is used to translate high-dimensional vectors like words, which can
            have a large vocbulary size. Ideally, after an embedding is trained, it
            captures some of the semantics of the input by placing semantically similar
            inputs close together in the embedding space.


            In most cases, the embedding size is chosen empirically, by trial and
            error. From https://www.amazon.com/dp/1098115783, "one rule of thumb is
            to use the fourth root of the total number of unique categorical elements
            while another is that the embedding dimension should be approximately
            1.6 times the square root of the number of unique elements in the category,
            and no less than 600."


            Increasing the embedding size may cause the model to train more slowly,
            but the higher dimensionality can also improve overall quality.'
        expected_impact: 2
        literature_references:
            - https://developers.google.com/machine-learning/crash-course/embeddings/video-lecture
        suggested_values: 1.6 * sqrt(vocab_size)
        suggested_values_reasoning:
            Rule of thumb suggested by a deep learning textbook.
            Try models with smaller or larger embedding sizes to observe relative
            impact.
        ui_display_name: Embedding Size
    embeddings_on_cpu:
        default_value_reasoning:
            By default embeddings matrices are stored on GPU
            memory if a GPU is used, as it allows for faster access.
        description_implications:
            By default embeddings matrices are stored on GPU
            memory if a GPU is used, as it allows for faster access. However, in some
            cases when the vocabulary size is very large, the full embedding matrix
            may be really big and unwieldy to have in GPU memory. This parameter forces
            the placement of the embedding matrix in regular memory and the CPU is
            used to access them. This may slow down training due to additional data
            transfer between CPU and GPU memory, but can lead to healthier GPU memory
            resource usage.
        expected_impact: 1
        suggested_values:
            - false
        suggested_values_reasoning:
            If GPU memory is not a constraint, having embeddings
            stored and accessed within the GPU is faster.
        ui_display_name: Embeddings on CPU
    fc_layers:
        default_value_reasoning:
            By default the stack is built by using num_fc_layers,
            output_size, use_bias, weights_initializer, bias_initializer, norm, norm_params,
            activation, dropout. When a list of dictionaries is provided, the stack
            is built following the parameters of each dict for building each layer.
        description_implications:
            The more layers that are specified the deeper and
            higher capacity the model will be. This makes it possible to potentially
            achieve better performance when a big anough amount of data is provided,
            but also makes the model more computationally expensive and potentially
            more prone to overfitting.
        example_value:
            - dropout: 0.1
              output_size: 128
            - norm: layer
              output_size: 64
        expected_impact: 1
        related_parameters:
            - output_size
            - use_bias
            - weights_initializer
            - bias_initializer
            - norm
            - norm_params
            - activation
            - dropout
        suggested_values_reasoning:
            It is easier to define a stack of fully connected
            layers by just specifying num_fc_layers, output_size and the other individual
            parameters. It will create a stack of layers with identical properties.
            Use this parameter only if you need a fine grained level of control of
            each individual layer in the stack.
        ui_display_name: Fully Connected Layers
    norm:
        default_value_reasoning:
            While batch normalization and layer normalization
            usually lead to improvements, it can be useful to start with fewer bells
            and whistles.
        description_implications:
            Normalization helps stabilize the learning process
            and can have a regularizing effect that can help with generalization.
            It's often suggested that with normalization, you can use a higher learning
            rate.
        example_value:
            - batch
        expected_impact: 3
        literature_references:
            - https://machinelearningmastery.com/batch-normalization-for-training-of-deep-neural-networks/
        related_parameters:
            - norm_params
        suggested_values: '"batch" or "layer"'
        suggested_values_reasoning:
            Normalization tries to solve "internal covariate
            shift" that comes from the changing distributions of the inputs to layers
            deep in the network when weights are updated. For example, batch normalization
            standardizes the inputs to a layer for each mini-batch. Try out different
            normalizations to see if that helps with training stability
        ui_display_name: Normalization Type
    norm_params:
        default_value_reasoning:
            The default parameters that come with Torch's implementation
            of these normalization types are a trusted starting point.
        description_implications:
            There are a variety of ways a certain set of parameters
            specificed could influence performance here. Broadly speaking the different
            values passed in here allow for different levels of smoothness to be observed
            in the learning curves. Since setting this parameters depends on the type
            of `norm` set, see [BatchNorm2d](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html)
            for more information on the parameters to set for batch normalization,
            and see [LayerNorm](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html)
            for more information on the parameters to set for layer normalization.
        example_value:
            - affine: false
              momentum: 0.2
              num_features: 100
        expected_impact: 1
        literature_references:
            - "For BatchNorm2d: https://arxiv.org/abs/1502.03167


              For LayerNorm: https://arxiv.org/abs/1607.06450"
        related_parameters:
            - "`norm`"
        suggested_values: Depends on the type of `norm` set.
        suggested_values_reasoning: "NO"
        ui_display_name: Normalization Parameters
    num_fc_layers:
        default_value_reasoning:
            The encoder already has learnable parameters.Sometimes
            the default is 1 for modules where the FC stack is used for shape management,
            or the only source of learnable parameters.
        description_implications:
            Increasing num_fc_layers will increase the capacity
            of the model. The model will be slower to train, and there's a higher
            risk of overfitting.
        example_value:
            - 1
        expected_impact: 1
        other_information:
            Not all modules that have fc_layers also have an accompanying
            num_fc_layers parameter. Where both are present, fc_layers takes precedent
            over num_fc_layers. Specifying num_fc_layers alone uses fully connected
            layers that are configured by the defaults in FCStack.
        related_parameters:
            - fc_layers
        suggested_values: 0-1
        suggested_values_reasoning:
            The full model likely contains many learnable
            parameters. Consider starting with very few, or without any additional
            fully connected layers and add them if you observe evidence of limited
            model capacity. Sometimes the default is 1 for modules where the FC stack
            is used for shape management, or the only source of learnable parameters.
        ui_display_name: Number of Fully Connected Layers
    output_size:
        default_value_reasoning: A modest value, not too small, not too large.
        description_implications:
            If there are fully connected layers in this module,
            increasing the output size of each fully connected layer will increase
            the capacity of the model. However, the model may be slower to train,
            and there's a higher risk of overfitting. If it seems like the model could
            use even more capacity, consider increasing the number of fully connected
            layers, or explore other architectures.
        expected_impact: 2
        other_information:
            If num_fc_layers=0 and fc_layers=None, and there are no
            fully connected layers defined on the module, then this parameter may
            have no effect on the module's final output shape.
        related_parameters:
            - num_fc_layers, fc_layers
        suggested_values: 10 - 1024
        suggested_values_reasoning:
            Increasing the output size increases the capacity
            of the model. If this seems to have a positive effect, then it could be
            worth increasing the number of layers, or trying a different architecture
            with a larger capacity.
        ui_display_name: Output Size
    use_bias:
        default_value_reasoning:
            "Bias terms may improve model accuracy, and don't
            have much impact in terms of memory or training speed. For most models
            it is reasonable to use bias terms.


            Batch Normalization, however, adds a trainable shift parameter which is
            added to the activation. When Batch Normalization is used in a layer,
            bias terms are redundant and may be removed."
        description_implications:
            Bias terms may improve model accuracy, and don't
            have much impact in terms of memory or training speed. For most models
            it is reasonable to leave this parameter set to True.
        example_value:
            - true
        expected_impact: 1
        other_information:
            If fc_layers is not specified, or use_bias is not specified
            for individual layers, the value of use_bias will be used as the default
            for all layers.
        related_parameters:
            - bias_initializer, fc_layers
        suggested_values:
            - true
        ui_display_name: Use Bias
    weights_initializer:
        default_value_reasoning: Taken from [this paper](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf).
        description_implications:
            The method you choose to initialize layer weights
            during training can have a big impact on performance as well as the reproducibility
            of your final model between runs. As an example, if you were to randomly
            initialize weights you would risk non-reproducibility (and possibly general
            training performance), but sticking with constant values for initialization
            might significantly increase the time needed for model convergence. Generally,
            choosing one of the probabilistic approaches strikes a balance between
            the two extremes, and the literature kicked off by the landmark [*Xavier
            et al.* paper](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)
            provides a few good options. See this nice discussion from [Weights and
            Biases](https://wandb.ai/site/articles/the-effects-of-weight-initialization-on-neural-nets#:~:text=Studies%20have%20shown%20that%20initializing,net%20train%20better%20and%20faster.)
            for more information.
        expected_impact: 3
        literature_references:
            - "Weights and Biases blog post: https://wandb.ai/site/articles/the-effects-of-weight-initialization-on-neural-nets#:~:text=Studies%20have%20shown%20that%20initializing,net%20train%20better%20and%20faster.


              Xavier et al. paper: http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf"
        suggested_values: xavier_uniform
        suggested_values_reasoning:
            Changing the weights initialization scheme is
            something to consider if a model is having trouble with convergence, or
            otherwise it is something to experiment with after other factors are considered.
            The default choice (`xavier_uniform`) is a suitable starting point for
            most tasks.
        ui_display_name: Layer Weights Initializer
DateWave:
    type:
        short_description:
            Embeds the date elements by taking the cosine of their value before passing through fully connected layers.
        long_description:
            The Wave encoder passes the year through a fully connected layer of one neuron and represents
            all other elements for the date by taking the cosine of their value with a different period (12
            for months, 31 for days, etc.), concatenates them and passes the concatenated representation
            through fully connected layers.
    activation:
        default_value_reasoning:
            The Rectified Linear Units (ReLU) function is the
            standard activation function used for adding non-linearity. It is simple,
            fast, and empirically works well (https://arxiv.org/abs/1803.08375).
        description_implications:
            Changing the activation functions has an impact
            on the computational load of the model and might require further hypterparameter
            tuning
        expected_impact: 1
        suggested_values:
            The default value will work well in the majority of the
            cases
        ui_display_name: Activation
    bias_initializer:
        default_value_reasoning:
            It is possible and common to initialize the biases
            to be zero, since the asymmetry breaking is provided by the small random
            numbers in the weights.
        description_implications:
            It's rare to see any performance gains from choosing
            a different bias initialization. Some practitioners like to use a small
            constant value such as 0.01 for all biases to ensure that all ReLU units
            are activated in the beginning and have some effect on the gradient. However,
            it's still an open question as to whether this provides consistent improvement.
        expected_impact: 1
        literature_references:
            - https://cs231n.github.io/neural-networks-2/
        related_parameters:
            - weights_initializer
        suggested_values: zeros
        suggested_values_reasoning:
            It is possible and common to initialize the biases
            to be zero, since the asymmetry breaking is provided by the small random
            numbers in the weights. For ReLU non-linearities, some people like to
            use small constant value such as 0.01 for all biases because this ensures
            that all ReLU units fire in the beginning and therefore obtain and propagate
            some gradient. However, it is not clear if this provides a consistent
            improvement (in fact some results seem to indicate that this performs
            worse) and it is more common to simply use 0 bias initialization.
        ui_display_name: Bias Initializer
    dropout:
        default_value_reasoning:
            Dropout can cause training to become less stable.
            Consider start with a dropout-free baseline, and add dropout gradually
            in subsequent experiments.
        description_implications:
            "Dropout is a computationally cheap regularization\
            \ method where during training, some neurons are randomly ignored or \u201C\
            dropped out\u201D. Increasing dropout has the effect of making the training\
            \ process more noisy and lowering overall network capacity, but it can\
            \ be an effective regularization method to reduce overfitting and improve\
            \ generalization."
        example_value:
            - 0.2
        expected_impact: 3
        literature_references:
            - https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html
        suggested_values: 0.05 - 0.8
        suggested_values_reasoning:
            Tuning dropout is really something to be done
            when all of the big choices about architecture have been settled. Consider
            starting with 0.5 and adjusting the dropout depending on observed model
            performance.
        ui_display_name: Dropout
    fc_layers:
        default_value_reasoning:
            By default the stack is built by using num_fc_layers,
            output_size, use_bias, weights_initializer, bias_initializer, norm, norm_params,
            activation, dropout. When a list of dictionaries is provided, the stack
            is built following the parameters of each dict for building each layer.
        description_implications:
            The more layers that are specified the deeper and
            higher capacity the model will be. This makes it possible to potentially
            achieve better performance when a big anough amount of data is provided,
            but also makes the model more computationally expensive and potentially
            more prone to overfitting.
        example_value:
            - dropout: 0.1
              output_size: 128
            - norm: layer
              output_size: 64
        expected_impact: 1
        related_parameters:
            - output_size
            - use_bias
            - weights_initializer
            - bias_initializer
            - norm
            - norm_params
            - activation
            - dropout
        suggested_values_reasoning:
            It is easier to define a stack of fully connected
            layers by just specifying num_fc_layers, output_size and the other individual
            parameters. It will create a stack of layers with identical properties.
            Use this parameter only if you need a fine grained level of control of
            each individual layer in the stack.
        ui_display_name: Fully Connected Layers
    norm:
        default_value_reasoning:
            While batch normalization and layer normalization
            usually lead to improvements, it can be useful to start with fewer bells
            and whistles.
        description_implications:
            Normalization helps stabilize the learning process
            and can have a regularizing effect that can help with generalization.
            It's often suggested that with normalization, you can use a higher learning
            rate.
        example_value:
            - batch
        expected_impact: 3
        literature_references:
            - https://machinelearningmastery.com/batch-normalization-for-training-of-deep-neural-networks/
        related_parameters:
            - norm_params
        suggested_values: '"batch" or "layer"'
        suggested_values_reasoning:
            Normalization tries to solve "internal covariate
            shift" that comes from the changing distributions of the inputs to layers
            deep in the network when weights are updated. For example, batch normalization
            standardizes the inputs to a layer for each mini-batch. Try out different
            normalizations to see if that helps with training stability
        ui_display_name: Normalization Type
    norm_params:
        default_value_reasoning:
            The default parameters that come with Torch's implementation
            of these normalization types are a trusted starting point.
        description_implications:
            There are a variety of ways a certain set of parameters
            specificed could influence performance here. Broadly speaking the different
            values passed in here allow for different levels of smoothness to be observed
            in the learning curves. Since setting this parameters depends on the type
            of `norm` set, see [BatchNorm2d](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html)
            for more information on the parameters to set for batch normalization,
            and see [LayerNorm](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html)
            for more information on the parameters to set for layer normalization.
        example_value:
            - affine: false
              momentum: 0.2
              num_features: 100
        expected_impact: 1
        literature_references:
            - "For BatchNorm2d: https://arxiv.org/abs/1502.03167


              For LayerNorm: https://arxiv.org/abs/1607.06450"
        related_parameters:
            - "`norm`"
        suggested_values: Depends on the type of `norm` set.
        suggested_values_reasoning: "NO"
        ui_display_name: Normalization Parameters
    num_fc_layers:
        default_value_reasoning:
            The encoder already has learnable parameters.Sometimes
            the default is 1 for modules where the FC stack is used for shape management,
            or the only source of learnable parameters.
        description_implications:
            Increasing num_fc_layers will increase the capacity
            of the model. The model will be slower to train, and there's a higher
            risk of overfitting.
        example_value:
            - 1
        expected_impact: 1
        other_information:
            Not all modules that have fc_layers also have an accompanying
            num_fc_layers parameter. Where both are present, fc_layers takes precedent
            over num_fc_layers. Specifying num_fc_layers alone uses fully connected
            layers that are configured by the defaults in FCStack.
        related_parameters:
            - fc_layers
        suggested_values: 0-1
        suggested_values_reasoning:
            The full model likely contains many learnable
            parameters. Consider starting with very few, or without any additional
            fully connected layers and add them if you observe evidence of limited
            model capacity. Sometimes the default is 1 for modules where the FC stack
            is used for shape management, or the only source of learnable parameters.
        ui_display_name: Number of Fully Connected Layers
    output_size:
        default_value_reasoning: A modest value, not too small, not too large.
        description_implications:
            If there are fully connected layers in this module,
            increasing the output size of each fully connected layer will increase
            the capacity of the model. However, the model may be slower to train,
            and there's a higher risk of overfitting. If it seems like the model could
            use even more capacity, consider increasing the number of fully connected
            layers, or explore other architectures.
        expected_impact: 2
        other_information:
            If num_fc_layers=0 and fc_layers=None, and there are no
            fully connected layers defined on the module, then this parameter may
            have no effect on the module's final output shape.
        related_parameters:
            - num_fc_layers, fc_layers
        suggested_values: 10 - 1024
        suggested_values_reasoning:
            Increasing the output size increases the capacity
            of the model. If this seems to have a positive effect, then it could be
            worth increasing the number of layers, or trying a different architecture
            with a larger capacity.
        ui_display_name: Output Size
    use_bias:
        default_value_reasoning:
            "Bias terms may improve model accuracy, and don't
            have much impact in terms of memory or training speed. For most models
            it is reasonable to use bias terms.


            Batch Normalization, however, adds a trainable shift parameter which is
            added to the activation. When Batch Normalization is used in a layer,
            bias terms are redundant and may be removed."
        description_implications:
            Bias terms may improve model accuracy, and don't
            have much impact in terms of memory or training speed. For most models
            it is reasonable to leave this parameter set to True.
        example_value:
            - true
        expected_impact: 1
        other_information:
            If fc_layers is not specified, or use_bias is not specified
            for individual layers, the value of use_bias will be used as the default
            for all layers.
        related_parameters:
            - bias_initializer, fc_layers
        suggested_values:
            - true
        ui_display_name: Use Bias
    weights_initializer:
        default_value_reasoning: Taken from [this paper](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf).
        description_implications:
            The method you choose to initialize layer weights
            during training can have a big impact on performance as well as the reproducibility
            of your final model between runs. As an example, if you were to randomly
            initialize weights you would risk non-reproducibility (and possibly general
            training performance), but sticking with constant values for initialization
            might significantly increase the time needed for model convergence. Generally,
            choosing one of the probabilistic approaches strikes a balance between
            the two extremes, and the literature kicked off by the landmark [*Xavier
            et al.* paper](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)
            provides a few good options. See this nice discussion from [Weights and
            Biases](https://wandb.ai/site/articles/the-effects-of-weight-initialization-on-neural-nets#:~:text=Studies%20have%20shown%20that%20initializing,net%20train%20better%20and%20faster.)
            for more information.
        expected_impact: 3
        literature_references:
            - "Weights and Biases blog post: https://wandb.ai/site/articles/the-effects-of-weight-initialization-on-neural-nets#:~:text=Studies%20have%20shown%20that%20initializing,net%20train%20better%20and%20faster.


              Xavier et al. paper: http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf"
        suggested_values: xavier_uniform
        suggested_values_reasoning:
            Changing the weights initialization scheme is
            something to consider if a model is having trouble with convergence, or
            otherwise it is something to experiment with after other factors are considered.
            The default choice (`xavier_uniform`) is a suitable starting point for
            most tasks.
        ui_display_name: Layer Weights Initializer
DenseEncoder:
    type:
        short_description:
            Passes the raw numerical values through fully connected layers.
        long_description:
            The dense encoder passes the raw numerical values through fully connected layers. In this case
            the inputs of size `b` are transformed to size `b x h`.
    activation:
        default_value_reasoning:
            The Rectified Linear Units (ReLU) function is the
            standard activation function used for adding non-linearity. It is simple,
            fast, and empirically works well (https://arxiv.org/abs/1803.08375).
        description_implications:
            Changing the activation functions has an impact
            on the computational load of the model and might require further hypterparameter
            tuning
        expected_impact: 1
        suggested_values:
            The default value will work well in the majority of the
            cases
        ui_display_name: Activation
    bias_initializer:
        default_value_reasoning:
            It is possible and common to initialize the biases
            to be zero, since the asymmetry breaking is provided by the small random
            numbers in the weights.
        description_implications:
            It's rare to see any performance gains from choosing
            a different bias initialization. Some practitioners like to use a small
            constant value such as 0.01 for all biases to ensure that all ReLU units
            are activated in the beginning and have some effect on the gradient. However,
            it's still an open question as to whether this provides consistent improvement.
        expected_impact: 1
        literature_references:
            - https://cs231n.github.io/neural-networks-2/
        related_parameters:
            - weights_initializer
        suggested_values: zeros
        suggested_values_reasoning:
            It is possible and common to initialize the biases
            to be zero, since the asymmetry breaking is provided by the small random
            numbers in the weights. For ReLU non-linearities, some people like to
            use small constant value such as 0.01 for all biases because this ensures
            that all ReLU units fire in the beginning and therefore obtain and propagate
            some gradient. However, it is not clear if this provides a consistent
            improvement (in fact some results seem to indicate that this performs
            worse) and it is more common to simply use 0 bias initialization.
        ui_display_name: Bias Initializer
    dropout:
        default_value_reasoning:
            Dropout can cause training to become less stable.
            Consider start with a dropout-free baseline, and add dropout gradually
            in subsequent experiments.
        description_implications:
            "Dropout is a computationally cheap regularization\
            \ method where during training, some neurons are randomly ignored or \u201C\
            dropped out\u201D. Increasing dropout has the effect of making the training\
            \ process more noisy and lowering overall network capacity, but it can\
            \ be an effective regularization method to reduce overfitting and improve\
            \ generalization."
        example_value:
            - 0.2
        expected_impact: 3
        literature_references:
            - https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html
        suggested_values: 0.05 - 0.8
        suggested_values_reasoning:
            Tuning dropout is really something to be done
            when all of the big choices about architecture have been settled. Consider
            starting with 0.5 and adjusting the dropout depending on observed model
            performance.
        ui_display_name: Dropout
    input_size:
        other_information: Internal Only
        related_parameters:
            - "No"
        ui_display_name: Not Displayed
    layers:
        ui_display_name: null
    norm:
        default_value_reasoning:
            While batch normalization and layer normalization
            usually lead to improvements, it can be useful to start with fewer bells
            and whistles.
        description_implications:
            Normalization helps stabilize the learning process
            and can have a regularizing effect that can help with generalization.
            It's often suggested that with normalization, you can use a higher learning
            rate.
        example_value:
            - batch
        expected_impact: 3
        literature_references:
            - https://machinelearningmastery.com/batch-normalization-for-training-of-deep-neural-networks/
        related_parameters:
            - norm_params
        suggested_values: '"batch" or "layer"'
        suggested_values_reasoning:
            Normalization tries to solve "internal covariate
            shift" that comes from the changing distributions of the inputs to layers
            deep in the network when weights are updated. For example, batch normalization
            standardizes the inputs to a layer for each mini-batch. Try out different
            normalizations to see if that helps with training stability
        ui_display_name: Normalization Type
    norm_params:
        default_value_reasoning:
            The default parameters that come with Torch's implementation
            of these normalization types are a trusted starting point.
        description_implications:
            There are a variety of ways a certain set of parameters
            specificed could influence performance here. Broadly speaking the different
            values passed in here allow for different levels of smoothness to be observed
            in the learning curves. Since setting this parameters depends on the type
            of `norm` set, see [BatchNorm2d](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html)
            for more information on the parameters to set for batch normalization,
            and see [LayerNorm](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html)
            for more information on the parameters to set for layer normalization.
        example_value:
            - affine: false
              momentum: 0.2
              num_features: 100
        expected_impact: 1
        literature_references:
            - "For BatchNorm2d: https://arxiv.org/abs/1502.03167


              For LayerNorm: https://arxiv.org/abs/1607.06450"
        related_parameters:
            - "`norm`"
        suggested_values: Depends on the type of `norm` set.
        suggested_values_reasoning: "NO"
        ui_display_name: Normalization Parameters
    num_layers:
        default_value_reasoning:
            The ideal number of layers depends on the data. For
            many data types, one layer is sufficient.
        description_implications:
            "Increasing the number of layers may improve model
            performance by allowing the model to synthesize learned features derived
            from the original input. If the input is simple, ex. a category with a
            few options, increasing the number of layers has no benefit. For more
            complex inputs, additional layers add more 'processing power' to extract
            useful information from the input.


            However, more layers will increase training time and may reduce accuracy
            due to overfitting."
        example_value:
            - 1
        expected_impact: 1
        other_information:
            If you have multiple input features, varying the number
            of layers in the combiner or output feature decoder will have more impact.
        related_parameters:
            - layers
        suggested_values: 1-3
        suggested_values_reasoning:
            Increasing the number of layers may improve encoder
            performance.  However, more layers will increase training time and may
            cause overfitting.  Small numbers of layers usually work best.
        ui_display_name: Number of Layers
    output_size:
        default_value_reasoning: A modest value, not too small, not too large.
        description_implications:
            If there are fully connected layers in this module,
            increasing the output size of each fully connected layer will increase
            the capacity of the model. However, the model may be slower to train,
            and there's a higher risk of overfitting. If it seems like the model could
            use even more capacity, consider increasing the number of fully connected
            layers, or explore other architectures.
        expected_impact: 2
        other_information:
            If num_fc_layers=0 and fc_layers=None, and there are no
            fully connected layers defined on the module, then this parameter may
            have no effect on the module's final output shape.
        related_parameters:
            - num_fc_layers, fc_layers
        suggested_values: 10 - 1024
        suggested_values_reasoning:
            Increasing the output size increases the capacity
            of the model. If this seems to have a positive effect, then it could be
            worth increasing the number of layers, or trying a different architecture
            with a larger capacity.
        ui_display_name: Output Size
    use_bias:
        ui_display_name: null
    weights_initializer:
        default_value_reasoning: Taken from [this paper](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf).
        description_implications:
            The method you choose to initialize layer weights
            during training can have a big impact on performance as well as the reproducibility
            of your final model between runs. As an example, if you were to randomly
            initialize weights you would risk non-reproducibility (and possibly general
            training performance), but sticking with constant values for initialization
            might significantly increase the time needed for model convergence. Generally,
            choosing one of the probabilistic approaches strikes a balance between
            the two extremes, and the literature kicked off by the landmark [*Xavier
            et al.* paper](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)
            provides a few good options. See this nice discussion from [Weights and
            Biases](https://wandb.ai/site/articles/the-effects-of-weight-initialization-on-neural-nets#:~:text=Studies%20have%20shown%20that%20initializing,net%20train%20better%20and%20faster.)
            for more information.
        expected_impact: 3
        literature_references:
            - "Weights and Biases blog post: https://wandb.ai/site/articles/the-effects-of-weight-initialization-on-neural-nets#:~:text=Studies%20have%20shown%20that%20initializing,net%20train%20better%20and%20faster.


              Xavier et al. paper: http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf"
        suggested_values: xavier_uniform
        suggested_values_reasoning:
            Changing the weights initialization scheme is
            something to consider if a model is having trouble with convergence, or
            otherwise it is something to experiment with after other factors are considered.
            The default choice (`xavier_uniform`) is a suitable starting point for
            most tasks.
        ui_display_name: Layer Weights Initializer
DistilBERT:
    type:
        short_description:
            A distilled version of BERT base that is 40% smaller and 60% faster with 95% of performance preserved.
        long_description:
            The distilbert encoder loads a pretrained DistilBERT (default distilbert-base-uncased) model
            using the Hugging Face transformers package. DistilBERT is a small, fast, cheap and light
            Transformer model trained by distilling BERT base. It has 40% less parameters than
            bert-base-uncased, runs 60% faster while preserving over 95% of BERT’s performances as measured
            on the GLUE language understanding benchmark.
    activation:
        default_value_reasoning:
            This is the default activation function used in the
            Distillbert huggingface implementation
        description_implications:
            Changing the activation functions has an impact
            on the computational load of the model and might require further hypterparameter
            tuning
        expected_impact: 1
        suggested_values:
            The default value will work well in the majority of the
            cases
        ui_display_name: Activation
    attention_dropout:
        default_value_reasoning: Huggingface default.
        description_implications:
            "Dropout is a computationally cheap regularization\
            \ method where during training, some neurons are randomly ignored or \u201C\
            dropped out\u201D. Increasing dropout has the effect of making the training\
            \ process more noisy and lowering overall network capacity, but it can\
            \ be an effective regularization method to reduce overfitting and improve\
            \ generalization."
        example_value:
            - 0.2
        expected_impact: 3
        literature_references:
            - https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html
        related_parameters:
            - dropout, qa_dropout, seq_classif_dropout
        suggested_values: 0.05 - 0.8
        suggested_values_reasoning:
            Tuning dropout is really something to be done
            when all of the big choices about architecture have been settled. Consider
            starting with 0.5 and adjusting the dropout depending on observed model
            performance.
        ui_display_name: attention_dropout
    dim:
        ui_display_name: null
    dropout:
        default_value_reasoning: Huggingface default.
        description_implications:
            "Dropout is a computationally cheap regularization\
            \ method where during training, some neurons are randomly ignored or \u201C\
            dropped out\u201D. Increasing dropout has the effect of making the training\
            \ process more noisy and lowering overall network capacity, but it can\
            \ be an effective regularization method to reduce overfitting and improve\
            \ generalization."
        example_value:
            - 0.2
        expected_impact: 3
        literature_references:
            - https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html
        related_parameters:
            - "attention_dropout,

              qa_dropout,

              seq_classif_dropout"
        suggested_values: 0.05 - 0.8
        suggested_values_reasoning:
            Tuning dropout is really something to be done
            when all of the big choices about architecture have been settled. Consider
            starting with 0.5 and adjusting the dropout depending on observed model
            performance.
        ui_display_name: dropout
    hidden_dim:
        ui_display_name: null
    initializer_range:
        description_implications:
            There is an ideal value for this variable that doesn't
            lead to the outputs of these matrices to vanish or explode
        example_value:
            - 0.02
        expected_impact: 3
        other_information: Must be greater than 0
        related_parameters:
            - weights_initializer
        suggested_values: 0.01-0.05
        suggested_values_reasoning:
            Large values will likely lead to very large outputs.
            Small values will lead to vanishing outputs.
        ui_display_name: null
    max_position_embeddings:
        default_value_reasoning: Taken from huggingface.
        description_implications:
            "An embedding is a relatively low-dimensional space
            that is used to translate high-dimensional vectors like words or positions,
            which can have a large vocbulary size. Ideally, after an embedding is
            trained, it captures some of the semantics of the input by placing semantically
            similar inputs close together in the embedding space.


            Increasing the embedding size may cause the model to train more slowly,
            but the higher dimensionality can also improve overall quality."
        expected_impact: 2
        suggested_values: 512
        suggested_values_reasoning:
            Out of the box value based on published literature.
            Try models with smaller or larger embedding sizes to observe relative
            impact.
        ui_display_name: Max Position Embeddings
    max_sequence_length:
        default_value_reasoning:
            Sets the maximum sequence length of the expected
            inputs, so input/output shapes are computed accurately.
        expected_impact: 1
        ui_display_name: null
    n_heads:
        ui_display_name: null
    n_layers:
        ui_display_name: null
    pretrained_kwargs:
        ui_display_name: null
    pretrained_model_name_or_path:
        ui_display_name: null
    qa_dropout:
        default_value_reasoning: Huggingface default.
        description_implications:
            "Dropout is a computationally cheap regularization\
            \ method where during training, some neurons are randomly ignored or \u201C\
            dropped out\u201D. Increasing dropout has the effect of making the training\
            \ process more noisy and lowering overall network capacity, but it can\
            \ be an effective regularization method to reduce overfitting and improve\
            \ generalization."
        example_value:
            - 0.2
        expected_impact: 3
        literature_references:
            - https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html
        related_parameters:
            - dropout, attention_dropout, seq_classif_dropout
        suggested_values: 0.05 - 0.8
        suggested_values_reasoning:
            Tuning dropout is really something to be done
            when all of the big choices about architecture have been settled. Consider
            starting with 0.5 and adjusting the dropout depending on observed model
            performance.
        ui_display_name: qa_dropout
    reduce_output:
        ui_display_name: null
    saved_weights_in_checkpoint:
        default_value_reasoning:
            The weights of the encoder are not necessarily saved
            in the checkpoint. The user has to save them first.
        description_implications:
            The memory footprint for some of these encoders
            can be large.
        expected_impact: 1
        related_parameters:
            - skip_save_model
        suggested_values:
            - false
        suggested_values_reasoning:
            Some of these encoders are large, so it might
            be better to load them as needed, especially if 1. they're not used frequently
            2. the user doesn't have a lot of storage.
        ui_display_name: null
    seq_classif_dropout:
        default_value_reasoning: Huggingface default.
        description_implications:
            "Dropout is a computationally cheap regularization\
            \ method where during training, some neurons are randomly ignored or \u201C\
            dropped out\u201D. Increasing dropout has the effect of making the training\
            \ process more noisy and lowering overall network capacity, but it can\
            \ be an effective regularization method to reduce overfitting and improve\
            \ generalization."
        example_value:
            - 0.2
        expected_impact: 3
        literature_references:
            - https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html
        related_parameters:
            - "dropout,

              attention_dropout,

              qa_dropout"
        suggested_values: 0.05 - 0.8
        suggested_values_reasoning:
            Tuning dropout is really something to be done
            when all of the big choices about architecture have been settled. Consider
            starting with 0.5 and adjusting the dropout depending on observed model
            performance.
        ui_display_name: seq_classif_dropout
    sinusoidal_pos_embds:
        ui_display_name: null
    trainable:
        expected_impact: 2
        ui_display_name: null
    use_pretrained:
        ui_display_name: null
    vocab:
        default_value_reasoning:
            Computed and passed along internally according to
            preprocessing settings.
        example_value:
            - a
            - b
            - c
        expected_impact: 2
        internal_only: true
        ui_display_name: Not Displayed
    vocab_size:
        internal_only: true
        ui_display_name: Not displayed
ELECTRA:
    type:
        short_description:
                Transformer encoder that can be used to encode a sequence of tokens with little compute
        long_description:
              The electra encoder loads a pretrained ELECTRA model using the Hugging Face transformers package.
              ELECTRA is a new pretraining approach which trains two transformer models the generator and the
              discriminator. The generator’s role is to replace tokens in a sequence, and is therefore trained
              as a masked language model. The discriminator, which is the model we’re interested in, tries to
              identify which tokens were replaced by the generator in the sequence.
        literature_references:
            - https://openreview.net/pdf?id=r1xMH1BtvB
    attention_probs_dropout_prob:
        default_value_reasoning: Huggingface default.
        description_implications:
            "Dropout is a computationally cheap regularization\
            \ method where during training, some neurons are randomly ignored or \u201C\
            dropped out\u201D. Increasing dropout has the effect of making the training\
            \ process more noisy and lowering overall network capacity, but it can\
            \ be an effective regularization method to reduce overfitting and improve\
            \ generalization."
        example_value:
            - 0.2
        expected_impact: 3
        literature_references:
            - https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html
        related_parameters:
            - hidden_dropout_prob, classifier_dropout
        suggested_values: 0.05 - 0.8
        suggested_values_reasoning:
            Tuning dropout is really something to be done
            when all of the big choices about architecture have been settled. Consider
            starting with 0.5 and adjusting the dropout depending on observed model
            performance.
        ui_display_name: attention_probs_dropout_prob
    classifier_dropout:
        default_value_reasoning: Huggingface default.
        description_implications:
            "Dropout is a computationally cheap regularization\
            \ method where during training, some neurons are randomly ignored or \u201C\
            dropped out\u201D. Increasing dropout has the effect of making the training\
            \ process more noisy and lowering overall network capacity, but it can\
            \ be an effective regularization method to reduce overfitting and improve\
            \ generalization."
        example_value:
            - 0.2
        expected_impact: 3
        literature_references:
            - https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html
        related_parameters:
            - hidden_dropout_prob, attention_probs_dropout_prob
        suggested_values: 0.05 - 0.8
        suggested_values_reasoning:
            Tuning dropout is really something to be done
            when all of the big choices about architecture have been settled. Consider
            starting with 0.5 and adjusting the dropout depending on observed model
            performance.
        ui_display_name: classifier_dropout
    embedding_size:
        default_value_reasoning: Not too big, not too small.
        description_implications:
            'An embedding is a relatively low-dimensional space
            that is used to translate high-dimensional vectors like words, which can
            have a large vocbulary size. Ideally, after an embedding is trained, it
            captures some of the semantics of the input by placing semantically similar
            inputs close together in the embedding space.


            In most cases, the embedding size is chosen empirically, by trial and
            error. From https://www.amazon.com/dp/1098115783, "one rule of thumb is
            to use the fourth root of the total number of unique categorical elements
            while another is that the embedding dimension should be approximately
            1.6 times the square root of the number of unique elements in the category,
            and no less than 600."


            Increasing the embedding size may cause the model to train more slowly,
            but the higher dimensionality can also improve overall quality.'
        expected_impact: 2
        literature_references:
            - https://developers.google.com/machine-learning/crash-course/embeddings/video-lecture
        suggested_values: 1.6 * sqrt(vocab_size)
        suggested_values_reasoning:
            Rule of thumb suggested by a deep learning textbook.
            Try models with smaller or larger embedding sizes to observe relative
            impact.
        ui_display_name: Embedding Size
    hidden_act:
        default_value_reasoning: Taken from huggingface.
        description_implications:
            Changing this activation function will only affect
            the feed-forward layers of the transformer.
        example_value:
            - relu
        expected_impact: 2
        literature_references:
            - "[Huggingface docs for ELECTRA config](https://huggingface.co/docs/transformers/model_doc/electra#transformers.ElectraConfig.hidden_act)


              [Relevant StackOverflow discussion](https://ai.stackexchange.com/questions/30341/why-does-a-transformer-not-use-an-activation-function-following-the-multi-head-a)"
        suggested_values: gelu
        suggested_values_reasoning: Taken from huggingface defaults.
        ui_display_name: Hidden Layer Activation
    hidden_dropout_prob:
        default_value_reasoning: Huggingface default.
        description_implications:
            "Dropout is a computationally cheap regularization\
            \ method where during training, some neurons are randomly ignored or \u201C\
            dropped out\u201D. Increasing dropout has the effect of making the training\
            \ process more noisy and lowering overall network capacity, but it can\
            \ be an effective regularization method to reduce overfitting and improve\
            \ generalization."
        example_value:
            - 0.2
        expected_impact: 3
        literature_references:
            - https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html
        related_parameters:
            - "attention_probs_dropout_prob,

              classifier_dropout"
        suggested_values: 0.05 - 0.8
        suggested_values_reasoning:
            Tuning dropout is really something to be done
            when all of the big choices about architecture have been settled. Consider
            starting with 0.5 and adjusting the dropout depending on observed model
            performance.
        ui_display_name: hidden_dropout_prob
    hidden_size:
        default_value_reasoning: Huggingface default.
        description_implications:
            Increasing the hidden size makes the model larger
            and slower to train, increases the model's capacity to capture more complexity.
            It also increases the chance of overfitting.
        expected_impact: 2
        suggested_values: 10 - 2048
        suggested_values_reasoning:
            Increasing the hidden size makes sense if the
            model is underfitting. It's useful to train both smaller and larger models
            to see how model capacity affects performance. This should only be explored
            after the architecture of the model has been settled.
        ui_display_name: Hidden Size
    initializer_range:
        description_implications:
            There is an ideal value for this variable that doesn't
            lead to the outputs of these matrices to vanish or explode
        example_value:
            - 0.02
        expected_impact: 3
        other_information: Must be greater than 0
        related_parameters:
            - weights_initializer
        suggested_values: 0.01-0.05
        suggested_values_reasoning:
            Large values will likely lead to very large outputs.
            Small values will lead to vanishing outputs.
        ui_display_name: null
    intermediate_size:
        ui_display_name: null
    layer_norm_eps:
        ui_display_name: null
    max_position_embeddings:
        default_value_reasoning: Taken from huggingface.
        description_implications:
            "An embedding is a relatively low-dimensional space
            that is used to translate high-dimensional vectors like words or positions,
            which can have a large vocbulary size. Ideally, after an embedding is
            trained, it captures some of the semantics of the input by placing semantically
            similar inputs close together in the embedding space.


            Increasing the embedding size may cause the model to train more slowly,
            but the higher dimensionality can also improve overall quality."
        expected_impact: 2
        suggested_values: 512
        suggested_values_reasoning:
            Out of the box value based on published literature.
            Try models with smaller or larger embedding sizes to observe relative
            impact.
        ui_display_name: Max Position Embeddings
    max_sequence_length:
        default_value_reasoning:
            Sets the maximum sequence length of the expected
            inputs, so input/output shapes are computed accurately.
        expected_impact: 1
        ui_display_name: null
    num_attention_heads:
        ui_display_name: null
    num_hidden_layers:
        ui_display_name: null
    position_embedding_type:
        ui_display_name: null
    pretrained_kwargs:
        ui_display_name: null
    pretrained_model_name_or_path:
        ui_display_name: null
    reduce_output:
        ui_display_name: null
    saved_weights_in_checkpoint:
        default_value_reasoning:
            The weights of the encoder are not necessarily saved
            in the checkpoint. The user has to save them first.
        description_implications:
            The memory footprint for some of these encoders
            can be large.
        expected_impact: 1
        related_parameters:
            - skip_save_model
        suggested_values:
            - false
        suggested_values_reasoning:
            Some of these encoders are large, so it might
            be better to load them as needed, especially if 1. they're not used frequently
            2. the user doesn't have a lot of storage.
        ui_display_name: null
    trainable:
        expected_impact: 2
        ui_display_name: null
    type_vocab_size:
        ui_display_name: null
    use_pretrained:
        ui_display_name: null
    vocab:
        default_value_reasoning:
            Computed and passed along internally according to
            preprocessing settings.
        example_value:
            - a
            - b
            - c
        expected_impact: 2
        internal_only: true
        ui_display_name: Not Displayed
    vocab_size:
        internal_only: true
        ui_display_name: Not displayed
FlauBERT:
    type:
        short_description:
            Language model with BERT related architecture trained on large French text corpus.
        long_description:
            The flaubert encoder loads a pretrained FlauBERT (default jplu/tf-flaubert-base-uncased) model
            using the Hugging Face transformers package. FlauBERT has an architecture similar to BERT and is
            pre-trained on a large French language corpus.
        literature_references:
            - https://arxiv.org/abs/1912.05372
    asm:
        ui_display_name: null
    attention_dropout:
        default_value_reasoning: Huggingface default.
        description_implications:
            "Dropout is a computationally cheap regularization\
            \ method where during training, some neurons are randomly ignored or \u201C\
            dropped out\u201D. Increasing dropout has the effect of making the training\
            \ process more noisy and lowering overall network capacity, but it can\
            \ be an effective regularization method to reduce overfitting and improve\
            \ generalization."
        example_value:
            - 0.2
        expected_impact: 3
        literature_references:
            - https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html
        related_parameters:
            - dropout
        suggested_values: 0.05 - 0.8
        suggested_values_reasoning:
            Tuning dropout is really something to be done
            when all of the big choices about architecture have been settled. Consider
            starting with 0.5 and adjusting the dropout depending on observed model
            performance.
        ui_display_name: attention_dropout
    bos_index:
        ui_display_name: null
    causal:
        ui_display_name: null
    dropout:
        default_value_reasoning: Huggingface default.
        description_implications:
            "Dropout is a computationally cheap regularization\
            \ method where during training, some neurons are randomly ignored or \u201C\
            dropped out\u201D. Increasing dropout has the effect of making the training\
            \ process more noisy and lowering overall network capacity, but it can\
            \ be an effective regularization method to reduce overfitting and improve\
            \ generalization."
        example_value:
            - 0.2
        expected_impact: 3
        literature_references:
            - https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html
        related_parameters:
            - attention_dropout
        suggested_values: 0.05 - 0.8
        suggested_values_reasoning:
            Tuning dropout is really something to be done
            when all of the big choices about architecture have been settled. Consider
            starting with 0.5 and adjusting the dropout depending on observed model
            performance.
        ui_display_name: dropout
    emb_dim:
        ui_display_name: null
    embed_init_std:
        ui_display_name: null
    eos_index:
        ui_display_name: null
    gelu_activation:
        ui_display_name: null
    init_std:
        ui_display_name: null
    is_encoder:
        ui_display_name: null
    lang_id:
        ui_display_name: null
    layer_norm_eps:
        ui_display_name: null
    layerdrop:
        ui_display_name: null
    mask_index:
        ui_display_name: null
    mask_token_id:
        default_value_reasoning: Default value used in pre-trained HF encoder.
        ui_display_name: Mask Token ID
    max_position_embeddings:
        default_value_reasoning: Taken from huggingface.
        description_implications:
            "An embedding is a relatively low-dimensional space
            that is used to translate high-dimensional vectors like words or positions,
            which can have a large vocbulary size. Ideally, after an embedding is
            trained, it captures some of the semantics of the input by placing semantically
            similar inputs close together in the embedding space.


            Increasing the embedding size may cause the model to train more slowly,
            but the higher dimensionality can also improve overall quality."
        expected_impact: 2
        suggested_values: 512
        suggested_values_reasoning:
            Out of the box value based on published literature.
            Try models with smaller or larger embedding sizes to observe relative
            impact.
        ui_display_name: Max Position Embeddings
    max_sequence_length:
        default_value_reasoning:
            Sets the maximum sequence length of the expected
            inputs, so input/output shapes are computed accurately.
        expected_impact: 1
        ui_display_name: null
    n_head:
        ui_display_name: null
    n_langs:
        default_value_reasoning: Default value used in pre-trained HF encoder.
        expected_impact: 1
        ui_display_name: Number of Languages
    n_layer:
        ui_display_name: null
    pad_index:
        ui_display_name: null
    pre_norm:
        ui_display_name: null
    pretrained_kwargs:
        ui_display_name: null
    pretrained_model_name_or_path:
        ui_display_name: null
    reduce_output:
        ui_display_name: null
    saved_weights_in_checkpoint:
        default_value_reasoning:
            The weights of the encoder are not necessarily saved
            in the checkpoint. The user has to save them first.
        description_implications:
            The memory footprint for some of these encoders
            can be large.
        expected_impact: 1
        related_parameters:
            - skip_save_model
        suggested_values:
            - false
        suggested_values_reasoning:
            Some of these encoders are large, so it might
            be better to load them as needed, especially if 1. they're not used frequently
            2. the user doesn't have a lot of storage.
        ui_display_name: null
    sinusoidal_embeddings:
        ui_display_name: null
    trainable:
        expected_impact: 2
        ui_display_name: null
    unk_index:
        ui_display_name: null
    use_lang_emb:
        ui_display_name: null
    use_pretrained:
        ui_display_name: null
    vocab:
        default_value_reasoning:
            Computed and passed along internally according to
            preprocessing settings.
        example_value:
            - a
            - b
            - c
        expected_impact: 2
        internal_only: true
        ui_display_name: Not Displayed
    vocab_size:
        internal_only: true
        ui_display_name: Not displayed
GPT2:
    type:
        short_description:
            GPT-2 is a pre-trained language model used for NLP tasks like generation, summarization, and translation.
        long_description:
            The gpt2 encoder loads a pretrained GPT-2 (default gpt2) model using the Hugging Face
            transformers package. GPT-2 is a causal (unidirectional) transformer pretrained using language
            modeling on a very large corpus of ~40 GB of text data.
        literature_references:
            - https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf
    activation_function:
        ui_display_name: null
    attn_pdrop:
        ui_display_name: null
    embd_pdrop:
        ui_display_name: null
    initializer_range:
        description_implications:
            There is an ideal value for this variable that doesn't
            lead to the outputs of these matrices to vanish or explode
        example_value:
            - 0.02
        expected_impact: 3
        other_information: Must be greater than 0
        related_parameters:
            - weights_initializer
        suggested_values: 0.01-0.05
        suggested_values_reasoning:
            Large values will likely lead to very large outputs.
            Small values will lead to vanishing outputs.
        ui_display_name: null
    layer_norm_epsilon:
        ui_display_name: null
    max_sequence_length:
        default_value_reasoning:
            Sets the maximum sequence length of the expected
            inputs, so input/output shapes are computed accurately.
        expected_impact: 1
        ui_display_name: null
    n_ctx:
        ui_display_name: null
    n_embd:
        ui_display_name: null
    n_head:
        ui_display_name: null
    n_inner:
        ui_display_name: null
    n_layer:
        ui_display_name: null
    n_positions:
        ui_display_name: null
    pretrained_kwargs:
        ui_display_name: null
    pretrained_model_name_or_path:
        ui_display_name: null
    reduce_output:
        ui_display_name: null
    resid_pdrop:
        ui_display_name: null
    scale_attn_weights:
        ui_display_name: null
    trainable:
        expected_impact: 2
        ui_display_name: null
    use_pretrained:
        ui_display_name: null
    vocab:
        default_value_reasoning:
            Computed and passed along internally according to
            preprocessing settings.
        example_value:
            - a
            - b
            - c
        expected_impact: 2
        internal_only: true
        ui_display_name: Not Displayed
    vocab_size:
        internal_only: true
        ui_display_name: Not displayed
GPT:
    type:
        short_description:
            GPT is a pre-trained language model used for NLP tasks like generation, summarization, and translation.
        long_description:
            The gpt encoder loads a pretrained GPT (default openai-gpt) model using the Hugging Face "
            "transformers package. GPT is a causal (unidirectional) transformer pre-trained using language "
            "modeling on a large corpus with long range dependencies, the Toronto Book Corpus.
        literature_references:
            - https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf
    afn:
        ui_display_name: null
    attn_pdrop:
        ui_display_name: null
    embd_pdrop:
        ui_display_name: null
    initializer_range:
        description_implications:
            There is an ideal value for this variable that doesn't
            lead to the outputs of these matrices to vanish or explode
        example_value:
            - 0.02
        expected_impact: 3
        other_information: Must be greater than 0
        related_parameters:
            - weights_initializer
        suggested_values: 0.01-0.05
        suggested_values_reasoning:
            Large values will likely lead to very large outputs.
            Small values will lead to vanishing outputs.
        ui_display_name: null
    layer_norm_epsilon:
        ui_display_name: null
    max_sequence_length:
        default_value_reasoning:
            Sets the maximum sequence length of the expected
            inputs, so input/output shapes are computed accurately.
        expected_impact: 1
        ui_display_name: null
    n_ctx:
        ui_display_name: null
    n_embd:
        ui_display_name: null
    n_head:
        ui_display_name: null
    n_layer:
        ui_display_name: null
    n_positions:
        ui_display_name: null
    pretrained_kwargs:
        ui_display_name: null
    pretrained_model_name_or_path:
        ui_display_name: null
    reduce_output:
        ui_display_name: null
    resid_pdrop:
        ui_display_name: null
    saved_weights_in_checkpoint:
        default_value_reasoning:
            The weights of the encoder are not necessarily saved
            in the checkpoint. The user has to save them first.
        description_implications:
            The memory footprint for some of these encoders
            can be large.
        expected_impact: 1
        related_parameters:
            - skip_save_model
        suggested_values:
            - false
        suggested_values_reasoning:
            Some of these encoders are large, so it might
            be better to load them as needed, especially if 1. they're not used frequently
            2. the user doesn't have a lot of storage.
        ui_display_name: null
    trainable:
        expected_impact: 2
        ui_display_name: null
    use_pretrained:
        ui_display_name: null
    vocab:
        default_value_reasoning:
            Computed and passed along internally according to
            preprocessing settings.
        example_value:
            - a
            - b
            - c
        expected_impact: 2
        internal_only: true
        ui_display_name: Not Displayed
    vocab_size:
        internal_only: true
        ui_display_name: Not displayed
H3Embed:
    type:
        short_description:
            Encodes each H3 component with embeddings then takes a sum and passes them through fully connected layers.
        long_description:
            The Embed encoder encodes each component of the H3 representation (mode, edge, resolution,
            base cell and children cells) with embeddings. Children cells with value 0 will be masked out.
            After the embedding, all embeddings are summed and optionally passed through a stack of fully
            connected layers.
    activation:
        default_value_reasoning:
            The Rectified Linear Units (ReLU) function is the
            standard activation function used for adding non-linearity. It is simple,
            fast, and empirically works well (https://arxiv.org/abs/1803.08375).
        description_implications:
            Changing the activation functions has an impact
            on the computational load of the model and might require further hypterparameter
            tuning
        expected_impact: 1
        suggested_values:
            The default value will work well in the majority of the
            cases
        ui_display_name: Activation
    bias_initializer:
        default_value_reasoning:
            It is possible and common to initialize the biases
            to be zero, since the asymmetry breaking is provided by the small random
            numbers in the weights.
        description_implications:
            It's rare to see any performance gains from choosing
            a different bias initialization. Some practitioners like to use a small
            constant value such as 0.01 for all biases to ensure that all ReLU units
            are activated in the beginning and have some effect on the gradient. However,
            it's still an open question as to whether this provides consistent improvement.
        expected_impact: 1
        literature_references:
            - https://cs231n.github.io/neural-networks-2/
        related_parameters:
            - weights_initializer
        suggested_values: zeros
        suggested_values_reasoning:
            It is possible and common to initialize the biases
            to be zero, since the asymmetry breaking is provided by the small random
            numbers in the weights. For ReLU non-linearities, some people like to
            use small constant value such as 0.01 for all biases because this ensures
            that all ReLU units fire in the beginning and therefore obtain and propagate
            some gradient. However, it is not clear if this provides a consistent
            improvement (in fact some results seem to indicate that this performs
            worse) and it is more common to simply use 0 bias initialization.
        ui_display_name: Bias Initializer
    dropout:
        default_value_reasoning:
            Dropout can cause training to become less stable.
            Consider start with a dropout-free baseline, and add dropout gradually
            in subsequent experiments.
        description_implications:
            "Dropout is a computationally cheap regularization\
            \ method where during training, some neurons are randomly ignored or \u201C\
            dropped out\u201D. Increasing dropout has the effect of making the training\
            \ process more noisy and lowering overall network capacity, but it can\
            \ be an effective regularization method to reduce overfitting and improve\
            \ generalization."
        example_value:
            - 0.2
        expected_impact: 3
        literature_references:
            - https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html
        suggested_values: 0.05 - 0.8
        suggested_values_reasoning:
            Tuning dropout is really something to be done
            when all of the big choices about architecture have been settled. Consider
            starting with 0.5 and adjusting the dropout depending on observed model
            performance.
        ui_display_name: Dropout
    embedding_size:
        default_value_reasoning: Not too big, not too small.
        description_implications:
            'An embedding is a relatively low-dimensional space
            that is used to translate high-dimensional vectors like words, which can
            have a large vocbulary size. Ideally, after an embedding is trained, it
            captures some of the semantics of the input by placing semantically similar
            inputs close together in the embedding space.


            In most cases, the embedding size is chosen empirically, by trial and
            error. From https://www.amazon.com/dp/1098115783, "one rule of thumb is
            to use the fourth root of the total number of unique categorical elements
            while another is that the embedding dimension should be approximately
            1.6 times the square root of the number of unique elements in the category,
            and no less than 600."


            Increasing the embedding size may cause the model to train more slowly,
            but the higher dimensionality can also improve overall quality.'
        expected_impact: 2
        literature_references:
            - https://developers.google.com/machine-learning/crash-course/embeddings/video-lecture
        suggested_values: 1.6 * sqrt(vocab_size)
        suggested_values_reasoning:
            Rule of thumb suggested by a deep learning textbook.
            Try models with smaller or larger embedding sizes to observe relative
            impact.
        ui_display_name: Embedding Size
    embeddings_on_cpu:
        default_value_reasoning:
            By default embeddings matrices are stored on GPU
            memory if a GPU is used, as it allows for faster access.
        description_implications:
            By default embeddings matrices are stored on GPU
            memory if a GPU is used, as it allows for faster access. However, in some
            cases when the vocabulary size is very large, the full embedding matrix
            may be really big and unwieldy to have in GPU memory. This parameter forces
            the placement of the embedding matrix in regular memory and the CPU is
            used to access them. This may slow down training due to additional data
            transfer between CPU and GPU memory, but can lead to healthier GPU memory
            resource usage.
        expected_impact: 1
        suggested_values:
            - false
        suggested_values_reasoning:
            If GPU memory is not a constraint, having embeddings
            stored and accessed within the GPU is faster.
        ui_display_name: Embeddings on CPU
    fc_layers:
        default_value_reasoning:
            By default the stack is built by using num_fc_layers,
            output_size, use_bias, weights_initializer, bias_initializer, norm, norm_params,
            activation, dropout. When a list of dictionaries is provided, the stack
            is built following the parameters of each dict for building each layer.
        description_implications:
            The more layers that are specified the deeper and
            higher capacity the model will be. This makes it possible to potentially
            achieve better performance when a big anough amount of data is provided,
            but also makes the model more computationally expensive and potentially
            more prone to overfitting.
        example_value:
            - dropout: 0.1
              output_size: 128
            - norm: layer
              output_size: 64
        expected_impact: 1
        related_parameters:
            - output_size
            - use_bias
            - weights_initializer
            - bias_initializer
            - norm
            - norm_params
            - activation
            - dropout
        suggested_values_reasoning:
            It is easier to define a stack of fully connected
            layers by just specifying num_fc_layers, output_size and the other individual
            parameters. It will create a stack of layers with identical properties.
            Use this parameter only if you need a fine grained level of control of
            each individual layer in the stack.
        ui_display_name: Fully Connected Layers
    norm:
        default_value_reasoning:
            While batch normalization and layer normalization
            usually lead to improvements, it can be useful to start with fewer bells
            and whistles.
        description_implications:
            Normalization helps stabilize the learning process
            and can have a regularizing effect that can help with generalization.
            It's often suggested that with normalization, you can use a higher learning
            rate.
        example_value:
            - batch
        expected_impact: 3
        literature_references:
            - https://machinelearningmastery.com/batch-normalization-for-training-of-deep-neural-networks/
        related_parameters:
            - norm_params
        suggested_values: '"batch" or "layer"'
        suggested_values_reasoning:
            Normalization tries to solve "internal covariate
            shift" that comes from the changing distributions of the inputs to layers
            deep in the network when weights are updated. For example, batch normalization
            standardizes the inputs to a layer for each mini-batch. Try out different
            normalizations to see if that helps with training stability
        ui_display_name: Normalization Type
    norm_params:
        default_value_reasoning:
            The default parameters that come with Torch's implementation
            of these normalization types are a trusted starting point.
        description_implications:
            There are a variety of ways a certain set of parameters
            specificed could influence performance here. Broadly speaking the different
            values passed in here allow for different levels of smoothness to be observed
            in the learning curves. Since setting this parameters depends on the type
            of `norm` set, see [BatchNorm2d](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html)
            for more information on the parameters to set for batch normalization,
            and see [LayerNorm](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html)
            for more information on the parameters to set for layer normalization.
        example_value:
            - affine: false
              momentum: 0.2
              num_features: 100
        expected_impact: 1
        literature_references:
            - "For BatchNorm2d: https://arxiv.org/abs/1502.03167


              For LayerNorm: https://arxiv.org/abs/1607.06450"
        related_parameters:
            - "`norm`"
        suggested_values: Depends on the type of `norm` set.
        suggested_values_reasoning: "NO"
        ui_display_name: Normalization Parameters
    num_fc_layers:
        default_value_reasoning:
            The encoder already has learnable parameters.Sometimes
            the default is 1 for modules where the FC stack is used for shape management,
            or the only source of learnable parameters.
        description_implications:
            Increasing num_fc_layers will increase the capacity
            of the model. The model will be slower to train, and there's a higher
            risk of overfitting.
        example_value:
            - 1
        expected_impact: 1
        other_information:
            Not all modules that have fc_layers also have an accompanying
            num_fc_layers parameter. Where both are present, fc_layers takes precedent
            over num_fc_layers. Specifying num_fc_layers alone uses fully connected
            layers that are configured by the defaults in FCStack.
        related_parameters:
            - fc_layers
        suggested_values: 0-1
        suggested_values_reasoning:
            The full model likely contains many learnable
            parameters. Consider starting with very few, or without any additional
            fully connected layers and add them if you observe evidence of limited
            model capacity. Sometimes the default is 1 for modules where the FC stack
            is used for shape management, or the only source of learnable parameters.
        ui_display_name: Number of Fully Connected Layers
    output_size:
        default_value_reasoning: A modest value, not too small, not too large.
        description_implications:
            If there are fully connected layers in this module,
            increasing the output size of each fully connected layer will increase
            the capacity of the model. However, the model may be slower to train,
            and there's a higher risk of overfitting. If it seems like the model could
            use even more capacity, consider increasing the number of fully connected
            layers, or explore other architectures.
        expected_impact: 2
        other_information:
            If num_fc_layers=0 and fc_layers=None, and there are no
            fully connected layers defined on the module, then this parameter may
            have no effect on the module's final output shape.
        related_parameters:
            - num_fc_layers, fc_layers
        suggested_values: 10 - 1024
        suggested_values_reasoning:
            Increasing the output size increases the capacity
            of the model. If this seems to have a positive effect, then it could be
            worth increasing the number of layers, or trying a different architecture
            with a larger capacity.
        ui_display_name: Output Size
    reduce_output:
        default_value_reasoning: Sums the tensors along the sequence dimension.
        description_implications:
            "\"last\", \"sum\", \"mean\", and \"max\" are the\
            \ fastest and most memory-efficient operations\u2013 they result in tensors\
            \ that are the same-size as a single item in the input sequence. However,\
            \ these are simple aggregation operations, therefore some information\
            \ may be lost. \n\n\"concat\" concatenates each tensor together, creating\
            \ a `(sequence length)*(tensor size)`-element tensor. \"concat\" preserves\
            \ this information, but can be very memory-intensive and should only be\
            \ applied if the sequence length and/or tensor size is small. \n\n\"attention\"\
            \ takes a weighted sum of the items in the sequence, where the weights\
            \ for each item in the sequence are determined by the model on-the-fly\
            \ based on the features of the item itself. This is both slower and and\
            \ more memory-intensive than the other operations; however, it can also\
            \ provide a richer \"global\" representation of the sequence."
        expected_impact: 1
        related_parameters:
            - max_sequence_length
        suggested_values: '"attention". This and the default covers 95% of use cases.'
        suggested_values_reasoning:
            If you would like better performance and are not
            compute/memory-constrained, attention-based reduction can potentially
            provide a richer global representation than the default.
        ui_display_name: Sequence Reducer
    use_bias:
        ui_display_name: null
    weights_initializer:
        default_value_reasoning: Taken from [this paper](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf).
        description_implications:
            The method you choose to initialize layer weights
            during training can have a big impact on performance as well as the reproducibility
            of your final model between runs. As an example, if you were to randomly
            initialize weights you would risk non-reproducibility (and possibly general
            training performance), but sticking with constant values for initialization
            might significantly increase the time needed for model convergence. Generally,
            choosing one of the probabilistic approaches strikes a balance between
            the two extremes, and the literature kicked off by the landmark [*Xavier
            et al.* paper](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)
            provides a few good options. See this nice discussion from [Weights and
            Biases](https://wandb.ai/site/articles/the-effects-of-weight-initialization-on-neural-nets#:~:text=Studies%20have%20shown%20that%20initializing,net%20train%20better%20and%20faster.)
            for more information.
        expected_impact: 3
        literature_references:
            - "Weights and Biases blog post: https://wandb.ai/site/articles/the-effects-of-weight-initialization-on-neural-nets#:~:text=Studies%20have%20shown%20that%20initializing,net%20train%20better%20and%20faster.


              Xavier et al. paper: http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf"
        suggested_values: xavier_uniform
        suggested_values_reasoning:
            Changing the weights initialization scheme is
            something to consider if a model is having trouble with convergence, or
            otherwise it is something to experiment with after other factors are considered.
            The default choice (`xavier_uniform`) is a suitable starting point for
            most tasks.
        ui_display_name: Layer Weights Initializer
H3RNN:
    type:
        short_description:
            Encodes each H3 component with embeddings then passes them through an RNN encoder.
        long_description:
            The RNN encoder encodes each component of the H3 representation (mode, edge, resolution,
            base cell and children cells) with embeddings. Children cells with value 0 will be masked out.
            After the embedding, all embeddings are passed through an RNN encoder. The intuition behind this
            is that, starting from the base cell, the sequence of children cells can be seen as a sequence
            encoding the path in the tree of all H3 hexes.
    activation:
        ui_display_name: null
    bias_initializer:
        default_value_reasoning:
            It is possible and common to initialize the biases
            to be zero, since the asymmetry breaking is provided by the small random
            numbers in the weights.
        description_implications:
            It's rare to see any performance gains from choosing
            a different bias initialization. Some practitioners like to use a small
            constant value such as 0.01 for all biases to ensure that all ReLU units
            are activated in the beginning and have some effect on the gradient. However,
            it's still an open question as to whether this provides consistent improvement.
        expected_impact: 1
        literature_references:
            - https://cs231n.github.io/neural-networks-2/
        related_parameters:
            - weights_initializer
        suggested_values: zeros
        suggested_values_reasoning:
            It is possible and common to initialize the biases
            to be zero, since the asymmetry breaking is provided by the small random
            numbers in the weights. For ReLU non-linearities, some people like to
            use small constant value such as 0.01 for all biases because this ensures
            that all ReLU units fire in the beginning and therefore obtain and propagate
            some gradient. However, it is not clear if this provides a consistent
            improvement (in fact some results seem to indicate that this performs
            worse) and it is more common to simply use 0 bias initialization.
        ui_display_name: Bias Initializer
    bidirectional:
        default_value_reasoning:
            For short sequences, it is reasonable to use a vanilla
            RNN.
        description_implications:
            Setting bidirectional to True may increase the compute
            and memory requirements of the model, but may also increase model performance
            on long sequences.
        expected_impact: 3
        literature_references:
            - https://devopedia.org/bidirectional-rnn#:~:text=RNN%20has%20the%20limitation%20that,forward%20and%20reverse%20time%20order.
        related_parameters:
            - cell_type, activation, recurrent_activation, use_bias
        suggested_values:
            - true
        suggested_values_reasoning:
            "RNNs can sometimes suffer from catastrophic forgetting
            (source: https://en.wikipedia.org/wiki/Catastrophic_interference ) on
            long sequences. Allowing the RNN to read from both the beginning and end
            of the sequence can improve its representation at each timestep."
        ui_display_name: Bidirectional
    cell_type:
        default_value_reasoning:
            The LSTM cell has proven to be the most performant
            of the three cells.
        description_implications:
            "There are two reasons to consider other cell types:
            (1) compute costs and (2) catastrophic forgetting (source: https://en.wikipedia.org/wiki/Catastrophic_interference
            ). RNNs have marginally less compute costs, but are prone to catastrophic
            forgetting."
        expected_impact: 1
        related_parameters:
            - "bidirectional

              activation

              recurrent_activation

              use_bias"
        ui_display_name: Cell Type
    dropout:
        default_value_reasoning:
            Dropout can cause training to become less stable.
            Consider start with a dropout-free baseline, and add dropout gradually
            in subsequent experiments.
        description_implications:
            "Dropout is a computationally cheap regularization\
            \ method where during training, some neurons are randomly ignored or \u201C\
            dropped out\u201D. Increasing dropout has the effect of making the training\
            \ process more noisy and lowering overall network capacity, but it can\
            \ be an effective regularization method to reduce overfitting and improve\
            \ generalization."
        example_value:
            - 0.2
        expected_impact: 3
        literature_references:
            - https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html
        related_parameters:
            - recurrent_dropout
        suggested_values: 0.05 - 0.8
        suggested_values_reasoning:
            Tuning dropout is really something to be done
            when all of the big choices about architecture have been settled. Consider
            starting with 0.5 and adjusting the dropout depending on observed model
            performance.
        ui_display_name: Dropout
    embedding_size:
        default_value_reasoning: Not too big, not too small.
        description_implications:
            'An embedding is a relatively low-dimensional space
            that is used to translate high-dimensional vectors like words, which can
            have a large vocbulary size. Ideally, after an embedding is trained, it
            captures some of the semantics of the input by placing semantically similar
            inputs close together in the embedding space.


            In most cases, the embedding size is chosen empirically, by trial and
            error. From https://www.amazon.com/dp/1098115783, "one rule of thumb is
            to use the fourth root of the total number of unique categorical elements
            while another is that the embedding dimension should be approximately
            1.6 times the square root of the number of unique elements in the category,
            and no less than 600."


            Increasing the embedding size may cause the model to train more slowly,
            but the higher dimensionality can also improve overall quality.'
        expected_impact: 2
        literature_references:
            - https://developers.google.com/machine-learning/crash-course/embeddings/video-lecture
        suggested_values: 1.6 * sqrt(vocab_size)
        suggested_values_reasoning:
            Rule of thumb suggested by a deep learning textbook.
            Try models with smaller or larger embedding sizes to observe relative
            impact.
        ui_display_name: Embedding Size
    embeddings_on_cpu:
        default_value_reasoning:
            By default embeddings matrices are stored on GPU
            memory if a GPU is used, as it allows for faster access.
        description_implications:
            By default embeddings matrices are stored on GPU
            memory if a GPU is used, as it allows for faster access. However, in some
            cases when the vocabulary size is very large, the full embedding matrix
            may be really big and unwieldy to have in GPU memory. This parameter forces
            the placement of the embedding matrix in regular memory and the CPU is
            used to access them. This may slow down training due to additional data
            transfer between CPU and GPU memory, but can lead to healthier GPU memory
            resource usage.
        expected_impact: 1
        suggested_values:
            - false
        suggested_values_reasoning:
            If GPU memory is not a constraint, having embeddings
            stored and accessed within the GPU is faster.
        ui_display_name: Embeddings on CPU
    hidden_size:
        default_value_reasoning:
            H3 values numbers, so a small RNN dimensionality
            is likely sufficient.
        description_implications:
            Increasing the hidden size makes the model larger
            and slower to train, increases the model's capacity to capture more complexity.
            It also increases the chance of overfitting.
        expected_impact: 2
        suggested_values: 10 - 2048
        suggested_values_reasoning:
            Increasing the hidden size makes sense if the
            model is underfitting. It's useful to train both smaller and larger models
            to see how model capacity affects performance. This should only be explored
            after the architecture of the model has been settled.
        ui_display_name: Hidden Size
    num_layers:
        default_value_reasoning:
            The ideal number of layers depends on the data. For
            many data types, one layer is sufficient.
        description_implications:
            Increasing the number of layers may improve model
            performance for longer sequences or more complex tasks.
        example_value:
            - 1
        expected_impact: 1
        other_information:
            If you have multiple input features, varying the number
            of layers in the combiner or output feature decoder will have more impact.
        related_parameters:
            - layers
        suggested_values: 1-3
        suggested_values_reasoning:
            Increasing the number of layers may improve encoder
            performance.  However, more layers will increase training time and may
            cause overfitting.  Small numbers of layers usually work best.
        ui_display_name: Number of Recurrent Layers
    recurrent_activation:
        default_value_reasoning: sigmoid' is commonly used
        expected_impact: 3
        other_information:
            I don't think that this parameter is used anywhere in the
            code base. It's being passed down but not used in the actual RNN forwarding
            functions.
        suggested_values: sigmoid, ReLu, tanh
        ui_display_name: null
    recurrent_dropout:
        default_value_reasoning:
            Dropout can cause training to become less stable.
            Consider start with a dropout-free baseline, and add dropout gradually
            in subsequent experiments.
        description_implications:
            "Dropout is a computationally cheap regularization\
            \ method where during training, some neurons are randomly ignored or \u201C\
            dropped out\u201D. Increasing dropout has the effect of making the training\
            \ process more noisy and lowering overall network capacity, but it can\
            \ be an effective regularization method to reduce overfitting and improve\
            \ generalization."
        example_value:
            - 0.2
        expected_impact: 3
        literature_references:
            - https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html
        related_parameters:
            - dropout
        suggested_values: 0.05 - 0.8
        suggested_values_reasoning:
            Tuning dropout is really something to be done
            when all of the big choices about architecture have been settled. Consider
            starting with 0.5 and adjusting the dropout depending on observed model
            performance.
        ui_display_name: Recurrent Dropout
    recurrent_initializer:
        ui_display_name: null
    reduce_output:
        ui_display_name: null
    unit_forget_bias:
        ui_display_name: null
    use_bias:
        ui_display_name: null
    weights_initializer:
        default_value_reasoning: Taken from [this paper](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf).
        description_implications:
            The method you choose to initialize layer weights
            during training can have a big impact on performance as well as the reproducibility
            of your final model between runs. As an example, if you were to randomly
            initialize weights you would risk non-reproducibility (and possibly general
            training performance), but sticking with constant values for initialization
            might significantly increase the time needed for model convergence. Generally,
            choosing one of the probabilistic approaches strikes a balance between
            the two extremes, and the literature kicked off by the landmark [*Xavier
            et al.* paper](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)
            provides a few good options. See this nice discussion from [Weights and
            Biases](https://wandb.ai/site/articles/the-effects-of-weight-initialization-on-neural-nets#:~:text=Studies%20have%20shown%20that%20initializing,net%20train%20better%20and%20faster.)
            for more information.
        expected_impact: 3
        literature_references:
            - "Weights and Biases blog post: https://wandb.ai/site/articles/the-effects-of-weight-initialization-on-neural-nets#:~:text=Studies%20have%20shown%20that%20initializing,net%20train%20better%20and%20faster.


              Xavier et al. paper: http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf"
        suggested_values: xavier_uniform
        suggested_values_reasoning:
            Changing the weights initialization scheme is
            something to consider if a model is having trouble with convergence, or
            otherwise it is something to experiment with after other factors are considered.
            The default choice (`xavier_uniform`) is a suitable starting point for
            most tasks.
        ui_display_name: Layer Weights Initializer
H3WeightedSum:
    type:
        short_description:
            Encodes each H3 component with embeddings then takes a weighted sum.
        long_description:
            The Weighted Sum encoder encodes each component of the H3 representation (mode, edge,
            resolution, base cell and children cells) with embeddings. Children cells with value 0 will be
            masked out. After the embedding, all embeddings are summed with a weighted sum (with learned
            weights) and optionally passed through a stack of fully connected layers.
    activation:
        default_value_reasoning:
            The Rectified Linear Units (ReLU) function is the
            standard activation function used for adding non-linearity. It is simple,
            fast, and empirically works well (https://arxiv.org/abs/1803.08375).
        description_implications:
            Changing the activation functions has an impact
            on the computational load of the model and might require further hypterparameter
            tuning
        expected_impact: 1
        suggested_values:
            The default value will work well in the majority of the
            cases
        ui_display_name: Activation
    bias_initializer:
        default_value_reasoning:
            It is possible and common to initialize the biases
            to be zero, since the asymmetry breaking is provided by the small random
            numbers in the weights.
        description_implications:
            It's rare to see any performance gains from choosing
            a different bias initialization. Some practitioners like to use a small
            constant value such as 0.01 for all biases to ensure that all ReLU units
            are activated in the beginning and have some effect on the gradient. However,
            it's still an open question as to whether this provides consistent improvement.
        expected_impact: 1
        literature_references:
            - https://cs231n.github.io/neural-networks-2/
        related_parameters:
            - weights_initializer
        suggested_values: zeros
        suggested_values_reasoning:
            It is possible and common to initialize the biases
            to be zero, since the asymmetry breaking is provided by the small random
            numbers in the weights. For ReLU non-linearities, some people like to
            use small constant value such as 0.01 for all biases because this ensures
            that all ReLU units fire in the beginning and therefore obtain and propagate
            some gradient. However, it is not clear if this provides a consistent
            improvement (in fact some results seem to indicate that this performs
            worse) and it is more common to simply use 0 bias initialization.
        ui_display_name: Bias Initializer
    dropout:
        default_value_reasoning:
            Dropout can cause training to become less stable.
            Consider start with a dropout-free baseline, and add dropout gradually
            in subsequent experiments.
        description_implications:
            "Dropout is a computationally cheap regularization\
            \ method where during training, some neurons are randomly ignored or \u201C\
            dropped out\u201D. Increasing dropout has the effect of making the training\
            \ process more noisy and lowering overall network capacity, but it can\
            \ be an effective regularization method to reduce overfitting and improve\
            \ generalization."
        example_value:
            - 0.2
        expected_impact: 3
        literature_references:
            - https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html
        suggested_values: 0.05 - 0.8
        suggested_values_reasoning:
            Tuning dropout is really something to be done
            when all of the big choices about architecture have been settled. Consider
            starting with 0.5 and adjusting the dropout depending on observed model
            performance.
        ui_display_name: Dropout
    embedding_size:
        default_value_reasoning: Not too big, not too small.
        description_implications:
            'An embedding is a relatively low-dimensional space
            that is used to translate high-dimensional vectors like words, which can
            have a large vocbulary size. Ideally, after an embedding is trained, it
            captures some of the semantics of the input by placing semantically similar
            inputs close together in the embedding space.


            In most cases, the embedding size is chosen empirically, by trial and
            error. From https://www.amazon.com/dp/1098115783, "one rule of thumb is
            to use the fourth root of the total number of unique categorical elements
            while another is that the embedding dimension should be approximately
            1.6 times the square root of the number of unique elements in the category,
            and no less than 600."


            Increasing the embedding size may cause the model to train more slowly,
            but the higher dimensionality can also improve overall quality.'
        expected_impact: 2
        literature_references:
            - https://developers.google.com/machine-learning/crash-course/embeddings/video-lecture
        suggested_values: 1.6 * sqrt(vocab_size)
        suggested_values_reasoning:
            Rule of thumb suggested by a deep learning textbook.
            Try models with smaller or larger embedding sizes to observe relative
            impact.
        ui_display_name: Embedding Size
    embeddings_on_cpu:
        default_value_reasoning:
            By default embeddings matrices are stored on GPU
            memory if a GPU is used, as it allows for faster access.
        description_implications:
            By default embeddings matrices are stored on GPU
            memory if a GPU is used, as it allows for faster access. However, in some
            cases when the vocabulary size is very large, the full embedding matrix
            may be really big and unwieldy to have in GPU memory. This parameter forces
            the placement of the embedding matrix in regular memory and the CPU is
            used to access them. This may slow down training due to additional data
            transfer between CPU and GPU memory, but can lead to healthier GPU memory
            resource usage.
        expected_impact: 1
        suggested_values:
            - false
        suggested_values_reasoning:
            If GPU memory is not a constraint, having embeddings
            stored and accessed within the GPU is faster.
        ui_display_name: Embeddings on CPU
    fc_layers:
        default_value_reasoning:
            By default the stack is built by using num_fc_layers,
            output_size, use_bias, weights_initializer, bias_initializer, norm, norm_params,
            activation, dropout. When a list of dictionaries is provided, the stack
            is built following the parameters of each dict for building each layer.
        description_implications:
            The more layers that are specified the deeper and
            higher capacity the model will be. This makes it possible to potentially
            achieve better performance when a big anough amount of data is provided,
            but also makes the model more computationally expensive and potentially
            more prone to overfitting.
        example_value:
            - dropout: 0.1
              output_size: 128
            - norm: layer
              output_size: 64
        expected_impact: 1
        related_parameters:
            - output_size
            - use_bias
            - weights_initializer
            - bias_initializer
            - norm
            - norm_params
            - activation
            - dropout
        suggested_values_reasoning:
            It is easier to define a stack of fully connected
            layers by just specifying num_fc_layers, output_size and the other individual
            parameters. It will create a stack of layers with identical properties.
            Use this parameter only if you need a fine grained level of control of
            each individual layer in the stack.
        ui_display_name: Fully Connected Layers
    norm:
        default_value_reasoning:
            While batch normalization and layer normalization
            usually lead to improvements, it can be useful to start with fewer bells
            and whistles.
        description_implications:
            Normalization helps stabilize the learning process
            and can have a regularizing effect that can help with generalization.
            It's often suggested that with normalization, you can use a higher learning
            rate.
        example_value:
            - batch
        expected_impact: 3
        literature_references:
            - https://machinelearningmastery.com/batch-normalization-for-training-of-deep-neural-networks/
        related_parameters:
            - norm_params
        suggested_values: '"batch" or "layer"'
        suggested_values_reasoning:
            Normalization tries to solve "internal covariate
            shift" that comes from the changing distributions of the inputs to layers
            deep in the network when weights are updated. For example, batch normalization
            standardizes the inputs to a layer for each mini-batch. Try out different
            normalizations to see if that helps with training stability
        ui_display_name: Normalization Type
    norm_params:
        default_value_reasoning:
            The default parameters that come with Torch's implementation
            of these normalization types are a trusted starting point.
        description_implications:
            There are a variety of ways a certain set of parameters
            specificed could influence performance here. Broadly speaking the different
            values passed in here allow for different levels of smoothness to be observed
            in the learning curves. Since setting this parameters depends on the type
            of `norm` set, see [BatchNorm2d](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html)
            for more information on the parameters to set for batch normalization,
            and see [LayerNorm](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html)
            for more information on the parameters to set for layer normalization.
        example_value:
            - affine: false
              momentum: 0.2
              num_features: 100
        expected_impact: 1
        literature_references:
            - "For BatchNorm2d: https://arxiv.org/abs/1502.03167


              For LayerNorm: https://arxiv.org/abs/1607.06450"
        related_parameters:
            - "`norm`"
        suggested_values: Depends on the type of `norm` set.
        suggested_values_reasoning: "NO"
        ui_display_name: Normalization Parameters
    num_fc_layers:
        default_value_reasoning:
            The encoder already has learnable parameters.Sometimes
            the default is 1 for modules where the FC stack is used for shape management,
            or the only source of learnable parameters.
        description_implications:
            Increasing num_fc_layers will increase the capacity
            of the model. The model will be slower to train, and there's a higher
            risk of overfitting.
        example_value:
            - 1
        expected_impact: 1
        other_information:
            Not all modules that have fc_layers also have an accompanying
            num_fc_layers parameter. Where both are present, fc_layers takes precedent
            over num_fc_layers. Specifying num_fc_layers alone uses fully connected
            layers that are configured by the defaults in FCStack.
        related_parameters:
            - fc_layers
        suggested_values: 0-1
        suggested_values_reasoning:
            The full model likely contains many learnable
            parameters. Consider starting with very few, or without any additional
            fully connected layers and add them if you observe evidence of limited
            model capacity. Sometimes the default is 1 for modules where the FC stack
            is used for shape management, or the only source of learnable parameters.
        ui_display_name: Number of Fully Connected Layers
    output_size:
        default_value_reasoning: A modest value, not too small, not too large.
        description_implications:
            If there are fully connected layers in this module,
            increasing the output size of each fully connected layer will increase
            the capacity of the model. However, the model may be slower to train,
            and there's a higher risk of overfitting. If it seems like the model could
            use even more capacity, consider increasing the number of fully connected
            layers, or explore other architectures.
        expected_impact: 2
        other_information:
            If num_fc_layers=0 and fc_layers=None, and there are no
            fully connected layers defined on the module, then this parameter may
            have no effect on the module's final output shape.
        related_parameters:
            - num_fc_layers, fc_layers
        suggested_values: 10 - 1024
        suggested_values_reasoning:
            Increasing the output size increases the capacity
            of the model. If this seems to have a positive effect, then it could be
            worth increasing the number of layers, or trying a different architecture
            with a larger capacity.
        ui_display_name: Output Size
    should_softmax:
        ui_display_name: null
    use_bias:
        ui_display_name: null
    weights_initializer:
        default_value_reasoning: Taken from [this paper](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf).
        description_implications:
            The method you choose to initialize layer weights
            during training can have a big impact on performance as well as the reproducibility
            of your final model between runs. As an example, if you were to randomly
            initialize weights you would risk non-reproducibility (and possibly general
            training performance), but sticking with constant values for initialization
            might significantly increase the time needed for model convergence. Generally,
            choosing one of the probabilistic approaches strikes a balance between
            the two extremes, and the literature kicked off by the landmark [*Xavier
            et al.* paper](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)
            provides a few good options. See this nice discussion from [Weights and
            Biases](https://wandb.ai/site/articles/the-effects-of-weight-initialization-on-neural-nets#:~:text=Studies%20have%20shown%20that%20initializing,net%20train%20better%20and%20faster.)
            for more information.
        expected_impact: 3
        literature_references:
            - "Weights and Biases blog post: https://wandb.ai/site/articles/the-effects-of-weight-initialization-on-neural-nets#:~:text=Studies%20have%20shown%20that%20initializing,net%20train%20better%20and%20faster.


              Xavier et al. paper: http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf"
        suggested_values: xavier_uniform
        suggested_values_reasoning:
            Changing the weights initialization scheme is
            something to consider if a model is having trouble with convergence, or
            otherwise it is something to experiment with after other factors are considered.
            The default choice (`xavier_uniform`) is a suitable starting point for
            most tasks.
        ui_display_name: Layer Weights Initializer
Longformer:
    type:
        short_description:
            Transformer optimized for longer text inputs.
        long_description:
            The longformer encoder loads a pretrained Longformer (default allenai/longformer-base-4096)
            model using the Hugging Face transformers package. Longformer is a good choice for longer text,
            as it supports sequences up to 4096 tokens long.
        literature_references:
            - https://arxiv.org/pdf/2004.05150.pdf
    attention_window:
        ui_display_name: null
    max_sequence_length:
        default_value_reasoning:
            Sets the maximum sequence length of the expected
            inputs, so input/output shapes are computed accurately.
        expected_impact: 1
        ui_display_name: null
    num_tokens:
        ui_display_name: null
    pretrained_kwargs:
        ui_display_name: null
    pretrained_model_name_or_path:
        ui_display_name: null
    reduce_output:
        ui_display_name: null
    saved_weights_in_checkpoint:
        default_value_reasoning:
            The weights of the encoder are not necessarily saved
            in the checkpoint. The user has to save them first.
        description_implications:
            The memory footprint for some of these encoders
            can be large.
        expected_impact: 1
        related_parameters:
            - skip_save_model
        suggested_values:
            - false
        suggested_values_reasoning:
            Some of these encoders are large, so it might
            be better to load them as needed, especially if 1. they're not used frequently
            2. the user doesn't have a lot of storage.
        ui_display_name: null
    sep_token_id:
        ui_display_name: null
    trainable:
        expected_impact: 2
        ui_display_name: null
    use_pretrained:
        ui_display_name: null
    vocab:
        default_value_reasoning:
            Computed and passed along internally according to
            preprocessing settings.
        example_value:
            - a
            - b
            - c
        expected_impact: 2
        internal_only: true
        ui_display_name: Not Displayed
    vocab_size:
        internal_only: true
        ui_display_name: Not displayed
MLPMixer:
    type:
        short_description:
            Image encoder which applies fully connected layers to different patches of the image.
        long_description:
            MLP-Mixer divides the image into equal-sized patches, applying fully connected layers to each
            patch to compute per-patch representations (tokens) and combining the representations with
            fully-connected mixer layers.
    avg_pool:
        ui_display_name: null
    channel_dim:
        ui_display_name: null
    dropout:
        default_value_reasoning:
            Dropout can cause training to become less stable.
            Consider start with a dropout-free baseline, and add dropout gradually
            in subsequent experiments.
        description_implications:
            "Dropout is a computationally cheap regularization\
            \ method where during training, some neurons are randomly ignored or \u201C\
            dropped out\u201D. Increasing dropout has the effect of making the training\
            \ process more noisy and lowering overall network capacity, but it can\
            \ be an effective regularization method to reduce overfitting and improve\
            \ generalization."
        example_value:
            - 0.2
        expected_impact: 3
        literature_references:
            - https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html
        suggested_values: 0.05 - 0.8
        suggested_values_reasoning:
            Tuning dropout is really something to be done
            when all of the big choices about architecture have been settled. Consider
            starting with 0.5 and adjusting the dropout depending on observed model
            performance.
        ui_display_name: Dropout
    embed_size:
        ui_display_name: null
    height:
        internal_only: true
        ui_display_name: null
    num_channels:
        ui_display_name: null
    num_layers:
        default_value_reasoning:
            The ideal number of layers depends on the size and
            complexity of the input images. The default value is used in the paper
            and tested on several image datasets.
        description_implications:
            Increasing the number of layers may improve model
            performance for larger images or more complex image tasks.
        example_value:
            - 8
        expected_impact: 1
        literature_references:
            - "MLP-Mixer: An all-MLP Architecture for Vision

              https://arxiv.org/abs/2105.01601"
        suggested_values: 4 - 32
        suggested_values_reasoning:
            Values from 8 - 32 are tested in the paper. It
            is possible that fewer layers will be sufficient for some tasks.
        ui_display_name: Number of Layers
    patch_size:
        default_value_reasoning: Taken from MLP-Mixer paper.
        description_implications:
            "The implications of the image patch size for this\
            \ layer depend on other factors, such as the true resolution of the incoming\
            \ image dataset. If the patch size is kept consistent but a higher resolution\
            \ image is used as input, then the resulting chunked sequence of tokens\
            \ will be longer than it would have been if the input resolution was lower.\
            \ \n\nThe original MLP-Mixer paper also notes that there is a tradeoff\
            \ with respect to the projection units learned by a model. In their findings,\
            \ a 32x32 patch size model learned very structured low frequency projection\
            \ units, while the equivalent 16x16 model learned high frequencies and\
            \ showed no clear structure."
        expected_impact: 2
        literature_references:
            - "[MLP Mixer paper](https://arxiv.org/pdf/2105.01601.pdf)"
        suggested_values:
            - 16
            - 32
        suggested_values_reasoning:
            16 and 32 are the values used in the original
            MLP Mixer paper
        ui_display_name: Patch Size
    token_size:
        ui_display_name: null
    width:
        internal_only: true
        ui_display_name: null
MT5:
    type:
        short_description:
            MT5 is a multilingual variant of T5 useful for multilingual NLP use cases.
        long_description:
            The mt5 encoder loads a pretrained MT5 (default google/mt5-base) model using the Hugging Face
            transformers package. MT5 is a multilingual variant of T5 trained on a dataset of 101 languages.
    d_ff:
        default_value_reasoning: Default value matches the pre-trained encoder.
        description_implications:
            If using a pre-trained encoder, this parameter will
            be automatically derived from the pre-trained model.
        expected_impact: 1
        ui_display_name: Dimensionality of Feed-Forward Layer
    d_kv:
        ui_display_name: null
    d_model:
        ui_display_name: null
    decoder_start_token_id:
        ui_display_name: null
    dropout_rate:
        default_value_reasoning: Huggingface default.
        description_implications:
            "Dropout is a computationally cheap regularization\
            \ method where during training, some neurons are randomly ignored or \u201C\
            dropped out\u201D. Increasing dropout has the effect of making the training\
            \ process more noisy and lowering overall network capacity, but it can\
            \ be an effective regularization method to reduce overfitting and improve\
            \ generalization."
        example_value:
            - 0.2
        expected_impact: 3
        literature_references:
            - https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html
        suggested_values: 0.05 - 0.8
        suggested_values_reasoning:
            Tuning dropout is really something to be done
            when all of the big choices about architecture have been settled. Consider
            starting with 0.5 and adjusting the dropout depending on observed model
            performance.
        ui_display_name: dropout_rate
    eos_token_id:
        default_value_reasoning: Default value used in pre-trained HF encoder.
        ui_display_name: End-of-Sentence Token Id
    feed_forward_proj:
        ui_display_name: null
    initializer_factor:
        ui_display_name: null
    is_encoder_decoder:
        ui_display_name: null
    layer_norm_epsilon:
        ui_display_name: null
    max_sequence_length:
        default_value_reasoning:
            Sets the maximum sequence length of the expected
            inputs, so input/output shapes are computed accurately.
        expected_impact: 1
        ui_display_name: null
    num_decoder_layers:
        ui_display_name: null
    num_heads:
        ui_display_name: null
    num_layers:
        default_value_reasoning:
            The default value matches the number of layers in
            the default pretrained encoder.
        description_implications:
            "The ideal number of transformer layers depends
            on the length and complexity of input sequences, as well as the task.


            If using a pre-trained encoder, this parameter will be automatically derived
            from the pre-trained model."
        example_value:
            - 8
        expected_impact: 1
        related_parameters:
            - pretrained_model_or_path
        suggested_values: 1 - 12
        suggested_values_reasoning:
            Increasing the number of layers may improve encoder
            performance.  However, more layers will increase training time and may
            cause overfitting.  Small numbers of layers usually work best.
        ui_display_name: Number of Transformer Layers
    pad_token_id:
        ui_display_name: null
    pretrained_kwargs:
        ui_display_name: null
    pretrained_model_name_or_path:
        ui_display_name: null
    reduce_output:
        ui_display_name: null
    relative_attention_num_buckets:
        ui_display_name: null
    saved_weights_in_checkpoint:
        default_value_reasoning:
            The weights of the encoder are not necessarily saved
            in the checkpoint. The user has to save them first.
        description_implications:
            The memory footprint for some of these encoders
            can be large.
        expected_impact: 1
        related_parameters:
            - skip_save_model
        suggested_values:
            - false
        suggested_values_reasoning:
            Some of these encoders are large, so it might
            be better to load them as needed, especially if 1. they're not used frequently
            2. the user doesn't have a lot of storage.
        ui_display_name: null
    tie_word_embeddings:
        default_value_reasoning:
            Keeping the word embeddings separate ensures maximum
            modeling flexibility.
        description_implications:
            The main tradeoff between True and False values
            is in compute costs and model flexibility. If set to False, the model
            will require more memory, but may be more flexible. If set to True, the
            opposite is true.
        example_value:
            - true
        expected_impact: 2
        suggested_values:
            - true
        suggested_values_reasoning:
            "If set to True, then the word embeddings will
            be shared between the encoder and decoder. There are two main reasons
            to set this value to True: (1) saving compute resources. Word embedding
            tables can be very large and using a single table between the encoder
            and decoder can cut one's memory usage in half. (2) If the domain of
            the generated text is highly similar to the input text. For example, if
            training a Question and Answering (QA) text model, where both the questions
            and answers are in the same language, the word embeddings used by the
            encoder are likely usable by the decoder and vice-versa. On the other
            hand, if training a translation model between two languages, the word
            embeddings are not likely to be shareable by both model components."
        ui_display_name: null
    tokenizer_class:
        ui_display_name: null
    trainable:
        expected_impact: 2
        ui_display_name: null
    use_cache:
        ui_display_name: null
    use_pretrained:
        ui_display_name: null
    vocab:
        default_value_reasoning:
            Computed and passed along internally according to
            preprocessing settings.
        example_value:
            - a
            - b
            - c
        expected_impact: 2
        internal_only: true
        ui_display_name: Not Displayed
    vocab_size:
        internal_only: true
        ui_display_name: Not displayed
ParallelCNN:
    type:
        short_description:
            Default option for processing sequence, audio, and text data types.
        long_description:
            The Parallel CNN works by first mapping the input integer sequence b x s (where b is the batch
            size and s is the length of the sequence) into a sequence of embeddings, then it passes the
            embedding through a number of parallel 1d convolutional layers with different filter size (by
            default 4 layers with filter size 2, 3, 4 and 5), followed by max pooling and concatenation.
            This single vector concatenating the outputs of the parallel convolutional layers is then passed
            through a stack of fully connected layers and returned as a b x h tensor where h is the output
            size of the last fully connected layer.
    activation:
        default_value_reasoning:
            The Rectified Linear Units (ReLU) function is the
            standard activation function used for adding non-linearity. It is simple,
            fast, and empirically works well (https://arxiv.org/abs/1803.08375).
        description_implications:
            Changing the activation functions has an impact
            on the computational load of the model and might require further hypterparameter
            tuning
        expected_impact: 1
        suggested_values:
            The default value will work well in the majority of the
            cases
        ui_display_name: Activation
    bias_initializer:
        default_value_reasoning:
            It is possible and common to initialize the biases
            to be zero, since the asymmetry breaking is provided by the small random
            numbers in the weights.
        description_implications:
            It's rare to see any performance gains from choosing
            a different bias initialization. Some practitioners like to use a small
            constant value such as 0.01 for all biases to ensure that all ReLU units
            are activated in the beginning and have some effect on the gradient. However,
            it's still an open question as to whether this provides consistent improvement.
        expected_impact: 1
        literature_references:
            - https://cs231n.github.io/neural-networks-2/
        related_parameters:
            - weights_initializer
        suggested_values: zeros
        suggested_values_reasoning:
            It is possible and common to initialize the biases
            to be zero, since the asymmetry breaking is provided by the small random
            numbers in the weights. For ReLU non-linearities, some people like to
            use small constant value such as 0.01 for all biases because this ensures
            that all ReLU units fire in the beginning and therefore obtain and propagate
            some gradient. However, it is not clear if this provides a consistent
            improvement (in fact some results seem to indicate that this performs
            worse) and it is more common to simply use 0 bias initialization.
        ui_display_name: Bias Initializer
    conv_layers:
        description_implications:
            The more layers that are specified the deeper and
            higher capacity the model will be. This makes it possible to potentially
            achieve better performance when a large amount of data is provided, but
            also makes the model more computationally expensive and potentially more
            prone to overfitting.
        example_value:
            - dropout: 0.1
              output_size: 128
            - norm: layer
              output_size: 64
        expected_impact: 1
        related_parameters:
            - num_conv_layers
        ui_display_name: Convolutional Layers
    dropout:
        default_value_reasoning:
            Dropout can cause training to become less stable.
            Consider start with a dropout-free baseline, and add dropout gradually
            in subsequent experiments.
        description_implications:
            "Dropout is a computationally cheap regularization\
            \ method where during training, some neurons are randomly ignored or \u201C\
            dropped out\u201D. Increasing dropout has the effect of making the training\
            \ process more noisy and lowering overall network capacity, but it can\
            \ be an effective regularization method to reduce overfitting and improve\
            \ generalization."
        example_value:
            - 0.2
        expected_impact: 3
        literature_references:
            - https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html
        suggested_values: 0.05 - 0.8
        suggested_values_reasoning:
            Tuning dropout is really something to be done
            when all of the big choices about architecture have been settled. Consider
            starting with 0.5 and adjusting the dropout depending on observed model
            performance.
        ui_display_name: Dropout
    embedding_size:
        default_value_reasoning: Not too big, not too small.
        description_implications:
            'An embedding is a relatively low-dimensional space
            that is used to translate high-dimensional vectors like words, which can
            have a large vocbulary size. Ideally, after an embedding is trained, it
            captures some of the semantics of the input by placing semantically similar
            inputs close together in the embedding space.


            In most cases, the embedding size is chosen empirically, by trial and
            error. From https://www.amazon.com/dp/1098115783, "one rule of thumb is
            to use the fourth root of the total number of unique categorical elements
            while another is that the embedding dimension should be approximately
            1.6 times the square root of the number of unique elements in the category,
            and no less than 600."


            Increasing the embedding size may cause the model to train more slowly,
            but the higher dimensionality can also improve overall quality.'
        expected_impact: 2
        literature_references:
            - https://developers.google.com/machine-learning/crash-course/embeddings/video-lecture
        suggested_values: 1.6 * sqrt(vocab_size)
        suggested_values_reasoning:
            Rule of thumb suggested by a deep learning textbook.
            Try models with smaller or larger embedding sizes to observe relative
            impact.
        ui_display_name: Embedding Size
    embeddings_on_cpu:
        default_value_reasoning:
            By default embeddings matrices are stored on GPU
            memory if a GPU is used, as it allows for faster access.
        description_implications:
            By default embeddings matrices are stored on GPU
            memory if a GPU is used, as it allows for faster access. However, in some
            cases when the vocabulary size is very large, the full embedding matrix
            may be really big and unwieldy to have in GPU memory. This parameter forces
            the placement of the embedding matrix in regular memory and the CPU is
            used to access them. This may slow down training due to additional data
            transfer between CPU and GPU memory, but can lead to healthier GPU memory
            resource usage.
        expected_impact: 1
        suggested_values:
            - false
        suggested_values_reasoning:
            If GPU memory is not a constraint, having embeddings
            stored and accessed within the GPU is faster.
        ui_display_name: Embeddings on CPU
    embeddings_trainable:
        ui_display_name: null
    fc_layers:
        default_value_reasoning:
            By default the stack is built by using num_fc_layers,
            output_size, use_bias, weights_initializer, bias_initializer, norm, norm_params,
            activation, dropout. When a list of dictionaries is provided, the stack
            is built following the parameters of each dict for building each layer.
        description_implications:
            The more layers that are specified the deeper and
            higher capacity the model will be. This makes it possible to potentially
            achieve better performance when a big anough amount of data is provided,
            but also makes the model more computationally expensive and potentially
            more prone to overfitting.
        example_value:
            - dropout: 0.1
              output_size: 128
            - norm: layer
              output_size: 64
        expected_impact: 1
        related_parameters:
            - output_size
            - use_bias
            - weights_initializer
            - bias_initializer
            - norm
            - norm_params
            - activation
            - dropout
        suggested_values_reasoning:
            It is easier to define a stack of fully connected
            layers by just specifying num_fc_layers, output_size and the other individual
            parameters. It will create a stack of layers with identical properties.
            Use this parameter only if you need a fine grained level of control of
            each individual layer in the stack.
        ui_display_name: Fully Connected Layers
    filter_size:
        ui_display_name: null
    max_sequence_length:
        default_value_reasoning:
            Sets the maximum sequence length of the expected
            inputs, so input/output shapes are computed accurately.
        expected_impact: 1
        ui_display_name: null
    norm:
        default_value_reasoning:
            While batch normalization and layer normalization
            usually lead to improvements, it can be useful to start with fewer bells
            and whistles.
        description_implications:
            Normalization helps stabilize the learning process
            and can have a regularizing effect that can help with generalization.
            It's often suggested that with normalization, you can use a higher learning
            rate.
        example_value:
            - batch
        expected_impact: 3
        literature_references:
            - https://machinelearningmastery.com/batch-normalization-for-training-of-deep-neural-networks/
        related_parameters:
            - norm_params
        suggested_values: '"batch" or "layer"'
        suggested_values_reasoning:
            Normalization tries to solve "internal covariate
            shift" that comes from the changing distributions of the inputs to layers
            deep in the network when weights are updated. For example, batch normalization
            standardizes the inputs to a layer for each mini-batch. Try out different
            normalizations to see if that helps with training stability
        ui_display_name: Normalization Type
    norm_params:
        default_value_reasoning:
            The default parameters that come with Torch's implementation
            of these normalization types are a trusted starting point.
        description_implications:
            There are a variety of ways a certain set of parameters
            specificed could influence performance here. Broadly speaking the different
            values passed in here allow for different levels of smoothness to be observed
            in the learning curves. Since setting this parameters depends on the type
            of `norm` set, see [BatchNorm2d](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html)
            for more information on the parameters to set for batch normalization,
            and see [LayerNorm](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html)
            for more information on the parameters to set for layer normalization.
        example_value:
            - affine: false
              momentum: 0.2
              num_features: 100
        expected_impact: 1
        literature_references:
            - "For BatchNorm2d: https://arxiv.org/abs/1502.03167


              For LayerNorm: https://arxiv.org/abs/1607.06450"
        related_parameters:
            - "`norm`"
        suggested_values: Depends on the type of `norm` set.
        suggested_values_reasoning: "NO"
        ui_display_name: Normalization Parameters
    num_conv_layers:
        description_implications:
            The more layers that are specified the deeper and
            higher capacity the model will be. This makes it possible to potentially
            achieve better performance when a large amount of data is provided, but
            also makes the model more computationally expensive and potentially more
            prone to overfitting.
        expected_impact: 2
        related_parameters:
            - conv_layers
        ui_display_name: Number of Convolutional Layers
    num_fc_layers:
        default_value_reasoning:
            The encoder already has learnable parameters.Sometimes
            the default is 1 for modules where the FC stack is used for shape management,
            or the only source of learnable parameters.
        description_implications:
            Increasing num_fc_layers will increase the capacity
            of the model. The model will be slower to train, and there's a higher
            risk of overfitting.
        example_value:
            - 1
        expected_impact: 1
        other_information:
            Not all modules that have fc_layers also have an accompanying
            num_fc_layers parameter. Where both are present, fc_layers takes precedent
            over num_fc_layers. Specifying num_fc_layers alone uses fully connected
            layers that are configured by the defaults in FCStack.
        related_parameters:
            - fc_layers
        suggested_values: 0-1
        suggested_values_reasoning:
            The full model likely contains many learnable
            parameters. Consider starting with very few, or without any additional
            fully connected layers and add them if you observe evidence of limited
            model capacity. Sometimes the default is 1 for modules where the FC stack
            is used for shape management, or the only source of learnable parameters.
        ui_display_name: Number of Fully Connected Layers
    num_filters:
        ui_display_name: null
    output_size:
        default_value_reasoning: A modest value, not too small, not too large.
        description_implications:
            If there are fully connected layers in this module,
            increasing the output size of each fully connected layer will increase
            the capacity of the model. However, the model may be slower to train,
            and there's a higher risk of overfitting. If it seems like the model could
            use even more capacity, consider increasing the number of fully connected
            layers, or explore other architectures.
        expected_impact: 2
        other_information:
            If num_fc_layers=0 and fc_layers=None, and there are no
            fully connected layers defined on the module, then this parameter may
            have no effect on the module's final output shape.
        related_parameters:
            - num_fc_layers, fc_layers
        suggested_values: 10 - 1024
        suggested_values_reasoning:
            Increasing the output size increases the capacity
            of the model. If this seems to have a positive effect, then it could be
            worth increasing the number of layers, or trying a different architecture
            with a larger capacity.
        ui_display_name: Output Size
    pool_function:
        ui_display_name: Pooling function
    pool_size:
        ui_display_name: null
    pretrained_embeddings:
        ui_display_name: null
    reduce_output:
        ui_display_name: null
    representation:
        ui_display_name: null
    should_embed:
        internal_only: true
        ui_display_name: Not displayed
    use_bias:
        default_value_reasoning:
            "Bias terms may improve model accuracy, and don't
            have much impact in terms of memory or training speed. For most models
            it is reasonable to use bias terms.


            Batch Normalization, however, adds a trainable shift parameter which is
            added to the activation. When Batch Normalization is used in a layer,
            bias terms are redundant and may be removed."
        description_implications:
            Bias terms may improve model accuracy, and don't
            have much impact in terms of memory or training speed. For most models
            it is reasonable to leave this parameter set to True.
        example_value:
            - true
        expected_impact: 1
        other_information:
            If fc_layers is not specified, or use_bias is not specified
            for individual layers, the value of use_bias will be used as the default
            for all layers.
        related_parameters:
            - bias_initializer, fc_layers
        suggested_values:
            - true
        ui_display_name: Use Bias
    vocab:
        default_value_reasoning:
            Computed and passed along internally according to
            preprocessing settings.
        example_value:
            - a
            - b
            - c
        expected_impact: 2
        internal_only: true
        ui_display_name: Not Displayed
    weights_initializer:
        default_value_reasoning: Taken from [this paper](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf).
        description_implications:
            The method you choose to initialize layer weights
            during training can have a big impact on performance as well as the reproducibility
            of your final model between runs. As an example, if you were to randomly
            initialize weights you would risk non-reproducibility (and possibly general
            training performance), but sticking with constant values for initialization
            might significantly increase the time needed for model convergence. Generally,
            choosing one of the probabilistic approaches strikes a balance between
            the two extremes, and the literature kicked off by the landmark [*Xavier
            et al.* paper](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)
            provides a few good options. See this nice discussion from [Weights and
            Biases](https://wandb.ai/site/articles/the-effects-of-weight-initialization-on-neural-nets#:~:text=Studies%20have%20shown%20that%20initializing,net%20train%20better%20and%20faster.)
            for more information.
        expected_impact: 3
        literature_references:
            - "Weights and Biases blog post: https://wandb.ai/site/articles/the-effects-of-weight-initialization-on-neural-nets#:~:text=Studies%20have%20shown%20that%20initializing,net%20train%20better%20and%20faster.


              Xavier et al. paper: http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf"
        suggested_values: xavier_uniform
        suggested_values_reasoning:
            Changing the weights initialization scheme is
            something to consider if a model is having trouble with convergence, or
            otherwise it is something to experiment with after other factors are considered.
            The default choice (`xavier_uniform`) is a suitable starting point for
            most tasks.
        ui_display_name: Layer Weights Initializer
PassthroughEncoder:
    type:
        short_description:
            Passes the raw input through to the combiner.
        long_description:
            The passthrough encoder simply returns the raw numerical values coming from the input
            placeholders as outputs. Inputs are of size `b` while outputs are of size `b x 1` where `b` is
            the batch size.
    input_size:
        other_information: Internal Only
        related_parameters:
            - "No"
        ui_display_name: Not Displayed
BinaryPassthroughEncoder:
    type:
        short_description:
            Passes the raw input through to the combiner.
        long_description:
            The passthrough encoder simply returns the raw numerical values coming from the input
            placeholders as outputs. Inputs are of size `b` while outputs are of size `b x 1` where `b` is
            the batch size.
    input_size:
        other_information: Internal Only
        related_parameters:
            - "No"
        ui_display_name: Not Displayed
CategoricalPassthroughEncoder:
    type:
        short_description:
            Passes the raw input through to the combiner.
        long_description:
            The passthrough encoder simply returns the raw numerical values coming from the input
            placeholders as outputs. Inputs are of size `b` while outputs are of size `b x 1` where `b` is
            the batch size.
    input_size:
        other_information: Internal Only
        related_parameters:
            - "No"
        ui_display_name: Not Displayed
ResNet:
    type:
        short_description:
            Residual network achieving very high performance on computer vision tasks.
        long_description:
            ResNet - short for residual network is part of a family of extremely deep architectures showing
            compelling accuracy and nice convergence behaviors for computer vision applications. It is a type
            of CNN architecture designed to support hundreds or thousands of convolutional layers.
    activation:
        default_value_reasoning:
            The Rectified Linear Units (ReLU) function is the
            standard activation function used for adding non-linearity. It is simple,
            fast, and empirically works well (https://arxiv.org/abs/1803.08375).
        description_implications:
            Changing the activation functions has an impact
            on the computational load of the model and might require further hypterparameter
            tuning
        expected_impact: 1
        suggested_values:
            The default value will work well in the majority of the
            cases
        ui_display_name: Activation
    batch_norm_epsilon:
        ui_display_name: null
    batch_norm_momentum:
        ui_display_name: null
    bias_initializer:
        default_value_reasoning:
            It is possible and common to initialize the biases
            to be zero, since the asymmetry breaking is provided by the small random
            numbers in the weights.
        description_implications:
            It's rare to see any performance gains from choosing
            a different bias initialization. Some practitioners like to use a small
            constant value such as 0.01 for all biases to ensure that all ReLU units
            are activated in the beginning and have some effect on the gradient. However,
            it's still an open question as to whether this provides consistent improvement.
        expected_impact: 1
        literature_references:
            - https://cs231n.github.io/neural-networks-2/
        related_parameters:
            - weights_initializer
        suggested_values: zeros
        suggested_values_reasoning:
            It is possible and common to initialize the biases
            to be zero, since the asymmetry breaking is provided by the small random
            numbers in the weights. For ReLU non-linearities, some people like to
            use small constant value such as 0.01 for all biases because this ensures
            that all ReLU units fire in the beginning and therefore obtain and propagate
            some gradient. However, it is not clear if this provides a consistent
            improvement (in fact some results seem to indicate that this performs
            worse) and it is more common to simply use 0 bias initialization.
        ui_display_name: Bias Initializer
    conv_stride:
        ui_display_name: null
    dropout:
        default_value_reasoning:
            Dropout can cause training to become less stable.
            Consider start with a dropout-free baseline, and add dropout gradually
            in subsequent experiments.
        description_implications:
            "Dropout is a computationally cheap regularization\
            \ method where during training, some neurons are randomly ignored or \u201C\
            dropped out\u201D. Increasing dropout has the effect of making the training\
            \ process more noisy and lowering overall network capacity, but it can\
            \ be an effective regularization method to reduce overfitting and improve\
            \ generalization."
        example_value:
            - 0.2
        expected_impact: 3
        literature_references:
            - https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html
        suggested_values: 0.05 - 0.8
        suggested_values_reasoning:
            Tuning dropout is really something to be done
            when all of the big choices about architecture have been settled. Consider
            starting with 0.5 and adjusting the dropout depending on observed model
            performance.
        ui_display_name: Dropout
    fc_layers:
        default_value_reasoning:
            By default the stack is built by using num_fc_layers,
            output_size, use_bias, weights_initializer, bias_initializer, norm, norm_params,
            activation, dropout. When a list of dictionaries is provided, the stack
            is built following the parameters of each dict for building each layer.
        description_implications:
            The more layers that are specified the deeper and
            higher capacity the model will be. This makes it possible to potentially
            achieve better performance when a big anough amount of data is provided,
            but also makes the model more computationally expensive and potentially
            more prone to overfitting.
        example_value:
            - dropout: 0.1
              output_size: 128
            - norm: layer
              output_size: 64
        expected_impact: 1
        related_parameters:
            - output_size
            - use_bias
            - weights_initializer
            - bias_initializer
            - norm
            - norm_params
            - activation
            - dropout
        suggested_values_reasoning:
            It is easier to define a stack of fully connected
            layers by just specifying num_fc_layers, output_size and the other individual
            parameters. It will create a stack of layers with identical properties.
            Use this parameter only if you need a fine grained level of control of
            each individual layer in the stack.
        ui_display_name: Fully Connected Layers
    first_pool_kernel_size:
        ui_display_name: null
    first_pool_stride:
        ui_display_name: null
    height:
        internal_only: true
        ui_display_name: null
    kernel_size:
        ui_display_name: null
    norm:
        default_value_reasoning:
            While batch normalization and layer normalization
            usually lead to improvements, it can be useful to start with fewer bells
            and whistles.
        description_implications:
            Normalization helps stabilize the learning process
            and can have a regularizing effect that can help with generalization.
            It's often suggested that with normalization, you can use a higher learning
            rate.
        example_value:
            - batch
        expected_impact: 3
        literature_references:
            - https://machinelearningmastery.com/batch-normalization-for-training-of-deep-neural-networks/
        related_parameters:
            - norm_params
        suggested_values: '"batch" or "layer"'
        suggested_values_reasoning:
            Normalization tries to solve "internal covariate
            shift" that comes from the changing distributions of the inputs to layers
            deep in the network when weights are updated. For example, batch normalization
            standardizes the inputs to a layer for each mini-batch. Try out different
            normalizations to see if that helps with training stability
        ui_display_name: Normalization Type
    norm_params:
        default_value_reasoning:
            The default parameters that come with Torch's implementation
            of these normalization types are a trusted starting point.
        description_implications:
            There are a variety of ways a certain set of parameters
            specificed could influence performance here. Broadly speaking the different
            values passed in here allow for different levels of smoothness to be observed
            in the learning curves. Since setting this parameters depends on the type
            of `norm` set, see [BatchNorm2d](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html)
            for more information on the parameters to set for batch normalization,
            and see [LayerNorm](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html)
            for more information on the parameters to set for layer normalization.
        example_value:
            - affine: false
              momentum: 0.2
              num_features: 100
        expected_impact: 1
        literature_references:
            - "For BatchNorm2d: https://arxiv.org/abs/1502.03167


              For LayerNorm: https://arxiv.org/abs/1607.06450"
        related_parameters:
            - "`norm`"
        suggested_values: Depends on the type of `norm` set.
        suggested_values_reasoning: "NO"
        ui_display_name: Normalization Parameters
    num_channels:
        ui_display_name: null
    num_fc_layers:
        default_value_reasoning:
            The encoder already has learnable parameters.Sometimes
            the default is 1 for modules where the FC stack is used for shape management,
            or the only source of learnable parameters.
        description_implications:
            Increasing num_fc_layers will increase the capacity
            of the model. The model will be slower to train, and there's a higher
            risk of overfitting.
        example_value:
            - 1
        expected_impact: 1
        other_information:
            Not all modules that have fc_layers also have an accompanying
            num_fc_layers parameter. Where both are present, fc_layers takes precedent
            over num_fc_layers. Specifying num_fc_layers alone uses fully connected
            layers that are configured by the defaults in FCStack.
        related_parameters:
            - fc_layers
        suggested_values: 0-1
        suggested_values_reasoning:
            The full model likely contains many learnable
            parameters. Consider starting with very few, or without any additional
            fully connected layers and add them if you observe evidence of limited
            model capacity. Sometimes the default is 1 for modules where the FC stack
            is used for shape management, or the only source of learnable parameters.
        ui_display_name: Number of Fully Connected Layers
    out_channels:
        ui_display_name: null
    output_size:
        default_value_reasoning: A modest value, not too small, not too large.
        description_implications:
            If there are fully connected layers in this module,
            increasing the output size of each fully connected layer will increase
            the capacity of the model. However, the model may be slower to train,
            and there's a higher risk of overfitting. If it seems like the model could
            use even more capacity, consider increasing the number of fully connected
            layers, or explore other architectures.
        expected_impact: 2
        other_information:
            If num_fc_layers=0 and fc_layers=None, and there are no
            fully connected layers defined on the module, then this parameter may
            have no effect on the module's final output shape.
        related_parameters:
            - num_fc_layers, fc_layers
        suggested_values: 10 - 1024
        suggested_values_reasoning:
            Increasing the output size increases the capacity
            of the model. If this seems to have a positive effect, then it could be
            worth increasing the number of layers, or trying a different architecture
            with a larger capacity.
        ui_display_name: Output Size
    resnet_size:
        ui_display_name: null
    use_bias:
        ui_display_name: null
    weights_initializer:
        default_value_reasoning: Taken from [this paper](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf).
        description_implications:
            The method you choose to initialize layer weights
            during training can have a big impact on performance as well as the reproducibility
            of your final model between runs. As an example, if you were to randomly
            initialize weights you would risk non-reproducibility (and possibly general
            training performance), but sticking with constant values for initialization
            might significantly increase the time needed for model convergence. Generally,
            choosing one of the probabilistic approaches strikes a balance between
            the two extremes, and the literature kicked off by the landmark [*Xavier
            et al.* paper](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)
            provides a few good options. See this nice discussion from [Weights and
            Biases](https://wandb.ai/site/articles/the-effects-of-weight-initialization-on-neural-nets#:~:text=Studies%20have%20shown%20that%20initializing,net%20train%20better%20and%20faster.)
            for more information.
        expected_impact: 3
        literature_references:
            - "Weights and Biases blog post: https://wandb.ai/site/articles/the-effects-of-weight-initialization-on-neural-nets#:~:text=Studies%20have%20shown%20that%20initializing,net%20train%20better%20and%20faster.


              Xavier et al. paper: http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf"
        suggested_values: xavier_uniform
        suggested_values_reasoning:
            Changing the weights initialization scheme is
            something to consider if a model is having trouble with convergence, or
            otherwise it is something to experiment with after other factors are considered.
            The default choice (`xavier_uniform`) is a suitable starting point for
            most tasks.
        ui_display_name: Layer Weights Initializer
    width:
        internal_only: true
        ui_display_name: null
RoBERTa:
    type:
        short_description:
            BERT based model that has higher accuracy and is easier parallelize due to larger mini-batches.
        long_description:
            The roberta encoder loads a pretrained RoBERTa (default roberta-base) model using the Hugging
            Face transformers package. RoBERTa builds on BERT and modifies key hyperparameters, removing the
            next-sentence pretraining objective and training with much larger mini-batches and learning
            rates.
        literature_references:
            - https://arxiv.org/abs/1907.11692
    bos_token_id:
        default_value_reasoning: Default value used in pre-trained HF encoder.
        ui_display_name: Beginning-of-Sentence Token Id
    eos_token_id:
        default_value_reasoning: <class 'int'>
        example_value:
            - Default value used in pre-trained HF encoder.
        expected_impact: 1
        ui_display_name: null
    max_sequence_length:
        default_value_reasoning:
            Sets the maximum sequence length of the expected
            inputs, so input/output shapes are computed accurately.
        expected_impact: 1
        ui_display_name: null
    pad_token_id:
        ui_display_name: null
    pretrained_kwargs:
        ui_display_name: null
    pretrained_model_name_or_path:
        ui_display_name: null
    reduce_output:
        ui_display_name: null
    saved_weights_in_checkpoint:
        default_value_reasoning:
            The weights of the encoder are not necessarily saved
            in the checkpoint. The user has to save them first.
        description_implications:
            The memory footprint for some of these encoders
            can be large.
        expected_impact: 1
        related_parameters:
            - skip_save_model
        suggested_values:
            - false
        suggested_values_reasoning:
            Some of these encoders are large, so it might
            be better to load them as needed, especially if 1. they're not used frequently
            2. the user doesn't have a lot of storage.
        ui_display_name: null
    trainable:
        expected_impact: 2
        ui_display_name: null
    use_pretrained:
        ui_display_name: null
    vocab:
        default_value_reasoning:
            Computed and passed along internally according to
            preprocessing settings.
        example_value:
            - a
            - b
            - c
        expected_impact: 2
        internal_only: true
        ui_display_name: Not Displayed
    vocab_size:
        internal_only: true
        ui_display_name: Not displayed
SequenceEmbed:
    type:
        short_description:
            Maps each element of the sequence to an embedding.
        long_description:
            The embed encoder simply maps each integer in the sequence to an embedding, creating a `b x s x h`
            tensor where `b` is the batch size, `s` is the length of the sequence and `h` is the embedding
            size. The tensor is reduced along the `s` dimension to obtain a single vector of size `h` for each
            element of the batch.
    dropout:
        default_value_reasoning:
            Dropout can cause training to become less stable.
            Consider start with a dropout-free baseline, and add dropout gradually
            in subsequent experiments.
        description_implications:
            "Dropout is a computationally cheap regularization\
            \ method where during training, some neurons are randomly ignored or \u201C\
            dropped out\u201D. Increasing dropout has the effect of making the training\
            \ process more noisy and lowering overall network capacity, but it can\
            \ be an effective regularization method to reduce overfitting and improve\
            \ generalization."
        example_value:
            - 0.2
        expected_impact: 3
        literature_references:
            - https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html
        suggested_values: 0.05 - 0.8
        suggested_values_reasoning:
            Tuning dropout is really something to be done
            when all of the big choices about architecture have been settled. Consider
            starting with 0.5 and adjusting the dropout depending on observed model
            performance.
        ui_display_name: Dropout
    embedding_size:
        default_value_reasoning: Not too big, not too small.
        description_implications:
            'An embedding is a relatively low-dimensional space
            that is used to translate high-dimensional vectors like words, which can
            have a large vocbulary size. Ideally, after an embedding is trained, it
            captures some of the semantics of the input by placing semantically similar
            inputs close together in the embedding space.


            In most cases, the embedding size is chosen empirically, by trial and
            error. From https://www.amazon.com/dp/1098115783, "one rule of thumb is
            to use the fourth root of the total number of unique categorical elements
            while another is that the embedding dimension should be approximately
            1.6 times the square root of the number of unique elements in the category,
            and no less than 600."


            Increasing the embedding size may cause the model to train more slowly,
            but the higher dimensionality can also improve overall quality.'
        expected_impact: 2
        literature_references:
            - https://developers.google.com/machine-learning/crash-course/embeddings/video-lecture
        suggested_values: 1.6 * sqrt(vocab_size)
        suggested_values_reasoning:
            Rule of thumb suggested by a deep learning textbook.
            Try models with smaller or larger embedding sizes to observe relative
            impact.
        ui_display_name: Embedding Size
    embeddings_on_cpu:
        default_value_reasoning:
            By default embeddings matrices are stored on GPU
            memory if a GPU is used, as it allows for faster access.
        description_implications:
            By default embeddings matrices are stored on GPU
            memory if a GPU is used, as it allows for faster access. However, in some
            cases when the vocabulary size is very large, the full embedding matrix
            may be really big and unwieldy to have in GPU memory. This parameter forces
            the placement of the embedding matrix in regular memory and the CPU is
            used to access them. This may slow down training due to additional data
            transfer between CPU and GPU memory, but can lead to healthier GPU memory
            resource usage.
        expected_impact: 1
        suggested_values:
            - false
        suggested_values_reasoning:
            If GPU memory is not a constraint, having embeddings
            stored and accessed within the GPU is faster.
        ui_display_name: Embeddings on CPU
    embeddings_trainable:
        ui_display_name: null
    max_sequence_length:
        default_value_reasoning:
            Sets the maximum sequence length of the expected
            inputs, so input/output shapes are computed accurately.
        expected_impact: 1
        ui_display_name: null
    pretrained_embeddings:
        ui_display_name: null
    reduce_output:
        ui_display_name: null
    representation:
        ui_display_name: null
    vocab:
        default_value_reasoning:
            Computed and passed along internally according to
            preprocessing settings.
        example_value:
            - a
            - b
            - c
        expected_impact: 2
        internal_only: true
        ui_display_name: Not Displayed
    weights_initializer:
        default_value_reasoning: Taken from [this paper](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf).
        description_implications:
            The method you choose to initialize layer weights
            during training can have a big impact on performance as well as the reproducibility
            of your final model between runs. As an example, if you were to randomly
            initialize weights you would risk non-reproducibility (and possibly general
            training performance), but sticking with constant values for initialization
            might significantly increase the time needed for model convergence. Generally,
            choosing one of the probabilistic approaches strikes a balance between
            the two extremes, and the literature kicked off by the landmark [*Xavier
            et al.* paper](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)
            provides a few good options. See this nice discussion from [Weights and
            Biases](https://wandb.ai/site/articles/the-effects-of-weight-initialization-on-neural-nets#:~:text=Studies%20have%20shown%20that%20initializing,net%20train%20better%20and%20faster.)
            for more information.
        expected_impact: 3
        literature_references:
            - "Weights and Biases blog post: https://wandb.ai/site/articles/the-effects-of-weight-initialization-on-neural-nets#:~:text=Studies%20have%20shown%20that%20initializing,net%20train%20better%20and%20faster.


              Xavier et al. paper: http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf"
        suggested_values: xavier_uniform
        suggested_values_reasoning:
            Changing the weights initialization scheme is
            something to consider if a model is having trouble with convergence, or
            otherwise it is something to experiment with after other factors are considered.
            The default choice (`xavier_uniform`) is a suitable starting point for
            most tasks.
        ui_display_name: Layer Weights Initializer
SequencePassthrough:
    type:
        short_description:
            Transforms sequence values to a floats then reduces to obtain a vector for each element.
        long_description:
            The passthrough encoder simply transforms each input value into a float value and adds a
            dimension to the input tensor, creating a b x s x 1 tensor where b is the batch size and s is
            the length of the sequence. The tensor is reduced along the s dimension to obtain a single
            vector of size h for each element of the batch.
    encoding_size:
        default_value_reasoning:
            The default `reduce_output` method does not use this
            parameter, so by default this parameter is not set.
        description_implications:
            This parameter must be equal to the size of the
            input. Otherwise, an error will occur.
        example_value:
            - 128
        expected_impact: 1
        related_parameters:
            - reduce_output
        suggested_values_reasoning: NONE
        ui_display_name: null
    max_sequence_length:
        default_value_reasoning:
            Sets the maximum sequence length of the expected
            inputs, so input/output shapes are computed accurately.
        expected_impact: 1
        ui_display_name: null
    reduce_output:
        ui_display_name: null
SetSparseEncoder:
    type:
        short_description:
            Maps raw values to sparse integer lists, then maps to dense/sparse embeddings, then reduces to final vector.
        long_description:
            The Embed encoder takes the raw binary values coming from the input placeholders and transforms
            them to sparse integer lists, then they are mapped to either dense or sparse embeddings (one-hot
            encodings), finally they are reduced on the sequence dimension and returned as an aggregated
            embedding vector. Inputs are of size b while outputs are of size b x h where b is the batch size
            and h is the dimensionality of the embeddings.
    activation:
        default_value_reasoning:
            The Rectified Linear Units (ReLU) function is the
            standard activation function used for adding non-linearity. It is simple,
            fast, and empirically works well (https://arxiv.org/abs/1803.08375).
        description_implications:
            Changing the activation functions has an impact
            on the computational load of the model and might require further hypterparameter
            tuning
        expected_impact: 1
        suggested_values:
            The default value will work well in the majority of the
            cases
        ui_display_name: Activation
    bias_initializer:
        default_value_reasoning:
            It is possible and common to initialize the biases
            to be zero, since the asymmetry breaking is provided by the small random
            numbers in the weights.
        description_implications:
            It's rare to see any performance gains from choosing
            a different bias initialization. Some practitioners like to use a small
            constant value such as 0.01 for all biases to ensure that all ReLU units
            are activated in the beginning and have some effect on the gradient. However,
            it's still an open question as to whether this provides consistent improvement.
        expected_impact: 1
        literature_references:
            - https://cs231n.github.io/neural-networks-2/
        related_parameters:
            - weights_initializer
        suggested_values: zeros
        suggested_values_reasoning:
            It is possible and common to initialize the biases
            to be zero, since the asymmetry breaking is provided by the small random
            numbers in the weights. For ReLU non-linearities, some people like to
            use small constant value such as 0.01 for all biases because this ensures
            that all ReLU units fire in the beginning and therefore obtain and propagate
            some gradient. However, it is not clear if this provides a consistent
            improvement (in fact some results seem to indicate that this performs
            worse) and it is more common to simply use 0 bias initialization.
        ui_display_name: Bias Initializer
    dropout:
        default_value_reasoning:
            Dropout can cause training to become less stable.
            Consider start with a dropout-free baseline, and add dropout gradually
            in subsequent experiments.
        description_implications:
            "Dropout is a computationally cheap regularization\
            \ method where during training, some neurons are randomly ignored or \u201C\
            dropped out\u201D. Increasing dropout has the effect of making the training\
            \ process more noisy and lowering overall network capacity, but it can\
            \ be an effective regularization method to reduce overfitting and improve\
            \ generalization."
        example_value:
            - 0.2
        expected_impact: 3
        literature_references:
            - https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html
        suggested_values: 0.05 - 0.8
        suggested_values_reasoning:
            Tuning dropout is really something to be done
            when all of the big choices about architecture have been settled. Consider
            starting with 0.5 and adjusting the dropout depending on observed model
            performance.
        ui_display_name: Dropout
    embedding_size:
        default_value_reasoning: Not too big, not too small.
        description_implications:
            'An embedding is a relatively low-dimensional space
            that is used to translate high-dimensional vectors like words, which can
            have a large vocbulary size. Ideally, after an embedding is trained, it
            captures some of the semantics of the input by placing semantically similar
            inputs close together in the embedding space.


            In most cases, the embedding size is chosen empirically, by trial and
            error. From https://www.amazon.com/dp/1098115783, "one rule of thumb is
            to use the fourth root of the total number of unique categorical elements
            while another is that the embedding dimension should be approximately
            1.6 times the square root of the number of unique elements in the category,
            and no less than 600."


            Increasing the embedding size may cause the model to train more slowly,
            but the higher dimensionality can also improve overall quality.'
        expected_impact: 2
        literature_references:
            - https://developers.google.com/machine-learning/crash-course/embeddings/video-lecture
        suggested_values: 1.6 * sqrt(vocab_size)
        suggested_values_reasoning:
            Rule of thumb suggested by a deep learning textbook.
            Try models with smaller or larger embedding sizes to observe relative
            impact.
        ui_display_name: Embedding Size
    embeddings_on_cpu:
        default_value_reasoning:
            By default embeddings matrices are stored on GPU
            memory if a GPU is used, as it allows for faster access.
        description_implications:
            By default embeddings matrices are stored on GPU
            memory if a GPU is used, as it allows for faster access. However, in some
            cases when the vocabulary size is very large, the full embedding matrix
            may be really big and unwieldy to have in GPU memory. This parameter forces
            the placement of the embedding matrix in regular memory and the CPU is
            used to access them. This may slow down training due to additional data
            transfer between CPU and GPU memory, but can lead to healthier GPU memory
            resource usage.
        expected_impact: 1
        suggested_values:
            - false
        suggested_values_reasoning:
            If GPU memory is not a constraint, having embeddings
            stored and accessed within the GPU is faster.
        ui_display_name: Embeddings on CPU
    embeddings_trainable:
        ui_display_name: null
    fc_layers:
        default_value_reasoning:
            By default the stack is built by using num_fc_layers,
            output_size, use_bias, weights_initializer, bias_initializer, norm, norm_params,
            activation, dropout. When a list of dictionaries is provided, the stack
            is built following the parameters of each dict for building each layer.
        description_implications:
            The more layers that are specified the deeper and
            higher capacity the model will be. This makes it possible to potentially
            achieve better performance when a big anough amount of data is provided,
            but also makes the model more computationally expensive and potentially
            more prone to overfitting.
        example_value:
            - dropout: 0.1
              output_size: 128
            - norm: layer
              output_size: 64
        expected_impact: 1
        related_parameters:
            - output_size
            - use_bias
            - weights_initializer
            - bias_initializer
            - norm
            - norm_params
            - activation
            - dropout
        suggested_values_reasoning:
            It is easier to define a stack of fully connected
            layers by just specifying num_fc_layers, output_size and the other individual
            parameters. It will create a stack of layers with identical properties.
            Use this parameter only if you need a fine grained level of control of
            each individual layer in the stack.
        ui_display_name: Fully Connected Layers
    norm:
        default_value_reasoning:
            While batch normalization and layer normalization
            usually lead to improvements, it can be useful to start with fewer bells
            and whistles.
        description_implications:
            Normalization helps stabilize the learning process
            and can have a regularizing effect that can help with generalization.
            It's often suggested that with normalization, you can use a higher learning
            rate.
        example_value:
            - batch
        expected_impact: 3
        literature_references:
            - https://machinelearningmastery.com/batch-normalization-for-training-of-deep-neural-networks/
        related_parameters:
            - norm_params
        suggested_values: '"batch" or "layer"'
        suggested_values_reasoning:
            Normalization tries to solve "internal covariate
            shift" that comes from the changing distributions of the inputs to layers
            deep in the network when weights are updated. For example, batch normalization
            standardizes the inputs to a layer for each mini-batch. Try out different
            normalizations to see if that helps with training stability
        ui_display_name: Normalization Type
    norm_params:
        default_value_reasoning:
            The default parameters that come with Torch's implementation
            of these normalization types are a trusted starting point.
        description_implications:
            There are a variety of ways a certain set of parameters
            specificed could influence performance here. Broadly speaking the different
            values passed in here allow for different levels of smoothness to be observed
            in the learning curves. Since setting this parameters depends on the type
            of `norm` set, see [BatchNorm2d](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html)
            for more information on the parameters to set for batch normalization,
            and see [LayerNorm](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html)
            for more information on the parameters to set for layer normalization.
        example_value:
            - affine: false
              momentum: 0.2
              num_features: 100
        expected_impact: 1
        literature_references:
            - "For BatchNorm2d: https://arxiv.org/abs/1502.03167


              For LayerNorm: https://arxiv.org/abs/1607.06450"
        related_parameters:
            - "`norm`"
        suggested_values: Depends on the type of `norm` set.
        suggested_values_reasoning: "NO"
        ui_display_name: Normalization Parameters
    num_fc_layers:
        default_value_reasoning:
            The encoder already has learnable parameters.Sometimes
            the default is 1 for modules where the FC stack is used for shape management,
            or the only source of learnable parameters.
        description_implications:
            Increasing num_fc_layers will increase the capacity
            of the model. The model will be slower to train, and there's a higher
            risk of overfitting.
        example_value:
            - 1
        expected_impact: 1
        other_information:
            Not all modules that have fc_layers also have an accompanying
            num_fc_layers parameter. Where both are present, fc_layers takes precedent
            over num_fc_layers. Specifying num_fc_layers alone uses fully connected
            layers that are configured by the defaults in FCStack.
        related_parameters:
            - fc_layers
        suggested_values: 0-1
        suggested_values_reasoning:
            The full model likely contains many learnable
            parameters. Consider starting with very few, or without any additional
            fully connected layers and add them if you observe evidence of limited
            model capacity. Sometimes the default is 1 for modules where the FC stack
            is used for shape management, or the only source of learnable parameters.
        ui_display_name: Number of Fully Connected Layers
    output_size:
        default_value_reasoning: A modest value, not too small, not too large.
        description_implications:
            If there are fully connected layers in this module,
            increasing the output size of each fully connected layer will increase
            the capacity of the model. However, the model may be slower to train,
            and there's a higher risk of overfitting. If it seems like the model could
            use even more capacity, consider increasing the number of fully connected
            layers, or explore other architectures.
        expected_impact: 2
        other_information:
            If num_fc_layers=0 and fc_layers=None, and there are no
            fully connected layers defined on the module, then this parameter may
            have no effect on the module's final output shape.
        related_parameters:
            - num_fc_layers, fc_layers
        suggested_values: 10 - 1024
        suggested_values_reasoning:
            Increasing the output size increases the capacity
            of the model. If this seems to have a positive effect, then it could be
            worth increasing the number of layers, or trying a different architecture
            with a larger capacity.
        ui_display_name: Output Size
    pretrained_embeddings:
        ui_display_name: null
    representation:
        ui_display_name: null
    use_bias:
        default_value_reasoning:
            "Bias terms may improve model accuracy, and don't
            have much impact in terms of memory or training speed. For most models
            it is reasonable to use bias terms.


            Batch Normalization, however, adds a trainable shift parameter which is
            added to the activation. When Batch Normalization is used in a layer,
            bias terms are redundant and may be removed."
        description_implications:
            Bias terms may improve model accuracy, and don't
            have much impact in terms of memory or training speed. For most models
            it is reasonable to leave this parameter set to True.
        example_value:
            - true
        expected_impact: 1
        other_information:
            If fc_layers is not specified, or use_bias is not specified
            for individual layers, the value of use_bias will be used as the default
            for all layers.
        related_parameters:
            - bias_initializer, fc_layers
        suggested_values:
            - true
        ui_display_name: Use Bias
    vocab:
        default_value_reasoning:
            Computed and passed along internally according to
            preprocessing settings.
        example_value:
            - a
            - b
            - c
        expected_impact: 2
        internal_only: true
        ui_display_name: Not Displayed
    weights_initializer:
        default_value_reasoning: Taken from [this paper](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf).
        description_implications:
            The method you choose to initialize layer weights
            during training can have a big impact on performance as well as the reproducibility
            of your final model between runs. As an example, if you were to randomly
            initialize weights you would risk non-reproducibility (and possibly general
            training performance), but sticking with constant values for initialization
            might significantly increase the time needed for model convergence. Generally,
            choosing one of the probabilistic approaches strikes a balance between
            the two extremes, and the literature kicked off by the landmark [*Xavier
            et al.* paper](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)
            provides a few good options. See this nice discussion from [Weights and
            Biases](https://wandb.ai/site/articles/the-effects-of-weight-initialization-on-neural-nets#:~:text=Studies%20have%20shown%20that%20initializing,net%20train%20better%20and%20faster.)
            for more information.
        expected_impact: 3
        literature_references:
            - "Weights and Biases blog post: https://wandb.ai/site/articles/the-effects-of-weight-initialization-on-neural-nets#:~:text=Studies%20have%20shown%20that%20initializing,net%20train%20better%20and%20faster.


              Xavier et al. paper: http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf"
        suggested_values: xavier_uniform
        suggested_values_reasoning:
            Changing the weights initialization scheme is
            something to consider if a model is having trouble with convergence, or
            otherwise it is something to experiment with after other factors are considered.
            The default choice (`xavier_uniform`) is a suitable starting point for
            most tasks.
        ui_display_name: Layer Weights Initializer
Stacked2DCNN:
    type:
        short_description:
            Stack of 2D convolutional layers followed by an optional stack of fully connected layers.
        long_description:
            Stack of 2D convolutional layers with optional normalization, dropout, and down-sampling
            pooling layers, followed by an optional stack of fully connected layers.
    conv_activation:
        expected_impact: 1
        ui_display_name: Convolutional Activation
    conv_bias:
        expected_impact: 1
        ui_display_name: Convolutional Bias
    conv_dropout:
        default_value_reasoning:
            Dropout can cause training to become less stable.
            Consider start with a dropout-free baseline, and add dropout gradually
            in subsequent experiments.
        description_implications:
            "Dropout is a computationally cheap regularization\
            \ method where during training, some neurons are randomly ignored or \u201C\
            dropped out\u201D. Increasing dropout has the effect of making the training\
            \ process more noisy and lowering overall network capacity, but it can\
            \ be an effective regularization method to reduce overfitting and improve\
            \ generalization."
        example_value:
            - 0.2
        expected_impact: 3
        literature_references:
            - https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html
        related_parameters:
            - "conv_dropout,

              fc_dropout"
        suggested_values: 0.05 - 0.8
        suggested_values_reasoning:
            Tuning dropout is really something to be done
            when all of the big choices about architecture have been settled. Consider
            starting with 0.5 and adjusting the dropout depending on observed model
            performance.
        ui_display_name: Convolutional Dropout
    conv_layers:
        description_implications:
            The more layers that are specified the deeper and
            higher capacity the model will be. This makes it possible to potentially
            achieve better performance when a large amount of data is provided, but
            also makes the model more computationally expensive and potentially more
            prone to overfitting.
        example_value:
            - dropout: 0.1
              output_size: 128
            - norm: layer
              output_size: 64
        expected_impact: 1
        related_parameters:
            - num_conv_layers
        ui_display_name: Convolutional Layers
    conv_norm:
        expected_impact: 2
        ui_display_name: Convolutional Normalization
    conv_norm_params:
        expected_impact: 1
        ui_display_name: Convolutional Normalization Parameters
    dilation:
        expected_impact: 1
        ui_display_name: Dilation
    fc_activation:
        default_value_reasoning:
            The Rectified Linear Units (ReLU) function is the
            standard activation function used for adding non-linearity. It is simple,
            fast, and empirically works well (https://arxiv.org/abs/1803.08375).
        description_implications:
            Changing the activation functions has an impact
            on the computational load of the model and might require further hypterparameter
            tuning
        example_value:
            - relu
        expected_impact: 1
        literature_references:
            - https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html
        related_parameters:
            - activation, activation_function, conv_activation, recurrent_activation
        suggested_values: relu, alternatively leakyRelu or elu
        suggested_values_reasoning:
            The default value will work well in the majority
            of the cases
        ui_display_name: FC Activation
    fc_bias_initializer:
        default_value_reasoning:
            It is possible and common to initialize the biases
            to be zero, since the asymmetry breaking is provided by the small random
            numbers in the weights.
        description_implications:
            It's rare to see any performance gains from choosing
            a different bias initialization. Some practitioners like to use a small
            constant value such as 0.01 for all biases to ensure that all ReLU units
            are activated in the beginning and have some effect on the gradient. However,
            it's still an open question as to whether this provides consistent improvement.
        expected_impact: 1
        literature_references:
            - https://cs231n.github.io/neural-networks-2/
        related_parameters:
            - weights_initializer
        suggested_values: zeros
        suggested_values_reasoning:
            It is possible and common to initialize the biases
            to be zero, since the asymmetry breaking is provided by the small random
            numbers in the weights. For ReLU non-linearities, some people like to
            use small constant value such as 0.01 for all biases because this ensures
            that all ReLU units fire in the beginning and therefore obtain and propagate
            some gradient. However, it is not clear if this provides a consistent
            improvement (in fact some results seem to indicate that this performs
            worse) and it is more common to simply use 0 bias initialization.
        ui_display_name: Bias Initializer
    fc_dropout:
        default_value_reasoning:
            Dropout can cause training to become less stable.
            Consider start with a dropout-free baseline, and add dropout gradually
            in subsequent experiments.
        description_implications:
            "Dropout is a computationally cheap regularization\
            \ method where during training, some neurons are randomly ignored or \u201C\
            dropped out\u201D. Increasing dropout has the effect of making the training\
            \ process more noisy and lowering overall network capacity, but it can\
            \ be an effective regularization method to reduce overfitting and improve\
            \ generalization."
        example_value:
            - 0.2
        expected_impact: 3
        literature_references:
            - https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html
        related_parameters:
            - "conv_dropout,

              fc_dropout"
        suggested_values: 0.05 - 0.8
        suggested_values_reasoning:
            Tuning dropout is really something to be done
            when all of the big choices about architecture have been settled. Consider
            starting with 0.5 and adjusting the dropout depending on observed model
            performance.
        ui_display_name: FC Dropout
    fc_layers:
        default_value_reasoning:
            By default the stack is built by using num_fc_layers,
            output_size, use_bias, weights_initializer, bias_initializer, norm, norm_params,
            activation, dropout. When a list of dictionaries is provided, the stack
            is built following the parameters of each dict for building each layer.
        description_implications:
            The more layers that are specified the deeper and
            higher capacity the model will be. This makes it possible to potentially
            achieve better performance when a big anough amount of data is provided,
            but also makes the model more computationally expensive and potentially
            more prone to overfitting.
        example_value:
            - dropout: 0.1
              output_size: 128
            - norm: layer
              output_size: 64
        expected_impact: 1
        related_parameters:
            - output_size
            - use_bias
            - weights_initializer
            - bias_initializer
            - norm
            - norm_params
            - activation
            - dropout
        suggested_values_reasoning:
            It is easier to define a stack of fully connected
            layers by just specifying num_fc_layers, output_size and the other individual
            parameters. It will create a stack of layers with identical properties.
            Use this parameter only if you need a fine grained level of control of
            each individual layer in the stack.
        ui_display_name: Fully Connected Layers
    fc_norm:
        description_implications:
            Normalization helps stabilize the learning process
            and can have a regularizing effect that can help with generalization.
            It's often suggested that with normalization, you can use a higher learning
            rate. See Torch's documentation on batch normalization or for layer see
            Torch's documentation on layer normalization.
        expected_impact: 2
        related_parameters:
            - fc_norm_params
        suggested_values: batch
        ui_display_name: Fully Connected Normalization
    fc_norm_params:
        description_implications:
            There are a variety of ways a certain set of parameters
            specificed could influence performance here. Broadly speaking the different
            values passed in here allow for different levels of smoothness to be observed
            in the learning curves. Since setting this parameters depends on the type
            of `norm` set, see [BatchNorm2d](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html)
            for more information on the parameters to set for batch normalization,
            and see [LayerNorm](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html)
            for more information on the parameters to set for layer normalization.
        expected_impact: 2
        related_parameters:
            - fc_norm
        suggested_values: Depends on the type of `norm` set.
        ui_display_name: Fully Connected Normalization Parameters
    fc_use_bias:
        expected_impact: 1
        ui_display_name: FC Use Bias
    fc_weights_initializer:
        expected_impact: 1
        ui_display_name: FC Weights Initializer
    groups:
        expected_impact: 1
        ui_display_name: Groups
    height:
        default_value_reasoning:
            Computed internally, automatically, based on image
            data preprocessing.
        internal_only: true
        ui_display_name: NOT DISPLAYED
    kernel_size:
        expected_impact: 1
        ui_display_name: Kernel Size
    num_channels:
        default_value_reasoning:
            Computed internally, automatically, based on image
            data preprocessing.
        ui_display_name: NOT DISPLAYED
    num_conv_layers:
        description_implications:
            The more layers that are specified the deeper and
            higher capacity the model will be. This makes it possible to potentially
            achieve better performance when a large amount of data is provided, but
            also makes the model more computationally expensive and potentially more
            prone to overfitting.
        expected_impact: 2
        related_parameters:
            - conv_layers
        ui_display_name: Number of Convolutional Layers
    num_fc_layers:
        default_value_reasoning:
            The encoder already has learnable parameters.Sometimes
            the default is 1 for modules where the FC stack is used for shape management,
            or the only source of learnable parameters.
        description_implications:
            Increasing num_fc_layers will increase the capacity
            of the model. The model will be slower to train, and there's a higher
            risk of overfitting.
        example_value:
            - 1
        expected_impact: 1
        other_information:
            Not all modules that have fc_layers also have an accompanying
            num_fc_layers parameter. Where both are present, fc_layers takes precedent
            over num_fc_layers. Specifying num_fc_layers alone uses fully connected
            layers that are configured by the defaults in FCStack.
        related_parameters:
            - fc_layers
        suggested_values: 0-1
        suggested_values_reasoning:
            The full model likely contains many learnable
            parameters. Consider starting with very few, or without any additional
            fully connected layers and add them if you observe evidence of limited
            model capacity. Sometimes the default is 1 for modules where the FC stack
            is used for shape management, or the only source of learnable parameters.
        ui_display_name: Number of Fully Connected Layers
    out_channels:
        expected_impact: 2
        ui_display_name: Number of Output Channels
    output_size:
        default_value_reasoning: A modest value, not too small, not too large.
        description_implications:
            If there are fully connected layers in this module,
            increasing the output size of each fully connected layer will increase
            the capacity of the model. However, the model may be slower to train,
            and there's a higher risk of overfitting. If it seems like the model could
            use even more capacity, consider increasing the number of fully connected
            layers, or explore other architectures.
        expected_impact: 2
        other_information:
            If num_fc_layers=0 and fc_layers=None, and there are no
            fully connected layers defined on the module, then this parameter may
            have no effect on the module's final output shape.
        related_parameters:
            - num_fc_layers, fc_layers
        suggested_values: 10 - 1024
        suggested_values_reasoning:
            Increasing the output size increases the capacity
            of the model. If this seems to have a positive effect, then it could be
            worth increasing the number of layers, or trying a different architecture
            with a larger capacity.
        ui_display_name: Output Size
    padding:
        default_value_reasoning:
            When padding is set to 'valid' like in the default
            case, no padding is added. As a default value putting in the raw image
            is the goal here.
        description_implications:
            By increasing the amount of padding, you can increase
            the accuracy of the image analysis for certain circumstances.
        example_value:
            - "'same'"
        expected_impact: 1
        literature_references:
            - https://www.geeksforgeeks.org/cnn-introduction-to-padding/
        related_parameters:
            - "padding_mode,

              resize method"
        suggested_values:
            "Same' padding if images are of different dimensions. \n\
            Specific [h, w] entries can be valuable on a per dataset basis."
        suggested_values_reasoning:
            If your images already have padding, there is
            no need to add padding, so the default is fine. If your images come in
            different dimensions, then 'same' padding can help pad the images to standardized
            dimensions. For certain images, adding padding to the edges can help the
            CNN process the images better which can improve model performance. This
            depends on the images however.
        ui_display_name: Padding
    padding_mode:
        expected_impact: 1
        ui_display_name: Padding Mode
    pool_dilation:
        expected_impact: 1
        ui_display_name: Pool Dilation
    pool_function:
        default_value_reasoning:
            "Within a given sliding window (e.g. a \"patch\"\
            \ of a 3-channel image), the maximum value for each channel is kept. All\
            \ other values in the patch are discarded. Repeat this step for every\
            \ patch and you have a more compact representation of the image. \n\n\
            Intuitively, each patch encodes the features from a particular part of\
            \ an image, and it is more informative to look at the most prominent features\
            \ of an image than the average of all of them."
        description_implications:
            Both average and max pooling can achieve strong
            performance.
        expected_impact: 1
        literature_references:
            - "https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html


              https://machinelearningmastery.com/pooling-layers-for-convolutional-neural-networks/"
        suggested_values: Default
        suggested_values_reasoning: "No"
        ui_display_name: Pooling function
    pool_kernel_size:
        expected_impact: 1
        ui_display_name: Pool Kernel Size
    pool_padding:
        expected_impact: 1
        ui_display_name: Pool Padding
    pool_stride:
        expected_impact: 1
        ui_display_name: Pool Stride
    stride:
        expected_impact: 1
        ui_display_name: Stride
    width:
        default_value_reasoning:
            Computed internally, automatically, based on image
            data preprocessing.
        internal_only: true
        ui_display_name: NOT DISPLAYED
StackedCNN:
    type:
        short_description:
            Maps inputs to embeddings then passes them through a stack of 1d convolutional layers.
        long_description:
            The Stacked CNN works by first mapping the input integer sequence b x s (where b is the batch
            size and s is the length of the sequence) into a sequence of embeddings, then it passes the
            embedding through a stack of 1d convolutional layers with different filter size (by default 6
            layers with filter size 7, 7, 3, 3, 3 and 3), followed by an optional final pool and by a
            flatten operation. This single flatten vector is then passed through a stack of fully connected
            layers and returned as a b x h tensor where h is the output size of the last fully connected
            layer.
    activation:
        default_value_reasoning:
            The Rectified Linear Units (ReLU) function is the
            standard activation function used for adding non-linearity. It is simple,
            fast, and empirically works well (https://arxiv.org/abs/1803.08375).
        description_implications:
            Changing the activation functions has an impact
            on the computational load of the model and might require further hypterparameter
            tuning
        expected_impact: 1
        suggested_values:
            The default value will work well in the majority of the
            cases
        ui_display_name: Activation
    bias_initializer:
        default_value_reasoning:
            It is possible and common to initialize the biases
            to be zero, since the asymmetry breaking is provided by the small random
            numbers in the weights.
        description_implications:
            It's rare to see any performance gains from choosing
            a different bias initialization. Some practitioners like to use a small
            constant value such as 0.01 for all biases to ensure that all ReLU units
            are activated in the beginning and have some effect on the gradient. However,
            it's still an open question as to whether this provides consistent improvement.
        expected_impact: 1
        literature_references:
            - https://cs231n.github.io/neural-networks-2/
        related_parameters:
            - weights_initializer
        suggested_values: zeros
        suggested_values_reasoning:
            It is possible and common to initialize the biases
            to be zero, since the asymmetry breaking is provided by the small random
            numbers in the weights. For ReLU non-linearities, some people like to
            use small constant value such as 0.01 for all biases because this ensures
            that all ReLU units fire in the beginning and therefore obtain and propagate
            some gradient. However, it is not clear if this provides a consistent
            improvement (in fact some results seem to indicate that this performs
            worse) and it is more common to simply use 0 bias initialization.
        ui_display_name: Bias Initializer
    conv_layers:
        description_implications:
            The more layers that are specified the deeper and
            higher capacity the model will be. This makes it possible to potentially
            achieve better performance when a large amount of data is provided, but
            also makes the model more computationally expensive and potentially more
            prone to overfitting.
        example_value:
            - dropout: 0.1
              output_size: 128
            - norm: layer
              output_size: 64
        expected_impact: 1
        related_parameters:
            - num_conv_layers
        ui_display_name: Convolutional Layers
    dilation_rate:
        default_value_reasoning:
            The standard discrete convolution is the same as
            a 1-dilated convolution.
        description_implications:
            Higher dilation rates increase the effective size
            of the convolutional filter.  Dilated convolution may improve performance
            if the data is very correlated locally and also contains long-term dependencies.
        example_value:
            - 2
        expected_impact: 1
        other_information: Dilated convolution is also known as atrous convolution.
        related_parameters:
            - filter_size
        suggested_values: 1-3
        suggested_values_reasoning:
            The dilation rate is a factor which increases
            the spacing between elements of the convolutional filter
        ui_display_name: Dilation Rate
    dropout:
        default_value_reasoning:
            Dropout can cause training to become less stable.
            Consider start with a dropout-free baseline, and add dropout gradually
            in subsequent experiments.
        description_implications:
            "Dropout is a computationally cheap regularization\
            \ method where during training, some neurons are randomly ignored or \u201C\
            dropped out\u201D. Increasing dropout has the effect of making the training\
            \ process more noisy and lowering overall network capacity, but it can\
            \ be an effective regularization method to reduce overfitting and improve\
            \ generalization."
        example_value:
            - 0.2
        expected_impact: 3
        literature_references:
            - https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html
        suggested_values: 0.05 - 0.8
        suggested_values_reasoning:
            Tuning dropout is really something to be done
            when all of the big choices about architecture have been settled. Consider
            starting with 0.5 and adjusting the dropout depending on observed model
            performance.
        ui_display_name: Dropout
    embedding_size:
        default_value_reasoning: Not too big, not too small.
        description_implications:
            'An embedding is a relatively low-dimensional space
            that is used to translate high-dimensional vectors like words, which can
            have a large vocbulary size. Ideally, after an embedding is trained, it
            captures some of the semantics of the input by placing semantically similar
            inputs close together in the embedding space.


            In most cases, the embedding size is chosen empirically, by trial and
            error. From https://www.amazon.com/dp/1098115783, "one rule of thumb is
            to use the fourth root of the total number of unique categorical elements
            while another is that the embedding dimension should be approximately
            1.6 times the square root of the number of unique elements in the category,
            and no less than 600."


            Increasing the embedding size may cause the model to train more slowly,
            but the higher dimensionality can also improve overall quality.'
        expected_impact: 2
        literature_references:
            - https://developers.google.com/machine-learning/crash-course/embeddings/video-lecture
        suggested_values: 1.6 * sqrt(vocab_size)
        suggested_values_reasoning:
            Rule of thumb suggested by a deep learning textbook.
            Try models with smaller or larger embedding sizes to observe relative
            impact.
        ui_display_name: Embedding Size
    embeddings_on_cpu:
        default_value_reasoning:
            By default embeddings matrices are stored on GPU
            memory if a GPU is used, as it allows for faster access.
        description_implications:
            By default embeddings matrices are stored on GPU
            memory if a GPU is used, as it allows for faster access. However, in some
            cases when the vocabulary size is very large, the full embedding matrix
            may be really big and unwieldy to have in GPU memory. This parameter forces
            the placement of the embedding matrix in regular memory and the CPU is
            used to access them. This may slow down training due to additional data
            transfer between CPU and GPU memory, but can lead to healthier GPU memory
            resource usage.
        expected_impact: 1
        suggested_values:
            - false
        suggested_values_reasoning:
            If GPU memory is not a constraint, having embeddings
            stored and accessed within the GPU is faster.
        ui_display_name: Embeddings on CPU
    embeddings_trainable:
        ui_display_name: null
    fc_layers:
        default_value_reasoning:
            By default the stack is built by using num_fc_layers,
            output_size, use_bias, weights_initializer, bias_initializer, norm, norm_params,
            activation, dropout. When a list of dictionaries is provided, the stack
            is built following the parameters of each dict for building each layer.
        description_implications:
            The more layers that are specified the deeper and
            higher capacity the model will be. This makes it possible to potentially
            achieve better performance when a big anough amount of data is provided,
            but also makes the model more computationally expensive and potentially
            more prone to overfitting.
        example_value:
            - dropout: 0.1
              output_size: 128
            - norm: layer
              output_size: 64
        expected_impact: 1
        related_parameters:
            - output_size
            - use_bias
            - weights_initializer
            - bias_initializer
            - norm
            - norm_params
            - activation
            - dropout
        suggested_values_reasoning:
            It is easier to define a stack of fully connected
            layers by just specifying num_fc_layers, output_size and the other individual
            parameters. It will create a stack of layers with identical properties.
            Use this parameter only if you need a fine grained level of control of
            each individual layer in the stack.
        ui_display_name: Fully Connected Layers
    filter_size:
        ui_display_name: null
    max_sequence_length:
        default_value_reasoning:
            Sets the maximum sequence length of the expected
            inputs, so input/output shapes are computed accurately.
        expected_impact: 1
        ui_display_name: null
    norm:
        default_value_reasoning:
            While batch normalization and layer normalization
            usually lead to improvements, it can be useful to start with fewer bells
            and whistles.
        description_implications:
            Normalization helps stabilize the learning process
            and can have a regularizing effect that can help with generalization.
            It's often suggested that with normalization, you can use a higher learning
            rate.
        example_value:
            - batch
        expected_impact: 3
        literature_references:
            - https://machinelearningmastery.com/batch-normalization-for-training-of-deep-neural-networks/
        related_parameters:
            - norm_params
        suggested_values: '"batch" or "layer"'
        suggested_values_reasoning:
            Normalization tries to solve "internal covariate
            shift" that comes from the changing distributions of the inputs to layers
            deep in the network when weights are updated. For example, batch normalization
            standardizes the inputs to a layer for each mini-batch. Try out different
            normalizations to see if that helps with training stability
        ui_display_name: Normalization Type
    norm_params:
        default_value_reasoning:
            The default parameters that come with Torch's implementation
            of these normalization types are a trusted starting point.
        description_implications:
            There are a variety of ways a certain set of parameters
            specificed could influence performance here. Broadly speaking the different
            values passed in here allow for different levels of smoothness to be observed
            in the learning curves. Since setting this parameters depends on the type
            of `norm` set, see [BatchNorm2d](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html)
            for more information on the parameters to set for batch normalization,
            and see [LayerNorm](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html)
            for more information on the parameters to set for layer normalization.
        example_value:
            - affine: false
              momentum: 0.2
              num_features: 100
        expected_impact: 1
        literature_references:
            - "For BatchNorm2d: https://arxiv.org/abs/1502.03167


              For LayerNorm: https://arxiv.org/abs/1607.06450"
        related_parameters:
            - "`norm`"
        suggested_values: Depends on the type of `norm` set.
        suggested_values_reasoning: "NO"
        ui_display_name: Normalization Parameters
    num_conv_layers:
        description_implications:
            The more layers that are specified the deeper and
            higher capacity the model will be. This makes it possible to potentially
            achieve better performance when a large amount of data is provided, but
            also makes the model more computationally expensive and potentially more
            prone to overfitting.
        expected_impact: 2
        related_parameters:
            - conv_layers
        ui_display_name: Number of Convolutional Layers
    num_fc_layers:
        default_value_reasoning:
            The encoder already has learnable parameters.Sometimes
            the default is 1 for modules where the FC stack is used for shape management,
            or the only source of learnable parameters.
        description_implications:
            Increasing num_fc_layers will increase the capacity
            of the model. The model will be slower to train, and there's a higher
            risk of overfitting.
        example_value:
            - 1
        expected_impact: 1
        other_information:
            Not all modules that have fc_layers also have an accompanying
            num_fc_layers parameter. Where both are present, fc_layers takes precedent
            over num_fc_layers. Specifying num_fc_layers alone uses fully connected
            layers that are configured by the defaults in FCStack.
        related_parameters:
            - fc_layers
        suggested_values: 0-1
        suggested_values_reasoning:
            The full model likely contains many learnable
            parameters. Consider starting with very few, or without any additional
            fully connected layers and add them if you observe evidence of limited
            model capacity. Sometimes the default is 1 for modules where the FC stack
            is used for shape management, or the only source of learnable parameters.
        ui_display_name: Number of Fully Connected Layers
    num_filters:
        ui_display_name: null
    output_size:
        default_value_reasoning: A modest value, not too small, not too large.
        description_implications:
            If there are fully connected layers in this module,
            increasing the output size of each fully connected layer will increase
            the capacity of the model. However, the model may be slower to train,
            and there's a higher risk of overfitting. If it seems like the model could
            use even more capacity, consider increasing the number of fully connected
            layers, or explore other architectures.
        expected_impact: 2
        other_information:
            If num_fc_layers=0 and fc_layers=None, and there are no
            fully connected layers defined on the module, then this parameter may
            have no effect on the module's final output shape.
        related_parameters:
            - num_fc_layers, fc_layers
        suggested_values: 10 - 1024
        suggested_values_reasoning:
            Increasing the output size increases the capacity
            of the model. If this seems to have a positive effect, then it could be
            worth increasing the number of layers, or trying a different architecture
            with a larger capacity.
        ui_display_name: Output Size
    padding:
        ui_display_name: null
    pool_function:
        ui_display_name: null
    pool_padding:
        ui_display_name: null
    pool_size:
        ui_display_name: null
    pool_strides:
        ui_display_name: null
    pretrained_embeddings:
        ui_display_name: null
    reduce_output:
        ui_display_name: null
    representation:
        ui_display_name: null
    should_embed:
        internal_only: true
        ui_display_name: Not displayed
    strides:
        default_value_reasoning:
            In general, it makes sense to have a smaller stride
            that fits the input. Imagining the simple 2D image as our input, two pixels
            next to eachother are strongly correlated while pixels that are further
            apart will have a comparatively weaker correlation. Consequently, a higher
            stride may cause significant information loss.
        description_implications:
            Changing the stride of a convolutional layer is
            one form of downsampling (another being pooling). In the case of a large
            stride, significant amounts of information is thrown away as the filter
            convolves over its input. This should be usually avoided but may be desirable
            in cases in which the user has some deep knowledge of the filter or of
            the rest of the model architecture that makes it comfortable to allow
            a higher level compression in the output feature map of this layer.
        example_value:
            - 1
        expected_impact: 2
        literature_references:
            - "[d2l.ai blog post](http://d2l.ai/chapter_convolutional-neural-networks/padding-and-strides.html)


              [machinelearningmastery blogpost](https://machinelearningmastery.com/padding-and-stride-for-convolutional-neural-networks/)


              [crossvalidated discussion](https://stats.stackexchange.com/questions/296027/choosing-filter-size-strides-etc-in-a-cnn)"
        related_parameters:
            - pool_strides, default_strides, default_pool_strides, block_strides
        suggested_values: 1-2
        suggested_values_reasoning:
            In general, points that are closer to eachother
            in the input feature space will be more strongly correlated to eachother,
            so it is a good idea to select a stride that captures these neighboring
            relationships.
        ui_display_name: Stride
    use_bias:
        ui_display_name: null
    vocab:
        default_value_reasoning:
            Computed and passed along internally according to
            preprocessing settings.
        example_value:
            - a
            - b
            - c
        expected_impact: 2
        internal_only: true
        ui_display_name: Not Displayed
    weights_initializer:
        default_value_reasoning: Taken from [this paper](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf).
        description_implications:
            The method you choose to initialize layer weights
            during training can have a big impact on performance as well as the reproducibility
            of your final model between runs. As an example, if you were to randomly
            initialize weights you would risk non-reproducibility (and possibly general
            training performance), but sticking with constant values for initialization
            might significantly increase the time needed for model convergence. Generally,
            choosing one of the probabilistic approaches strikes a balance between
            the two extremes, and the literature kicked off by the landmark [*Xavier
            et al.* paper](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)
            provides a few good options. See this nice discussion from [Weights and
            Biases](https://wandb.ai/site/articles/the-effects-of-weight-initialization-on-neural-nets#:~:text=Studies%20have%20shown%20that%20initializing,net%20train%20better%20and%20faster.)
            for more information.
        expected_impact: 3
        literature_references:
            - "Weights and Biases blog post: https://wandb.ai/site/articles/the-effects-of-weight-initialization-on-neural-nets#:~:text=Studies%20have%20shown%20that%20initializing,net%20train%20better%20and%20faster.


              Xavier et al. paper: http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf"
        suggested_values: xavier_uniform
        suggested_values_reasoning:
            Changing the weights initialization scheme is
            something to consider if a model is having trouble with convergence, or
            otherwise it is something to experiment with after other factors are considered.
            The default choice (`xavier_uniform`) is a suitable starting point for
            most tasks.
        ui_display_name: Layer Weights Initializer
StackedCNNRNN:
    type:
        short_description:
            Maps inputs to embeddings, passes them through convolutional layer stack, then recurrent layer stack.
        long_description:
            The cnnrnn encoder works by first mapping the input integer sequence b x s (where b is the batch
            size and s is the length of the sequence) into a sequence of embeddings, then it passes the
            embedding through a stack of convolutional layers (by default 2), that is followed by a stack of
            recurrent layers (by default 1), followed by a reduce operation that by default only returns the
            last output, but can perform other reduce functions.
    activation:
        ui_display_name: null
    bias_initializer:
        default_value_reasoning:
            It is possible and common to initialize the biases
            to be zero, since the asymmetry breaking is provided by the small random
            numbers in the weights.
        description_implications:
            It's rare to see any performance gains from choosing
            a different bias initialization. Some practitioners like to use a small
            constant value such as 0.01 for all biases to ensure that all ReLU units
            are activated in the beginning and have some effect on the gradient. However,
            it's still an open question as to whether this provides consistent improvement.
        expected_impact: 1
        literature_references:
            - https://cs231n.github.io/neural-networks-2/
        related_parameters:
            - weights_initializer
        suggested_values: zeros
        suggested_values_reasoning:
            It is possible and common to initialize the biases
            to be zero, since the asymmetry breaking is provided by the small random
            numbers in the weights. For ReLU non-linearities, some people like to
            use small constant value such as 0.01 for all biases because this ensures
            that all ReLU units fire in the beginning and therefore obtain and propagate
            some gradient. However, it is not clear if this provides a consistent
            improvement (in fact some results seem to indicate that this performs
            worse) and it is more common to simply use 0 bias initialization.
        ui_display_name: Bias Initializer
    bidirectional:
        ui_display_name: null
    cell_type:
        ui_display_name: null
    conv_activation:
        ui_display_name: null
    conv_dropout:
        default_value_reasoning:
            Dropout can cause training to become less stable.
            Consider start with a dropout-free baseline, and add dropout gradually
            in subsequent experiments.
        description_implications:
            "Dropout is a computationally cheap regularization\
            \ method where during training, some neurons are randomly ignored or \u201C\
            dropped out\u201D. Increasing dropout has the effect of making the training\
            \ process more noisy and lowering overall network capacity, but it can\
            \ be an effective regularization method to reduce overfitting and improve\
            \ generalization."
        example_value:
            - 0.2
        expected_impact: 3
        literature_references:
            - https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html
        related_parameters:
            - "conv_dropout,

              dropout,

              recurrent_dropout,

              fc_dropout"
        suggested_values: 0.05 - 0.8
        suggested_values_reasoning:
            Tuning dropout is really something to be done
            when all of the big choices about architecture have been settled. Consider
            starting with 0.5 and adjusting the dropout depending on observed model
            performance.
        ui_display_name: Convolutional Dropout
    conv_layers:
        description_implications:
            The more layers that are specified the deeper and
            higher capacity the model will be. This makes it possible to potentially
            achieve better performance when a large amount of data is provided, but
            also makes the model more computationally expensive and potentially more
            prone to overfitting.
        example_value:
            - dropout: 0.1
              output_size: 128
            - norm: layer
              output_size: 64
        expected_impact: 1
        related_parameters:
            - num_conv_layers
        ui_display_name: Convolutional Layers
    dilation_rate:
        default_value_reasoning:
            The standard discrete convolution is the same as
            a 1-dilated convolution.
        description_implications:
            Higher dilation rates increase the effective size
            of the convolutional filter.  Dilated convolution may improve performance
            if the data is very correlated locally and also contains long-term dependencies.
        example_value:
            - 2
        expected_impact: 1
        other_information: Dilated convolution is also known as atrous convolution.
        related_parameters:
            - filter_size
        suggested_values: 1-3
        suggested_values_reasoning:
            The dilation rate is a factor which increases
            the spacing between elements of the convolutional filter
        ui_display_name: Dilation Rate
    dropout:
        default_value_reasoning:
            Dropout can cause training to become less stable.
            Consider start with a dropout-free baseline, and add dropout gradually
            in subsequent experiments.
        description_implications:
            "Dropout is a computationally cheap regularization\
            \ method where during training, some neurons are randomly ignored or \u201C\
            dropped out\u201D. Increasing dropout has the effect of making the training\
            \ process more noisy and lowering overall network capacity, but it can\
            \ be an effective regularization method to reduce overfitting and improve\
            \ generalization."
        example_value:
            - 0.2
        expected_impact: 3
        literature_references:
            - https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html
        related_parameters:
            - "conv_dropout,

              dropout,

              recurrent_dropout,

              fc_dropout"
        suggested_values: 0.05 - 0.8
        suggested_values_reasoning:
            Tuning dropout is really something to be done
            when all of the big choices about architecture have been settled. Consider
            starting with 0.5 and adjusting the dropout depending on observed model
            performance.
        ui_display_name: Dropout
    embedding_size:
        default_value_reasoning: Not too big, not too small.
        description_implications:
            'An embedding is a relatively low-dimensional space
            that is used to translate high-dimensional vectors like words, which can
            have a large vocbulary size. Ideally, after an embedding is trained, it
            captures some of the semantics of the input by placing semantically similar
            inputs close together in the embedding space.


            In most cases, the embedding size is chosen empirically, by trial and
            error. From https://www.amazon.com/dp/1098115783, "one rule of thumb is
            to use the fourth root of the total number of unique categorical elements
            while another is that the embedding dimension should be approximately
            1.6 times the square root of the number of unique elements in the category,
            and no less than 600."


            Increasing the embedding size may cause the model to train more slowly,
            but the higher dimensionality can also improve overall quality.'
        expected_impact: 2
        literature_references:
            - https://developers.google.com/machine-learning/crash-course/embeddings/video-lecture
        suggested_values: 1.6 * sqrt(vocab_size)
        suggested_values_reasoning:
            Rule of thumb suggested by a deep learning textbook.
            Try models with smaller or larger embedding sizes to observe relative
            impact.
        ui_display_name: Embedding Size
    embeddings_on_cpu:
        default_value_reasoning:
            By default embeddings matrices are stored on GPU
            memory if a GPU is used, as it allows for faster access.
        description_implications:
            By default embeddings matrices are stored on GPU
            memory if a GPU is used, as it allows for faster access. However, in some
            cases when the vocabulary size is very large, the full embedding matrix
            may be really big and unwieldy to have in GPU memory. This parameter forces
            the placement of the embedding matrix in regular memory and the CPU is
            used to access them. This may slow down training due to additional data
            transfer between CPU and GPU memory, but can lead to healthier GPU memory
            resource usage.
        expected_impact: 1
        suggested_values:
            - false
        suggested_values_reasoning:
            If GPU memory is not a constraint, having embeddings
            stored and accessed within the GPU is faster.
        ui_display_name: Embeddings on CPU
    embeddings_trainable:
        ui_display_name: null
    fc_activation:
        default_value_reasoning:
            The Rectified Linear Units (ReLU) function is the
            standard activation function used for adding non-linearity. It is simple,
            fast, and empirically works well (https://arxiv.org/abs/1803.08375).
        description_implications:
            Changing the activation functions has an impact
            on the computational load of the model and might require further hypterparameter
            tuning
        example_value:
            - relu
        expected_impact: 1
        literature_references:
            - https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html
        related_parameters:
            - activation, activation_function, conv_activation, recurrent_activation
        suggested_values: relu, alternatively leakyRelu or elu
        suggested_values_reasoning:
            The default value will work well in the majority
            of the cases
        ui_display_name: FC Activation
    fc_dropout:
        default_value_reasoning:
            Dropout can cause training to become less stable.
            Consider start with a dropout-free baseline, and add dropout gradually
            in subsequent experiments.
        description_implications:
            "Dropout is a computationally cheap regularization\
            \ method where during training, some neurons are randomly ignored or \u201C\
            dropped out\u201D. Increasing dropout has the effect of making the training\
            \ process more noisy and lowering overall network capacity, but it can\
            \ be an effective regularization method to reduce overfitting and improve\
            \ generalization."
        example_value:
            - 0.2
        expected_impact: 3
        literature_references:
            - https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html
        related_parameters:
            - "conv_dropout,

              dropout,

              recurrent_dropout,

              fc_dropout"
        suggested_values: 0.05 - 0.8
        suggested_values_reasoning:
            Tuning dropout is really something to be done
            when all of the big choices about architecture have been settled. Consider
            starting with 0.5 and adjusting the dropout depending on observed model
            performance.
        ui_display_name: FC Dropout
    fc_layers:
        default_value_reasoning:
            By default the stack is built by using num_fc_layers,
            output_size, use_bias, weights_initializer, bias_initializer, norm, norm_params,
            activation, dropout. When a list of dictionaries is provided, the stack
            is built following the parameters of each dict for building each layer.
        description_implications:
            The more layers that are specified the deeper and
            higher capacity the model will be. This makes it possible to potentially
            achieve better performance when a big anough amount of data is provided,
            but also makes the model more computationally expensive and potentially
            more prone to overfitting.
        example_value:
            - dropout: 0.1
              output_size: 128
            - norm: layer
              output_size: 64
        expected_impact: 1
        related_parameters:
            - output_size
            - use_bias
            - weights_initializer
            - bias_initializer
            - norm
            - norm_params
            - activation
            - dropout
        suggested_values_reasoning:
            It is easier to define a stack of fully connected
            layers by just specifying num_fc_layers, output_size and the other individual
            parameters. It will create a stack of layers with identical properties.
            Use this parameter only if you need a fine grained level of control of
            each individual layer in the stack.
        ui_display_name: Fully Connected Layers
    filter_size:
        ui_display_name: null
    max_sequence_length:
        default_value_reasoning:
            Sets the maximum sequence length of the expected
            inputs, so input/output shapes are computed accurately.
        expected_impact: 1
        ui_display_name: null
    norm:
        default_value_reasoning:
            While batch normalization and layer normalization
            usually lead to improvements, it can be useful to start with fewer bells
            and whistles.
        description_implications:
            Normalization helps stabilize the learning process
            and can have a regularizing effect that can help with generalization.
            It's often suggested that with normalization, you can use a higher learning
            rate.
        example_value:
            - batch
        expected_impact: 3
        literature_references:
            - https://machinelearningmastery.com/batch-normalization-for-training-of-deep-neural-networks/
        related_parameters:
            - norm_params
        suggested_values: '"batch" or "layer"'
        suggested_values_reasoning:
            Normalization tries to solve "internal covariate
            shift" that comes from the changing distributions of the inputs to layers
            deep in the network when weights are updated. For example, batch normalization
            standardizes the inputs to a layer for each mini-batch. Try out different
            normalizations to see if that helps with training stability
        ui_display_name: Normalization Type
    norm_params:
        default_value_reasoning:
            The default parameters that come with Torch's implementation
            of these normalization types are a trusted starting point.
        description_implications:
            There are a variety of ways a certain set of parameters
            specificed could influence performance here. Broadly speaking the different
            values passed in here allow for different levels of smoothness to be observed
            in the learning curves. Since setting this parameters depends on the type
            of `norm` set, see [BatchNorm2d](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html)
            for more information on the parameters to set for batch normalization,
            and see [LayerNorm](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html)
            for more information on the parameters to set for layer normalization.
        example_value:
            - affine: false
              momentum: 0.2
              num_features: 100
        expected_impact: 1
        literature_references:
            - "For BatchNorm2d: https://arxiv.org/abs/1502.03167


              For LayerNorm: https://arxiv.org/abs/1607.06450"
        related_parameters:
            - "`norm`"
        suggested_values: Depends on the type of `norm` set.
        suggested_values_reasoning: "NO"
        ui_display_name: Normalization Parameters
    num_conv_layers:
        description_implications:
            The more layers that are specified the deeper and
            higher capacity the model will be. This makes it possible to potentially
            achieve better performance when a large amount of data is provided, but
            also makes the model more computationally expensive and potentially more
            prone to overfitting.
        expected_impact: 2
        related_parameters:
            - conv_layers
        ui_display_name: Number of Convolutional Layers
    num_fc_layers:
        default_value_reasoning:
            The encoder already has learnable parameters.Sometimes
            the default is 1 for modules where the FC stack is used for shape management,
            or the only source of learnable parameters.
        description_implications:
            Increasing num_fc_layers will increase the capacity
            of the model. The model will be slower to train, and there's a higher
            risk of overfitting.
        example_value:
            - 1
        expected_impact: 1
        other_information:
            Not all modules that have fc_layers also have an accompanying
            num_fc_layers parameter. Where both are present, fc_layers takes precedent
            over num_fc_layers. Specifying num_fc_layers alone uses fully connected
            layers that are configured by the defaults in FCStack.
        related_parameters:
            - fc_layers
        suggested_values: 0-1
        suggested_values_reasoning:
            The full model likely contains many learnable
            parameters. Consider starting with very few, or without any additional
            fully connected layers and add them if you observe evidence of limited
            model capacity. Sometimes the default is 1 for modules where the FC stack
            is used for shape management, or the only source of learnable parameters.
        ui_display_name: Number of Fully Connected Layers
    num_filters:
        ui_display_name: null
    num_rec_layers:
        ui_display_name: null
    output_size:
        default_value_reasoning: A modest value, not too small, not too large.
        description_implications:
            If there are fully connected layers in this module,
            increasing the output size of each fully connected layer will increase
            the capacity of the model. However, the model may be slower to train,
            and there's a higher risk of overfitting. If it seems like the model could
            use even more capacity, consider increasing the number of fully connected
            layers, or explore other architectures.
        expected_impact: 2
        other_information:
            If num_fc_layers=0 and fc_layers=None, and there are no
            fully connected layers defined on the module, then this parameter may
            have no effect on the module's final output shape.
        related_parameters:
            - num_fc_layers, fc_layers
        suggested_values: 10 - 1024
        suggested_values_reasoning:
            Increasing the output size increases the capacity
            of the model. If this seems to have a positive effect, then it could be
            worth increasing the number of layers, or trying a different architecture
            with a larger capacity.
        ui_display_name: Output Size
    padding:
        ui_display_name: null
    pool_function:
        ui_display_name: null
    pool_padding:
        ui_display_name: null
    pool_size:
        ui_display_name: null
    pool_strides:
        ui_display_name: null
    pretrained_embeddings:
        ui_display_name: null
    recurrent_activation:
        default_value_reasoning: sigmoid' is commonly used
        expected_impact: 3
        other_information:
            I don't think that this parameter is used anywhere in the
            code base. It's being passed down but not used in the actual RNN forwarding
            functions.
        suggested_values: sigmoid, ReLu, tanh
        ui_display_name: null
    recurrent_dropout:
        default_value_reasoning:
            Dropout can cause training to become less stable.
            Consider start with a dropout-free baseline, and add dropout gradually
            in subsequent experiments.
        description_implications:
            "Dropout is a computationally cheap regularization\
            \ method where during training, some neurons are randomly ignored or \u201C\
            dropped out\u201D. Increasing dropout has the effect of making the training\
            \ process more noisy and lowering overall network capacity, but it can\
            \ be an effective regularization method to reduce overfitting and improve\
            \ generalization."
        example_value:
            - 0.2
        expected_impact: 3
        literature_references:
            - https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html
        related_parameters:
            - "conv_dropout,

              dropout,

              recurrent_dropout,

              fc_dropout"
        suggested_values: 0.05 - 0.8
        suggested_values_reasoning:
            Tuning dropout is really something to be done
            when all of the big choices about architecture have been settled. Consider
            starting with 0.5 and adjusting the dropout depending on observed model
            performance.
        ui_display_name: Recurrent Dropout
    recurrent_initializer:
        ui_display_name: null
    reduce_output:
        ui_display_name: null
    representation:
        ui_display_name: null
    should_embed:
        internal_only: true
        ui_display_name: Not displayed
    state_size:
        ui_display_name: null
    strides:
        default_value_reasoning:
            In general, it makes sense to have a smaller stride
            that fits the input. Imagining the simple 2D image as our input, two pixels
            next to eachother are strongly correlated while pixels that are further
            apart will have a comparatively weaker correlation. Consequently, a higher
            stride may cause significant information loss.
        description_implications:
            Changing the stride of a convolutional layer is
            one form of downsampling (another being pooling). In the case of a large
            stride, significant amounts of information is thrown away as the filter
            convolves over its input. This should be usually avoided but may be desirable
            in cases in which the user has some deep knowledge of the filter or of
            the rest of the model architecture that makes it comfortable to allow
            a higher level compression in the output feature map of this layer.
        example_value:
            - 1
        expected_impact: 2
        literature_references:
            - "[d2l.ai blog post](http://d2l.ai/chapter_convolutional-neural-networks/padding-and-strides.html)


              [machinelearningmastery blogpost](https://machinelearningmastery.com/padding-and-stride-for-convolutional-neural-networks/)


              [crossvalidated discussion](https://stats.stackexchange.com/questions/296027/choosing-filter-size-strides-etc-in-a-cnn)"
        related_parameters:
            - pool_strides, default_strides, default_pool_strides, block_strides
        suggested_values: 1-2
        suggested_values_reasoning:
            In general, points that are closer to eachother
            in the input feature space will be more strongly correlated to eachother,
            so it is a good idea to select a stride that captures these neighboring
            relationships.
        ui_display_name: Stride
    unit_forget_bias:
        ui_display_name: null
    use_bias:
        default_value_reasoning:
            "Bias terms may improve model accuracy, and don't
            have much impact in terms of memory or training speed. For most models
            it is reasonable to use bias terms.


            Batch Normalization, however, adds a trainable shift parameter which is
            added to the activation. When Batch Normalization is used in a layer,
            bias terms are redundant and may be removed."
        description_implications:
            Bias terms may improve model accuracy, and don't
            have much impact in terms of memory or training speed. For most models
            it is reasonable to leave this parameter set to True.
        example_value:
            - true
        expected_impact: 1
        other_information:
            If fc_layers is not specified, or use_bias is not specified
            for individual layers, the value of use_bias will be used as the default
            for all layers.
        related_parameters:
            - bias_initializer, fc_layers
        suggested_values:
            - true
        ui_display_name: Use Bias
    vocab:
        default_value_reasoning:
            Computed and passed along internally according to
            preprocessing settings.
        example_value:
            - a
            - b
            - c
        expected_impact: 2
        internal_only: true
        ui_display_name: Not Displayed
    weights_initializer:
        default_value_reasoning: Taken from [this paper](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf).
        description_implications:
            The method you choose to initialize layer weights
            during training can have a big impact on performance as well as the reproducibility
            of your final model between runs. As an example, if you were to randomly
            initialize weights you would risk non-reproducibility (and possibly general
            training performance), but sticking with constant values for initialization
            might significantly increase the time needed for model convergence. Generally,
            choosing one of the probabilistic approaches strikes a balance between
            the two extremes, and the literature kicked off by the landmark [*Xavier
            et al.* paper](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)
            provides a few good options. See this nice discussion from [Weights and
            Biases](https://wandb.ai/site/articles/the-effects-of-weight-initialization-on-neural-nets#:~:text=Studies%20have%20shown%20that%20initializing,net%20train%20better%20and%20faster.)
            for more information.
        expected_impact: 3
        literature_references:
            - "Weights and Biases blog post: https://wandb.ai/site/articles/the-effects-of-weight-initialization-on-neural-nets#:~:text=Studies%20have%20shown%20that%20initializing,net%20train%20better%20and%20faster.


              Xavier et al. paper: http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf"
        suggested_values: xavier_uniform
        suggested_values_reasoning:
            Changing the weights initialization scheme is
            something to consider if a model is having trouble with convergence, or
            otherwise it is something to experiment with after other factors are considered.
            The default choice (`xavier_uniform`) is a suitable starting point for
            most tasks.
        ui_display_name: Layer Weights Initializer
StackedParallelCNN:
    type:
        short_description:
            Combination of Parallel CNN and Stacked CNN encoders utilizing a stack of parallel convolutional layers.
        long_description:
            The stacked parallel cnn encoder is a combination of the Parallel CNN and the Stacked CNN
            encoders where each layer of the stack is composed of parallel convolutional layers. It works by
            first mapping the input integer sequence b x s (where b is the batch size and s is the length of
            the sequence) into a sequence of embeddings, then it passes the embedding through a stack of
            several parallel 1d convolutional layers with different filter size, followed by an optional
            final pool and by a flatten operation. This single flattened vector is then passed through a
            stack of fully connected layers and returned as a b x h tensor where h is the output size of the
            last fully connected layer.
    activation:
        default_value_reasoning:
            The Rectified Linear Units (ReLU) function is the
            standard activation function used for adding non-linearity. It is simple,
            fast, and empirically works well (https://arxiv.org/abs/1803.08375).
        description_implications:
            Changing the activation functions has an impact
            on the computational load of the model and might require further hypterparameter
            tuning
        expected_impact: 1
        suggested_values:
            The default value will work well in the majority of the
            cases
        ui_display_name: Activation
    bias_initializer:
        default_value_reasoning:
            It is possible and common to initialize the biases
            to be zero, since the asymmetry breaking is provided by the small random
            numbers in the weights.
        description_implications:
            It's rare to see any performance gains from choosing
            a different bias initialization. Some practitioners like to use a small
            constant value such as 0.01 for all biases to ensure that all ReLU units
            are activated in the beginning and have some effect on the gradient. However,
            it's still an open question as to whether this provides consistent improvement.
        expected_impact: 1
        literature_references:
            - https://cs231n.github.io/neural-networks-2/
        related_parameters:
            - weights_initializer
        suggested_values: zeros
        suggested_values_reasoning:
            It is possible and common to initialize the biases
            to be zero, since the asymmetry breaking is provided by the small random
            numbers in the weights. For ReLU non-linearities, some people like to
            use small constant value such as 0.01 for all biases because this ensures
            that all ReLU units fire in the beginning and therefore obtain and propagate
            some gradient. However, it is not clear if this provides a consistent
            improvement (in fact some results seem to indicate that this performs
            worse) and it is more common to simply use 0 bias initialization.
        ui_display_name: Bias Initializer
    dropout:
        default_value_reasoning:
            Dropout can cause training to become less stable.
            Consider start with a dropout-free baseline, and add dropout gradually
            in subsequent experiments.
        description_implications:
            "Dropout is a computationally cheap regularization\
            \ method where during training, some neurons are randomly ignored or \u201C\
            dropped out\u201D. Increasing dropout has the effect of making the training\
            \ process more noisy and lowering overall network capacity, but it can\
            \ be an effective regularization method to reduce overfitting and improve\
            \ generalization."
        example_value:
            - 0.2
        expected_impact: 3
        literature_references:
            - https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html
        suggested_values: 0.05 - 0.8
        suggested_values_reasoning:
            Tuning dropout is really something to be done
            when all of the big choices about architecture have been settled. Consider
            starting with 0.5 and adjusting the dropout depending on observed model
            performance.
        ui_display_name: Dropout
    embedding_size:
        default_value_reasoning: Not too big, not too small.
        description_implications:
            'An embedding is a relatively low-dimensional space
            that is used to translate high-dimensional vectors like words, which can
            have a large vocbulary size. Ideally, after an embedding is trained, it
            captures some of the semantics of the input by placing semantically similar
            inputs close together in the embedding space.


            In most cases, the embedding size is chosen empirically, by trial and
            error. From https://www.amazon.com/dp/1098115783, "one rule of thumb is
            to use the fourth root of the total number of unique categorical elements
            while another is that the embedding dimension should be approximately
            1.6 times the square root of the number of unique elements in the category,
            and no less than 600."


            Increasing the embedding size may cause the model to train more slowly,
            but the higher dimensionality can also improve overall quality.'
        expected_impact: 2
        literature_references:
            - https://developers.google.com/machine-learning/crash-course/embeddings/video-lecture
        suggested_values: 1.6 * sqrt(vocab_size)
        suggested_values_reasoning:
            Rule of thumb suggested by a deep learning textbook.
            Try models with smaller or larger embedding sizes to observe relative
            impact.
        ui_display_name: Embedding Size
    embeddings_on_cpu:
        default_value_reasoning:
            By default embeddings matrices are stored on GPU
            memory if a GPU is used, as it allows for faster access.
        description_implications:
            By default embeddings matrices are stored on GPU
            memory if a GPU is used, as it allows for faster access. However, in some
            cases when the vocabulary size is very large, the full embedding matrix
            may be really big and unwieldy to have in GPU memory. This parameter forces
            the placement of the embedding matrix in regular memory and the CPU is
            used to access them. This may slow down training due to additional data
            transfer between CPU and GPU memory, but can lead to healthier GPU memory
            resource usage.
        expected_impact: 1
        suggested_values:
            - false
        suggested_values_reasoning:
            If GPU memory is not a constraint, having embeddings
            stored and accessed within the GPU is faster.
        ui_display_name: Embeddings on CPU
    embeddings_trainable:
        ui_display_name: null
    fc_layers:
        default_value_reasoning:
            By default the stack is built by using num_fc_layers,
            output_size, use_bias, weights_initializer, bias_initializer, norm, norm_params,
            activation, dropout. When a list of dictionaries is provided, the stack
            is built following the parameters of each dict for building each layer.
        description_implications:
            The more layers that are specified the deeper and
            higher capacity the model will be. This makes it possible to potentially
            achieve better performance when a big anough amount of data is provided,
            but also makes the model more computationally expensive and potentially
            more prone to overfitting.
        example_value:
            - dropout: 0.1
              output_size: 128
            - norm: layer
              output_size: 64
        expected_impact: 1
        related_parameters:
            - output_size
            - use_bias
            - weights_initializer
            - bias_initializer
            - norm
            - norm_params
            - activation
            - dropout
        suggested_values_reasoning:
            It is easier to define a stack of fully connected
            layers by just specifying num_fc_layers, output_size and the other individual
            parameters. It will create a stack of layers with identical properties.
            Use this parameter only if you need a fine grained level of control of
            each individual layer in the stack.
        ui_display_name: Fully Connected Layers
    filter_size:
        ui_display_name: null
    max_sequence_length:
        default_value_reasoning:
            Sets the maximum sequence length of the expected
            inputs, so input/output shapes are computed accurately.
        expected_impact: 1
        ui_display_name: null
    norm:
        default_value_reasoning:
            While batch normalization and layer normalization
            usually lead to improvements, it can be useful to start with fewer bells
            and whistles.
        description_implications:
            Normalization helps stabilize the learning process
            and can have a regularizing effect that can help with generalization.
            It's often suggested that with normalization, you can use a higher learning
            rate.
        example_value:
            - batch
        expected_impact: 3
        literature_references:
            - https://machinelearningmastery.com/batch-normalization-for-training-of-deep-neural-networks/
        related_parameters:
            - norm_params
        suggested_values: '"batch" or "layer"'
        suggested_values_reasoning:
            Normalization tries to solve "internal covariate
            shift" that comes from the changing distributions of the inputs to layers
            deep in the network when weights are updated. For example, batch normalization
            standardizes the inputs to a layer for each mini-batch. Try out different
            normalizations to see if that helps with training stability
        ui_display_name: Normalization Type
    norm_params:
        default_value_reasoning:
            The default parameters that come with Torch's implementation
            of these normalization types are a trusted starting point.
        description_implications:
            There are a variety of ways a certain set of parameters
            specificed could influence performance here. Broadly speaking the different
            values passed in here allow for different levels of smoothness to be observed
            in the learning curves. Since setting this parameters depends on the type
            of `norm` set, see [BatchNorm2d](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html)
            for more information on the parameters to set for batch normalization,
            and see [LayerNorm](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html)
            for more information on the parameters to set for layer normalization.
        example_value:
            - affine: false
              momentum: 0.2
              num_features: 100
        expected_impact: 1
        literature_references:
            - "For BatchNorm2d: https://arxiv.org/abs/1502.03167


              For LayerNorm: https://arxiv.org/abs/1607.06450"
        related_parameters:
            - "`norm`"
        suggested_values: Depends on the type of `norm` set.
        suggested_values_reasoning: "NO"
        ui_display_name: Normalization Parameters
    num_fc_layers:
        default_value_reasoning:
            The encoder already has learnable parameters.Sometimes
            the default is 1 for modules where the FC stack is used for shape management,
            or the only source of learnable parameters.
        description_implications:
            Increasing num_fc_layers will increase the capacity
            of the model. The model will be slower to train, and there's a higher
            risk of overfitting.
        example_value:
            - 1
        expected_impact: 1
        other_information:
            Not all modules that have fc_layers also have an accompanying
            num_fc_layers parameter. Where both are present, fc_layers takes precedent
            over num_fc_layers. Specifying num_fc_layers alone uses fully connected
            layers that are configured by the defaults in FCStack.
        related_parameters:
            - fc_layers
        suggested_values: 0-1
        suggested_values_reasoning:
            The full model likely contains many learnable
            parameters. Consider starting with very few, or without any additional
            fully connected layers and add them if you observe evidence of limited
            model capacity. Sometimes the default is 1 for modules where the FC stack
            is used for shape management, or the only source of learnable parameters.
        ui_display_name: Number of Fully Connected Layers
    num_filters:
        ui_display_name: null
    num_stacked_layers:
        description_implications:
            While superceded by `stacked_layers`, this can directly
            change the depth of the current stack of parallel convolutional layers.
        example_value:
            - 1
        expected_impact: 1
        related_parameters:
            - stacked_layers
        ui_display_name: Number of Stacked Layers
    output_size:
        default_value_reasoning: A modest value, not too small, not too large.
        description_implications:
            If there are fully connected layers in this module,
            increasing the output size of each fully connected layer will increase
            the capacity of the model. However, the model may be slower to train,
            and there's a higher risk of overfitting. If it seems like the model could
            use even more capacity, consider increasing the number of fully connected
            layers, or explore other architectures.
        expected_impact: 2
        other_information:
            If num_fc_layers=0 and fc_layers=None, and there are no
            fully connected layers defined on the module, then this parameter may
            have no effect on the module's final output shape.
        related_parameters:
            - num_fc_layers, fc_layers
        suggested_values: 10 - 1024
        suggested_values_reasoning:
            Increasing the output size increases the capacity
            of the model. If this seems to have a positive effect, then it could be
            worth increasing the number of layers, or trying a different architecture
            with a larger capacity.
        ui_display_name: Output Size
    pool_function:
        ui_display_name: null
    pool_size:
        ui_display_name: null
    pretrained_embeddings:
        ui_display_name: null
    reduce_output:
        ui_display_name: null
    representation:
        ui_display_name: null
    should_embed:
        internal_only: true
        ui_display_name: Not displayed
    stacked_layers:
        ui_display_name: null
    use_bias:
        default_value_reasoning:
            "Bias terms may improve model accuracy, and don't
            have much impact in terms of memory or training speed. For most models
            it is reasonable to use bias terms.


            Batch Normalization, however, adds a trainable shift parameter which is
            added to the activation. When Batch Normalization is used in a layer,
            bias terms are redundant and may be removed."
        description_implications:
            Bias terms may improve model accuracy, and don't
            have much impact in terms of memory or training speed. For most models
            it is reasonable to leave this parameter set to True.
        example_value:
            - true
        expected_impact: 1
        other_information:
            If fc_layers is not specified, or use_bias is not specified
            for individual layers, the value of use_bias will be used as the default
            for all layers.
        related_parameters:
            - bias_initializer, fc_layers
        suggested_values:
            - true
        ui_display_name: Use Bias
    vocab:
        default_value_reasoning:
            Computed and passed along internally according to
            preprocessing settings.
        example_value:
            - a
            - b
            - c
        expected_impact: 2
        internal_only: true
        ui_display_name: Not Displayed
    weights_initializer:
        default_value_reasoning: Taken from [this paper](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf).
        description_implications:
            The method you choose to initialize layer weights
            during training can have a big impact on performance as well as the reproducibility
            of your final model between runs. As an example, if you were to randomly
            initialize weights you would risk non-reproducibility (and possibly general
            training performance), but sticking with constant values for initialization
            might significantly increase the time needed for model convergence. Generally,
            choosing one of the probabilistic approaches strikes a balance between
            the two extremes, and the literature kicked off by the landmark [*Xavier
            et al.* paper](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)
            provides a few good options. See this nice discussion from [Weights and
            Biases](https://wandb.ai/site/articles/the-effects-of-weight-initialization-on-neural-nets#:~:text=Studies%20have%20shown%20that%20initializing,net%20train%20better%20and%20faster.)
            for more information.
        expected_impact: 3
        literature_references:
            - "Weights and Biases blog post: https://wandb.ai/site/articles/the-effects-of-weight-initialization-on-neural-nets#:~:text=Studies%20have%20shown%20that%20initializing,net%20train%20better%20and%20faster.


              Xavier et al. paper: http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf"
        suggested_values: xavier_uniform
        suggested_values_reasoning:
            Changing the weights initialization scheme is
            something to consider if a model is having trouble with convergence, or
            otherwise it is something to experiment with after other factors are considered.
            The default choice (`xavier_uniform`) is a suitable starting point for
            most tasks.
        ui_display_name: Layer Weights Initializer
StackedRNN:
    type:
        short_description:
            Utilizes a stack of recurrent layers followed by a reduce operation.
        long_description:
            The rnn encoder works by first mapping the input integer sequence b x s (where b is the batch
            size and s is the length of the sequence) into a sequence of embeddings, then it passes the
            embedding through a stack of recurrent layers (by default 1 layer), followed by a reduce
            operation that by default only returns the last output, but can perform other reduce functions.
    activation:
        ui_display_name: null
    bias_initializer:
        default_value_reasoning:
            It is possible and common to initialize the biases
            to be zero, since the asymmetry breaking is provided by the small random
            numbers in the weights.
        description_implications:
            It's rare to see any performance gains from choosing
            a different bias initialization. Some practitioners like to use a small
            constant value such as 0.01 for all biases to ensure that all ReLU units
            are activated in the beginning and have some effect on the gradient. However,
            it's still an open question as to whether this provides consistent improvement.
        expected_impact: 1
        literature_references:
            - https://cs231n.github.io/neural-networks-2/
        related_parameters:
            - weights_initializer
        suggested_values: zeros
        suggested_values_reasoning:
            It is possible and common to initialize the biases
            to be zero, since the asymmetry breaking is provided by the small random
            numbers in the weights. For ReLU non-linearities, some people like to
            use small constant value such as 0.01 for all biases because this ensures
            that all ReLU units fire in the beginning and therefore obtain and propagate
            some gradient. However, it is not clear if this provides a consistent
            improvement (in fact some results seem to indicate that this performs
            worse) and it is more common to simply use 0 bias initialization.
        ui_display_name: Bias Initializer
    bidirectional:
        ui_display_name: null
    cell_type:
        ui_display_name: null
    dropout:
        default_value_reasoning:
            Dropout can cause training to become less stable.
            Consider start with a dropout-free baseline, and add dropout gradually
            in subsequent experiments.
        description_implications:
            "Dropout is a computationally cheap regularization\
            \ method where during training, some neurons are randomly ignored or \u201C\
            dropped out\u201D. Increasing dropout has the effect of making the training\
            \ process more noisy and lowering overall network capacity, but it can\
            \ be an effective regularization method to reduce overfitting and improve\
            \ generalization."
        example_value:
            - 0.2
        expected_impact: 3
        literature_references:
            - https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html
        related_parameters:
            - "dropout,

              recurrent_dropout,

              fc_dropout"
        suggested_values: 0.05 - 0.8
        suggested_values_reasoning:
            Tuning dropout is really something to be done
            when all of the big choices about architecture have been settled. Consider
            starting with 0.5 and adjusting the dropout depending on observed model
            performance.
        ui_display_name: Dropout
    embedding_size:
        default_value_reasoning: Not too big, not too small.
        description_implications:
            'An embedding is a relatively low-dimensional space
            that is used to translate high-dimensional vectors like words, which can
            have a large vocbulary size. Ideally, after an embedding is trained, it
            captures some of the semantics of the input by placing semantically similar
            inputs close together in the embedding space.


            In most cases, the embedding size is chosen empirically, by trial and
            error. From https://www.amazon.com/dp/1098115783, "one rule of thumb is
            to use the fourth root of the total number of unique categorical elements
            while another is that the embedding dimension should be approximately
            1.6 times the square root of the number of unique elements in the category,
            and no less than 600."


            Increasing the embedding size may cause the model to train more slowly,
            but the higher dimensionality can also improve overall quality.'
        expected_impact: 2
        literature_references:
            - https://developers.google.com/machine-learning/crash-course/embeddings/video-lecture
        suggested_values: 1.6 * sqrt(vocab_size)
        suggested_values_reasoning:
            Rule of thumb suggested by a deep learning textbook.
            Try models with smaller or larger embedding sizes to observe relative
            impact.
        ui_display_name: Embedding Size
    embeddings_on_cpu:
        default_value_reasoning:
            By default embeddings matrices are stored on GPU
            memory if a GPU is used, as it allows for faster access.
        description_implications:
            By default embeddings matrices are stored on GPU
            memory if a GPU is used, as it allows for faster access. However, in some
            cases when the vocabulary size is very large, the full embedding matrix
            may be really big and unwieldy to have in GPU memory. This parameter forces
            the placement of the embedding matrix in regular memory and the CPU is
            used to access them. This may slow down training due to additional data
            transfer between CPU and GPU memory, but can lead to healthier GPU memory
            resource usage.
        expected_impact: 1
        suggested_values:
            - false
        suggested_values_reasoning:
            If GPU memory is not a constraint, having embeddings
            stored and accessed within the GPU is faster.
        ui_display_name: Embeddings on CPU
    embeddings_trainable:
        ui_display_name: null
    fc_activation:
        default_value_reasoning:
            The Rectified Linear Units (ReLU) function is the
            standard activation function used for adding non-linearity. It is simple,
            fast, and empirically works well (https://arxiv.org/abs/1803.08375).
        description_implications:
            Changing the activation functions has an impact
            on the computational load of the model and might require further hypterparameter
            tuning
        example_value:
            - relu
        expected_impact: 1
        literature_references:
            - https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html
        related_parameters:
            - activation, activation_function, conv_activation, recurrent_activation
        suggested_values: relu, alternatively leakyRelu or elu
        suggested_values_reasoning:
            The default value will work well in the majority
            of the cases
        ui_display_name: FC Activation
    fc_dropout:
        default_value_reasoning:
            Dropout can cause training to become less stable.
            Consider start with a dropout-free baseline, and add dropout gradually
            in subsequent experiments.
        description_implications:
            "Dropout is a computationally cheap regularization\
            \ method where during training, some neurons are randomly ignored or \u201C\
            dropped out\u201D. Increasing dropout has the effect of making the training\
            \ process more noisy and lowering overall network capacity, but it can\
            \ be an effective regularization method to reduce overfitting and improve\
            \ generalization."
        example_value:
            - 0.2
        expected_impact: 3
        literature_references:
            - https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html
        related_parameters:
            - dropout, recurrent_dropout
        suggested_values: 0.05 - 0.8
        suggested_values_reasoning:
            Tuning dropout is really something to be done
            when all of the big choices about architecture have been settled. Consider
            starting with 0.5 and adjusting the dropout depending on observed model
            performance.
        ui_display_name: FC Dropout
    fc_layers:
        default_value_reasoning:
            By default the stack is built by using num_fc_layers,
            output_size, use_bias, weights_initializer, bias_initializer, norm, norm_params,
            activation, dropout. When a list of dictionaries is provided, the stack
            is built following the parameters of each dict for building each layer.
        description_implications:
            The more layers that are specified the deeper and
            higher capacity the model will be. This makes it possible to potentially
            achieve better performance when a big anough amount of data is provided,
            but also makes the model more computationally expensive and potentially
            more prone to overfitting.
        example_value:
            - dropout: 0.1
              output_size: 128
            - norm: layer
              output_size: 64
        expected_impact: 1
        related_parameters:
            - output_size
            - use_bias
            - weights_initializer
            - bias_initializer
            - norm
            - norm_params
            - activation
            - dropout
        suggested_values_reasoning:
            It is easier to define a stack of fully connected
            layers by just specifying num_fc_layers, output_size and the other individual
            parameters. It will create a stack of layers with identical properties.
            Use this parameter only if you need a fine grained level of control of
            each individual layer in the stack.
        ui_display_name: Fully Connected Layers
    max_sequence_length:
        default_value_reasoning:
            Sets the maximum sequence length of the expected
            inputs, so input/output shapes are computed accurately.
        expected_impact: 1
        ui_display_name: null
    norm:
        default_value_reasoning:
            While batch normalization and layer normalization
            usually lead to improvements, it can be useful to start with fewer bells
            and whistles.
        description_implications:
            Normalization helps stabilize the learning process
            and can have a regularizing effect that can help with generalization.
            It's often suggested that with normalization, you can use a higher learning
            rate.
        example_value:
            - batch
        expected_impact: 3
        literature_references:
            - https://machinelearningmastery.com/batch-normalization-for-training-of-deep-neural-networks/
        related_parameters:
            - norm_params
        suggested_values: '"batch" or "layer"'
        suggested_values_reasoning:
            Normalization tries to solve "internal covariate
            shift" that comes from the changing distributions of the inputs to layers
            deep in the network when weights are updated. For example, batch normalization
            standardizes the inputs to a layer for each mini-batch. Try out different
            normalizations to see if that helps with training stability
        ui_display_name: Normalization Type
    norm_params:
        default_value_reasoning:
            The default parameters that come with Torch's implementation
            of these normalization types are a trusted starting point.
        description_implications:
            There are a variety of ways a certain set of parameters
            specificed could influence performance here. Broadly speaking the different
            values passed in here allow for different levels of smoothness to be observed
            in the learning curves. Since setting this parameters depends on the type
            of `norm` set, see [BatchNorm2d](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html)
            for more information on the parameters to set for batch normalization,
            and see [LayerNorm](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html)
            for more information on the parameters to set for layer normalization.
        example_value:
            - affine: false
              momentum: 0.2
              num_features: 100
        expected_impact: 1
        literature_references:
            - "For BatchNorm2d: https://arxiv.org/abs/1502.03167


              For LayerNorm: https://arxiv.org/abs/1607.06450"
        related_parameters:
            - "`norm`"
        suggested_values: Depends on the type of `norm` set.
        suggested_values_reasoning: "NO"
        ui_display_name: Normalization Parameters
    num_fc_layers:
        default_value_reasoning:
            The encoder already has learnable parameters.Sometimes
            the default is 1 for modules where the FC stack is used for shape management,
            or the only source of learnable parameters.
        description_implications:
            Increasing num_fc_layers will increase the capacity
            of the model. The model will be slower to train, and there's a higher
            risk of overfitting.
        example_value:
            - 1
        expected_impact: 1
        other_information:
            Not all modules that have fc_layers also have an accompanying
            num_fc_layers parameter. Where both are present, fc_layers takes precedent
            over num_fc_layers. Specifying num_fc_layers alone uses fully connected
            layers that are configured by the defaults in FCStack.
        related_parameters:
            - fc_layers
        suggested_values: 0-1
        suggested_values_reasoning:
            The full model likely contains many learnable
            parameters. Consider starting with very few, or without any additional
            fully connected layers and add them if you observe evidence of limited
            model capacity. Sometimes the default is 1 for modules where the FC stack
            is used for shape management, or the only source of learnable parameters.
        ui_display_name: Number of Fully Connected Layers
    num_layers:
        default_value_reasoning:
            The ideal number of layers depends on the data. For
            many data types, one layer is sufficient.
        description_implications:
            Increasing the number of layers may improve model
            performance for longer sequences or more complex tasks.
        example_value:
            - 1
        expected_impact: 1
        suggested_values: 1-3
        suggested_values_reasoning:
            Increasing the number of layers may improve encoder
            performance.  However, more layers will increase training time and may
            cause overfitting.  Small numbers of layers usually work best.
        ui_display_name: Number of Recurrent Layers
    output_size:
        default_value_reasoning: A modest value, not too small, not too large.
        description_implications:
            If there are fully connected layers in this module,
            increasing the output size of each fully connected layer will increase
            the capacity of the model. However, the model may be slower to train,
            and there's a higher risk of overfitting. If it seems like the model could
            use even more capacity, consider increasing the number of fully connected
            layers, or explore other architectures.
        expected_impact: 2
        other_information:
            If num_fc_layers=0 and fc_layers=None, and there are no
            fully connected layers defined on the module, then this parameter may
            have no effect on the module's final output shape.
        related_parameters:
            - num_fc_layers, fc_layers
        suggested_values: 10 - 1024
        suggested_values_reasoning:
            Increasing the output size increases the capacity
            of the model. If this seems to have a positive effect, then it could be
            worth increasing the number of layers, or trying a different architecture
            with a larger capacity.
        ui_display_name: Output Size
    pretrained_embeddings:
        ui_display_name: null
    recurrent_activation:
        default_value_reasoning: sigmoid' is commonly used
        expected_impact: 3
        other_information:
            I don't think that this parameter is used anywhere in the
            code base. It's being passed down but not used in the actual RNN forwarding
            functions.
        suggested_values: sigmoid, ReLu, tanh
        ui_display_name: null
    recurrent_dropout:
        default_value_reasoning:
            Dropout can cause training to become less stable.
            Consider start with a dropout-free baseline, and add dropout gradually
            in subsequent experiments.
        description_implications:
            "Dropout is a computationally cheap regularization\
            \ method where during training, some neurons are randomly ignored or \u201C\
            dropped out\u201D. Increasing dropout has the effect of making the training\
            \ process more noisy and lowering overall network capacity, but it can\
            \ be an effective regularization method to reduce overfitting and improve\
            \ generalization."
        example_value:
            - 0.2
        expected_impact: 3
        literature_references:
            - https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html
        related_parameters:
            - "dropout,

              recurrent_dropout,

              fc_dropout"
        suggested_values: 0.05 - 0.8
        suggested_values_reasoning:
            Tuning dropout is really something to be done
            when all of the big choices about architecture have been settled. Consider
            starting with 0.5 and adjusting the dropout depending on observed model
            performance.
        ui_display_name: Recurrent Dropout
    recurrent_initializer:
        ui_display_name: null
    reduce_output:
        ui_display_name: null
    representation:
        ui_display_name: null
    should_embed:
        internal_only: true
        ui_display_name: Not displayed
    state_size:
        ui_display_name: null
    unit_forget_bias:
        ui_display_name: null
    use_bias:
        default_value_reasoning:
            "Bias terms may improve model accuracy, and don't
            have much impact in terms of memory or training speed. For most models
            it is reasonable to use bias terms.


            Batch Normalization, however, adds a trainable shift parameter which is
            added to the activation. When Batch Normalization is used in a layer,
            bias terms are redundant and may be removed."
        description_implications:
            Bias terms may improve model accuracy, and don't
            have much impact in terms of memory or training speed. For most models
            it is reasonable to leave this parameter set to True.
        example_value:
            - true
        expected_impact: 1
        other_information:
            If fc_layers is not specified, or use_bias is not specified
            for individual layers, the value of use_bias will be used as the default
            for all layers.
        related_parameters:
            - bias_initializer, fc_layers
        suggested_values:
            - true
        ui_display_name: Use Bias
    vocab:
        default_value_reasoning:
            Computed and passed along internally according to
            preprocessing settings.
        example_value:
            - a
            - b
            - c
        expected_impact: 2
        internal_only: true
        ui_display_name: Not Displayed
    weights_initializer:
        default_value_reasoning: Taken from [this paper](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf).
        description_implications:
            The method you choose to initialize layer weights
            during training can have a big impact on performance as well as the reproducibility
            of your final model between runs. As an example, if you were to randomly
            initialize weights you would risk non-reproducibility (and possibly general
            training performance), but sticking with constant values for initialization
            might significantly increase the time needed for model convergence. Generally,
            choosing one of the probabilistic approaches strikes a balance between
            the two extremes, and the literature kicked off by the landmark [*Xavier
            et al.* paper](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)
            provides a few good options. See this nice discussion from [Weights and
            Biases](https://wandb.ai/site/articles/the-effects-of-weight-initialization-on-neural-nets#:~:text=Studies%20have%20shown%20that%20initializing,net%20train%20better%20and%20faster.)
            for more information.
        expected_impact: 3
        literature_references:
            - "Weights and Biases blog post: https://wandb.ai/site/articles/the-effects-of-weight-initialization-on-neural-nets#:~:text=Studies%20have%20shown%20that%20initializing,net%20train%20better%20and%20faster.


              Xavier et al. paper: http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf"
        suggested_values: xavier_uniform
        suggested_values_reasoning:
            Changing the weights initialization scheme is
            something to consider if a model is having trouble with convergence, or
            otherwise it is something to experiment with after other factors are considered.
            The default choice (`xavier_uniform`) is a suitable starting point for
            most tasks.
        ui_display_name: Layer Weights Initializer
StackedTransformer:
    type:
        short_description:
            Stack of transformer blocks with optional stack of fully connected layers.
        long_description:
            The transformer encoder implements a stack of transformer blocks, replicating the architecture
            introduced in the Attention is all you need paper, and adds am optional stack of fully connected
            layers at the end.
        literature_references:
            - https://arxiv.org/abs/1706.03762
    bias_initializer:
        default_value_reasoning:
            It is possible and common to initialize the biases
            to be zero, since the asymmetry breaking is provided by the small random
            numbers in the weights.
        description_implications:
            It's rare to see any performance gains from choosing
            a different bias initialization. Some practitioners like to use a small
            constant value such as 0.01 for all biases to ensure that all ReLU units
            are activated in the beginning and have some effect on the gradient. However,
            it's still an open question as to whether this provides consistent improvement.
        expected_impact: 1
        literature_references:
            - https://cs231n.github.io/neural-networks-2/
        related_parameters:
            - weights_initializer
        suggested_values: zeros
        suggested_values_reasoning:
            It is possible and common to initialize the biases
            to be zero, since the asymmetry breaking is provided by the small random
            numbers in the weights. For ReLU non-linearities, some people like to
            use small constant value such as 0.01 for all biases because this ensures
            that all ReLU units fire in the beginning and therefore obtain and propagate
            some gradient. However, it is not clear if this provides a consistent
            improvement (in fact some results seem to indicate that this performs
            worse) and it is more common to simply use 0 bias initialization.
        ui_display_name: Bias Initializer
    dropout:
        default_value_reasoning: Taken from published literature (https://arxiv.org/abs/1908.07442).
        description_implications:
            "Dropout is a computationally cheap regularization\
            \ method where during training, some neurons are randomly ignored or \u201C\
            dropped out\u201D. Increasing dropout has the effect of making the training\
            \ process more noisy and lowering overall network capacity, but it can\
            \ be an effective regularization method to reduce overfitting and improve\
            \ generalization."
        example_value:
            - 0.2
        expected_impact: 3
        literature_references:
            - https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html
        related_parameters:
            - fc_dropout
        suggested_values: 0.05 - 0.8
        suggested_values_reasoning:
            Tuning dropout is really something to be done
            when all of the big choices about architecture have been settled. Consider
            starting with 0.5 and adjusting the dropout depending on observed model
            performance.
        ui_display_name: Dropout
    embedding_size:
        default_value_reasoning: Not too big, not too small.
        description_implications:
            'An embedding is a relatively low-dimensional space
            that is used to translate high-dimensional vectors like words, which can
            have a large vocbulary size. Ideally, after an embedding is trained, it
            captures some of the semantics of the input by placing semantically similar
            inputs close together in the embedding space.


            In most cases, the embedding size is chosen empirically, by trial and
            error. From https://www.amazon.com/dp/1098115783, "one rule of thumb is
            to use the fourth root of the total number of unique categorical elements
            while another is that the embedding dimension should be approximately
            1.6 times the square root of the number of unique elements in the category,
            and no less than 600."


            Increasing the embedding size may cause the model to train more slowly,
            but the higher dimensionality can also improve overall quality.'
        expected_impact: 2
        literature_references:
            - https://developers.google.com/machine-learning/crash-course/embeddings/video-lecture
        suggested_values: 1.6 * sqrt(vocab_size)
        suggested_values_reasoning:
            Rule of thumb suggested by a deep learning textbook.
            Try models with smaller or larger embedding sizes to observe relative
            impact.
        ui_display_name: Embedding Size
    embeddings_on_cpu:
        default_value_reasoning:
            By default embeddings matrices are stored on GPU
            memory if a GPU is used, as it allows for faster access.
        description_implications:
            By default embeddings matrices are stored on GPU
            memory if a GPU is used, as it allows for faster access. However, in some
            cases when the vocabulary size is very large, the full embedding matrix
            may be really big and unwieldy to have in GPU memory. This parameter forces
            the placement of the embedding matrix in regular memory and the CPU is
            used to access them. This may slow down training due to additional data
            transfer between CPU and GPU memory, but can lead to healthier GPU memory
            resource usage.
        expected_impact: 1
        suggested_values:
            - false
        suggested_values_reasoning:
            If GPU memory is not a constraint, having embeddings
            stored and accessed within the GPU is faster.
        ui_display_name: Embeddings on CPU
    embeddings_trainable:
        ui_display_name: null
    fc_activation:
        default_value_reasoning:
            The Rectified Linear Units (ReLU) function is the
            standard activation function used for adding non-linearity. It is simple,
            fast, and empirically works well (https://arxiv.org/abs/1803.08375).
        description_implications:
            Changing the activation functions has an impact
            on the computational load of the model and might require further hypterparameter
            tuning
        example_value:
            - relu
        expected_impact: 1
        literature_references:
            - https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html
        related_parameters:
            - activation, activation_function, conv_activation, recurrent_activation
        suggested_values: relu, alternatively leakyRelu or elu
        suggested_values_reasoning:
            The default value will work well in the majority
            of the cases
        ui_display_name: FC Activation
    fc_dropout:
        default_value_reasoning:
            Dropout can cause training to become less stable.
            Consider start with a dropout-free baseline, and add dropout gradually
            in subsequent experiments.
        description_implications:
            "Dropout is a computationally cheap regularization\
            \ method where during training, some neurons are randomly ignored or \u201C\
            dropped out\u201D. Increasing dropout has the effect of making the training\
            \ process more noisy and lowering overall network capacity, but it can\
            \ be an effective regularization method to reduce overfitting and improve\
            \ generalization."
        example_value:
            - 0.2
        expected_impact: 3
        literature_references:
            - https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html
        related_parameters:
            - dropout
        suggested_values: 0.05 - 0.8
        suggested_values_reasoning:
            Tuning dropout is really something to be done
            when all of the big choices about architecture have been settled. Consider
            starting with 0.5 and adjusting the dropout depending on observed model
            performance.
        ui_display_name: FC Dropout
    fc_layers:
        default_value_reasoning:
            By default the stack is built by using num_fc_layers,
            output_size, use_bias, weights_initializer, bias_initializer, norm, norm_params,
            activation, dropout. When a list of dictionaries is provided, the stack
            is built following the parameters of each dict for building each layer.
        description_implications:
            The more layers that are specified the deeper and
            higher capacity the model will be. This makes it possible to potentially
            achieve better performance when a big anough amount of data is provided,
            but also makes the model more computationally expensive and potentially
            more prone to overfitting.
        example_value:
            - dropout: 0.1
              output_size: 128
            - norm: layer
              output_size: 64
        expected_impact: 1
        related_parameters:
            - output_size
            - use_bias
            - weights_initializer
            - bias_initializer
            - norm
            - norm_params
            - activation
            - dropout
        suggested_values_reasoning:
            It is easier to define a stack of fully connected
            layers by just specifying num_fc_layers, output_size and the other individual
            parameters. It will create a stack of layers with identical properties.
            Use this parameter only if you need a fine grained level of control of
            each individual layer in the stack.
        ui_display_name: Fully Connected Layers
    hidden_size:
        default_value_reasoning: Taken from literature (https://arxiv.org/abs/1706.03762)
        description_implications:
            Increasing the hidden size makes the model larger
            and slower to train, increases the model's capacity to capture more complexity.
            It also increases the chance of overfitting.
        expected_impact: 2
        suggested_values: 10 - 2048
        suggested_values_reasoning:
            Increasing the hidden size makes sense if the
            model is underfitting. It's useful to train both smaller and larger models
            to see how model capacity affects performance. This should only be explored
            after the architecture of the model has been settled.
        ui_display_name: Hidden Size
    max_sequence_length:
        default_value_reasoning:
            Sets the maximum sequence length of the expected
            inputs, so input/output shapes and the positional embedding matrix are
            computed accurately.
        expected_impact: 1
        ui_display_name: null
    norm:
        default_value_reasoning:
            While batch normalization and layer normalization
            usually lead to improvements, it can be useful to start with fewer bells
            and whistles.
        description_implications:
            Normalization helps stabilize the learning process
            and can have a regularizing effect that can help with generalization.
            It's often suggested that with normalization, you can use a higher learning
            rate.
        example_value:
            - batch
        expected_impact: 3
        literature_references:
            - https://machinelearningmastery.com/batch-normalization-for-training-of-deep-neural-networks/
        related_parameters:
            - norm_params
        suggested_values: '"batch" or "layer"'
        suggested_values_reasoning:
            Normalization tries to solve "internal covariate
            shift" that comes from the changing distributions of the inputs to layers
            deep in the network when weights are updated. For example, batch normalization
            standardizes the inputs to a layer for each mini-batch. Try out different
            normalizations to see if that helps with training stability
        ui_display_name: Normalization Type
    norm_params:
        default_value_reasoning:
            The default parameters that come with Torch's implementation
            of these normalization types are a trusted starting point.
        description_implications:
            There are a variety of ways a certain set of parameters
            specificed could influence performance here. Broadly speaking the different
            values passed in here allow for different levels of smoothness to be observed
            in the learning curves. Since setting this parameters depends on the type
            of `norm` set, see [BatchNorm2d](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html)
            for more information on the parameters to set for batch normalization,
            and see [LayerNorm](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html)
            for more information on the parameters to set for layer normalization.
        example_value:
            - affine: false
              momentum: 0.2
              num_features: 100
        expected_impact: 1
        literature_references:
            - "For BatchNorm2d: https://arxiv.org/abs/1502.03167
              For LayerNorm: https://arxiv.org/abs/1607.06450"
        related_parameters:
            - "`norm`"
        suggested_values: Depends on the type of `norm` set.
        suggested_values_reasoning: "NO"
        ui_display_name: Normalization Parameters
    num_fc_layers:
        default_value_reasoning:
            The encoder already has learnable parameters.Sometimes
            the default is 1 for modules where the FC stack is used for shape management,
            or the only source of learnable parameters.
        description_implications:
            Increasing num_fc_layers will increase the capacity
            of the model. The model will be slower to train, and there's a higher
            risk of overfitting.
        example_value:
            - 1
        expected_impact: 1
        other_information:
            Not all modules that have fc_layers also have an accompanying
            num_fc_layers parameter. Where both are present, fc_layers takes precedent
            over num_fc_layers. Specifying num_fc_layers alone uses fully connected
            layers that are configured by the defaults in FCStack.
        related_parameters:
            - fc_layers
        suggested_values: 0-1
        suggested_values_reasoning:
            The full model likely contains many learnable
            parameters. Consider starting with very few, or without any additional
            fully connected layers and add them if you observe evidence of limited
            model capacity. Sometimes the default is 1 for modules where the FC stack
            is used for shape management, or the only source of learnable parameters.
        ui_display_name: Number of Fully Connected Layers
    num_heads:
        ui_display_name: null
    num_layers:
        default_value_reasoning:
            The ideal number of layers depends on the data. For
            many data types, one layer is sufficient.
        description_implications:
            "The ideal number of transformer layers depends
            on the length and complexity of input sequences, as well as the task.


            For more complex tasks, and higher number of transformer layers may be
            useful. However, too many layers will increase memory and slow training
            while providing diminishing returns of model performance."
        example_value:
            - 1
        expected_impact: 1
        suggested_values: 1 - 12
        suggested_values_reasoning:
            Increasing the number of layers may improve encoder
            performance.  However, more layers will increase training time and may
            cause overfitting.  Small numbers of layers usually work best.
        ui_display_name: Number of Transformer Layers
    output_size:
        default_value_reasoning: A modest value, not too small, not too large.
        description_implications:
            If there are fully connected layers in this module,
            increasing the output size of each fully connected layer will increase
            the capacity of the model. However, the model may be slower to train,
            and there's a higher risk of overfitting. If it seems like the model could
            use even more capacity, consider increasing the number of fully connected
            layers, or explore other architectures.
        expected_impact: 2
        other_information:
            If num_fc_layers=0 and fc_layers=None, and there are no
            fully connected layers defined on the module, then this parameter may
            have no effect on the module's final output shape.
        related_parameters:
            - num_fc_layers, fc_layers
        suggested_values: 10 - 1024
        suggested_values_reasoning:
            Increasing the output size increases the capacity
            of the model. If this seems to have a positive effect, then it could be
            worth increasing the number of layers, or trying a different architecture
            with a larger capacity.
        ui_display_name: Output Size
    pretrained_embeddings:
        ui_display_name: null
    reduce_output:
        ui_display_name: null
    representation:
        ui_display_name: null
    should_embed:
        internal_only: true
        ui_display_name: Not displayed
    transformer_output_size:
        default_value_reasoning: A modest value, not too small, not too large.
        description_implications:
            If there are fully connected layers in this module,
            increasing the output size of each fully connected layer will increase
            the capacity of the model. However, the model may be slower to train,
            and there's a higher risk of overfitting. If it seems like the model could
            use even more capacity, consider increasing the number of fully connected
            layers, or explore other architectures.
        expected_impact: 2
        other_information:
            If num_fc_layers=0 and fc_layers=None, and there are no
            fully connected layers defined on the module, then this parameter may
            have no effect on the module's final output shape.
        related_parameters:
            - num_fc_layers, fc_layers
        suggested_values: 10 - 1024
        suggested_values_reasoning:
            Increasing the output size increases the capacity
            of the model. If this seems to have a positive effect, then it could be
            worth increasing the number of layers, or trying a different architecture
            with a larger capacity.
        ui_display_name: Transformer Output Size
    use_bias:
        default_value_reasoning:
            "Bias terms may improve model accuracy, and don't
            have much impact in terms of memory or training speed. For most models
            it is reasonable to use bias terms.


            Batch Normalization, however, adds a trainable shift parameter which is
            added to the activation. When Batch Normalization is used in a layer,
            bias terms are redundant and may be removed."
        description_implications:
            Bias terms may improve model accuracy, and don't
            have much impact in terms of memory or training speed. For most models
            it is reasonable to leave this parameter set to True.
        example_value:
            - true
        expected_impact: 1
        other_information:
            If fc_layers is not specified, or use_bias is not specified
            for individual layers, the value of use_bias will be used as the default
            for all layers.
        related_parameters:
            - bias_initializer, fc_layers
        suggested_values:
            - true
        ui_display_name: Use Bias
    vocab:
        default_value_reasoning:
            Computed and passed along internally according to
            preprocessing settings.
        example_value:
            - a
            - b
            - c
        expected_impact: 2
        internal_only: true
        ui_display_name: Not Displayed
    weights_initializer:
        default_value_reasoning: Taken from [this paper](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf).
        description_implications:
            The method you choose to initialize layer weights
            during training can have a big impact on performance as well as the reproducibility
            of your final model between runs. As an example, if you were to randomly
            initialize weights you would risk non-reproducibility (and possibly general
            training performance), but sticking with constant values for initialization
            might significantly increase the time needed for model convergence. Generally,
            choosing one of the probabilistic approaches strikes a balance between
            the two extremes, and the literature kicked off by the landmark [*Xavier
            et al.* paper](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)
            provides a few good options. See this nice discussion from [Weights and
            Biases](https://wandb.ai/site/articles/the-effects-of-weight-initialization-on-neural-nets#:~:text=Studies%20have%20shown%20that%20initializing,net%20train%20better%20and%20faster.)
            for more information.
        expected_impact: 3
        literature_references:
            - "Weights and Biases blog post: https://wandb.ai/site/articles/the-effects-of-weight-initialization-on-neural-nets#:~:text=Studies%20have%20shown%20that%20initializing,net%20train%20better%20and%20faster.


              Xavier et al. paper: http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf"
        suggested_values: xavier_uniform
        suggested_values_reasoning:
            Changing the weights initialization scheme is
            something to consider if a model is having trouble with convergence, or
            otherwise it is something to experiment with after other factors are considered.
            The default choice (`xavier_uniform`) is a suitable starting point for
            most tasks.
        ui_display_name: Layer Weights Initializer
T5:
    type:
        short_description:
            Text-to-text approach transformer with good transfer performance on multiple tasks.
        long_description:
            The T5 encoder loads a pretrained T5 (default t5-small) model using the Hugging Face transformers
            package. T5 (Text-to-Text Transfer Transformer) is pre-trained on a huge text dataset crawled
            from the web and shows good transfer performance on multiple tasks.
    d_ff:
        default_value_reasoning: Default value matches the pre-trained encoder.
        description_implications:
            If using a pre-trained encoder, this parameter will
            be automatically derived from the pre-trained model.
        expected_impact: 1
        ui_display_name: Dimensionality of Feed-Forward Layer
    d_kv:
        ui_display_name: null
    d_model:
        ui_display_name: null
    dropout_rate:
        default_value_reasoning: Huggingface default.
        description_implications:
            "Dropout is a computationally cheap regularization\
            \ method where during training, some neurons are randomly ignored or \u201C\
            dropped out\u201D. Increasing dropout has the effect of making the training\
            \ process more noisy and lowering overall network capacity, but it can\
            \ be an effective regularization method to reduce overfitting and improve\
            \ generalization."
        example_value:
            - 0.2
        expected_impact: 3
        literature_references:
            - https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html
        suggested_values: 0.05 - 0.8
        suggested_values_reasoning:
            Tuning dropout is really something to be done
            when all of the big choices about architecture have been settled. Consider
            starting with 0.5 and adjusting the dropout depending on observed model
            performance.
        ui_display_name: dropout_rate
    feed_forward_proj:
        ui_display_name: null
    initializer_factor:
        ui_display_name: null
    layer_norm_eps:
        ui_display_name: null
    max_sequence_length:
        default_value_reasoning:
            Sets the maximum sequence length of the expected
            inputs, so input/output shapes are computed accurately.
        expected_impact: 1
        ui_display_name: null
    num_decoder_layers:
        ui_display_name: null
    num_heads:
        ui_display_name: null
    num_layers:
        default_value_reasoning:
            The default value matches the number of layers in
            the default pretrained encoder.
        description_implications:
            "The ideal number of transformer layers depends
            on the length and complexity of input sequences, as well as the task.


            If using a pre-trained model, this parameter will be automatically derived
            from the pre-trained model."
        example_value:
            - 6
        expected_impact: 1
        related_parameters:
            - pretrained_model_or_path
        suggested_values: 1 - 12
        suggested_values_reasoning:
            Increasing the number of layers may improve encoder
            performance.  However, more layers will increase training time and may
            cause overfitting.  Small numbers of layers usually work best.
        ui_display_name: Number of Transformer Layers
    pretrained_kwargs:
        ui_display_name: null
    pretrained_model_name_or_path:
        ui_display_name: null
    reduce_output:
        ui_display_name: null
    relative_attention_num_buckets:
        ui_display_name: null
    saved_weights_in_checkpoint:
        default_value_reasoning:
            The weights of the encoder are not necessarily saved
            in the checkpoint. The user has to save them first.
        description_implications:
            The memory footprint for some of these encoders
            can be large.
        expected_impact: 1
        related_parameters:
            - skip_save_model
        suggested_values:
            - false
        suggested_values_reasoning:
            Some of these encoders are large, so it might
            be better to load them as needed, especially if 1. they're not used frequently
            2. the user doesn't have a lot of storage.
        ui_display_name: null
    trainable:
        expected_impact: 2
        ui_display_name: null
    use_pretrained:
        ui_display_name: null
    vocab:
        default_value_reasoning:
            Computed and passed along internally according to
            preprocessing settings.
        example_value:
            - a
            - b
            - c
        expected_impact: 2
        internal_only: true
        ui_display_name: Not Displayed
    vocab_size:
        internal_only: true
        ui_display_name: Not displayed
TransformerXL:
    type:
        short_description:
            Transformer architecture that introduces the notion of recurrence to the deep self-attention network.
        long_description:
            The transformer_xl encoder loads a pretrained Transformer-XL (default transfo-xl-wt103) model
            using the Hugging Face transformers package. Transformer-XL is a causal (uni-directional)
            transformer with relative positioning (sinusoïdal) embeddings which can reuse previously
            computed hidden-states to attend to longer context (memory). This model also uses adaptive
            softmax inputs and outputs (tied).
    adaptive:
        default_value_reasoning: Huggingface default.
        description_implications:
            Adaptive softmax is a speedup technique for computing
            probability distributions over words. For text with large vocabulary,
            adaptive softmax improves both training speed.
        expected_impact: 1
        related_parameters:
            - vocab_size
        ui_display_name: Adaptive Softmax
    attn_type:
        ui_display_name: null
    clamp_len:
        ui_display_name: null
    cutoffs:
        ui_display_name: null
    d_embed:
        ui_display_name: null
    d_head:
        ui_display_name: null
    d_inner:
        ui_display_name: null
    d_model:
        ui_display_name: null
    div_val:
        ui_display_name: null
    dropatt:
        ui_display_name: null
    dropout:
        default_value_reasoning: Huggingface default.
        description_implications:
            "Dropout is a computationally cheap regularization\
            \ method where during training, some neurons are randomly ignored or \u201C\
            dropped out\u201D. Increasing dropout has the effect of making the training\
            \ process more noisy and lowering overall network capacity, but it can\
            \ be an effective regularization method to reduce overfitting and improve\
            \ generalization."
        example_value:
            - 0.2
        expected_impact: 3
        literature_references:
            - https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html
        suggested_values: 0.05 - 0.8
        suggested_values_reasoning:
            Tuning dropout is really something to be done
            when all of the big choices about architecture have been settled. Consider
            starting with 0.5 and adjusting the dropout depending on observed model
            performance.
        ui_display_name: dropout
    eos_token_id:
        default_value_reasoning: Default value used in pre-trained HF encoder.
        ui_display_name: End-of-Sequence Token Id
    init:
        ui_display_name: null
    init_range:
        ui_display_name: null
    init_std:
        ui_display_name: null
    layer_norm_epsilon:
        ui_display_name: null
    max_sequence_length:
        default_value_reasoning:
            Sets the maximum sequence length of the expected
            inputs, so input/output shapes are computed accurately.
        expected_impact: 1
        ui_display_name: null
    mem_len:
        ui_display_name: null
    n_head:
        ui_display_name: null
    n_layer:
        ui_display_name: null
    pre_lnorm:
        ui_display_name: null
    pretrained_kwargs:
        ui_display_name: null
    pretrained_model_name_or_path:
        ui_display_name: null
    proj_init_std:
        ui_display_name: null
    proj_share_all_but_first:
        ui_display_name: null
    reduce_output:
        ui_display_name: null
    same_length:
        ui_display_name: null
    sample_softmax:
        ui_display_name: null
    saved_weights_in_checkpoint:
        default_value_reasoning:
            The weights of the encoder are not necessarily saved
            in the checkpoint. The user has to save them first.
        description_implications:
            The memory footprint for some of these encoders
            can be large.
        expected_impact: 1
        related_parameters:
            - skip_save_model
        suggested_values:
            - false
        suggested_values_reasoning:
            Some of these encoders are large, so it might
            be better to load them as needed, especially if 1. they're not used frequently
            2. the user doesn't have a lot of storage.
        ui_display_name: null
    trainable:
        expected_impact: 2
        ui_display_name: null
    untie_r:
        ui_display_name: null
    use_pretrained:
        ui_display_name: null
    vocab:
        default_value_reasoning:
            Computed and passed along internally according to
            preprocessing settings.
        example_value:
            - a
            - b
            - c
        expected_impact: 2
        internal_only: true
        ui_display_name: Not Displayed
    vocab_size:
        internal_only: true
        ui_display_name: Not displayed
TVAlexNetEncoder:
    model_variant:
        ui_display_name: Model Variant
    type:
        ui_display_name: Type
TVBaseEncoder:
    model_cache_dir:
        ui_display_name: Model Cache Directory
    saved_weights_in_checkpoint:
        default_value_reasoning:
            The weights of the encoder are not necessarily saved
            in the checkpoint. The user has to save them first.
        description_implications:
            The memory footprint for some of these encoders
            can be large.
        expected_impact: 1
        related_parameters:
            - skip_save_model
        suggested_values:
            - false
        suggested_values_reasoning:
            Some of these encoders are large, so it might
            be better to load them as needed, especially if 1. they're not used frequently
            2. the user doesn't have a lot of storage.
        ui_display_name: Saved Weights in Checkpoint
    trainable:
        default_value_reasoning: By default, model components are trainable.
        description_implications:
            The tradeoff when using `trainable` is between speed
            and flexibility. If False, less weights are subject to change and the
            model will therefore train faster. However, the representations output
            by this component are fixed for each input.
        expected_impact: 2
        literature_references:
            - "https://www.ibm.com/cloud/learn/overfitting


              http://d2l.ai/chapter_computer-vision/fine-tuning.html"
        related_parameters:
            - use_pretrained, pretrained_model, saved_weights_in_checkpoint
        suggested_values:
            - false
        suggested_values_reasoning:
            Freezing the weights (i.e. `trainable = False`)
            is only worth trying if you are loading in pretrained weights. In that
            case, check to see if your model is overfitting. If so, freezing the weights
            (and therefore reducing model complexity) may be beneficial.
        ui_display_name: Trainable
    use_pretrained:
        default_value_reasoning:
            By default, the model is initialized as a pretrained
            model.
        description_implications:
            Pretrained models have typically already learned
            features that are difficult to learn from scratch. They are particularly
            beneficial when training on small amounts of data.
        expected_impact: 3
        literature_references:
            - https://machinelearningmastery.com/transfer-learning-for-deep-learning/
        related_parameters:
            - trainable, pretrained_model_name, pretrained_model_name_or_path, pretrained_kwargs
        suggested_values:
            - false
        suggested_values_reasoning:
            If you have a large amount of data and/or you
            have data that differs from the typical distribution, then it might be
            worth training the model from scratch.
        ui_display_name: Use pretrained model
TVConvNeXtEncoder:
    model_variant:
        ui_display_name: Model Variant
    type:
        ui_display_name: Type
TVDenseNetEncoder:
    model_variant:
        ui_display_name: Model Variant
    type:
        ui_display_name: Type
TVEfficientNetEncoder:
    model_variant:
        ui_display_name: Model Variant
    type:
        ui_display_name: Type
TVGoogLeNetEncoder:
    model_variant:
        ui_display_name: Model Variant
    type:
        ui_display_name: Type
TVInceptionV3Encoder:
    model_variant:
        ui_display_name: Model Variant
    type:
        ui_display_name: Type
TVMaxVitEncoder:
    model_variant:
        ui_display_name: Model Variant
    type:
        ui_display_name: Type
TVMNASNetEncoder:
    model_variant:
        ui_display_name: Model Variant
    type:
        ui_display_name: Type
TVMobileNetV2Encoder:
    model_variant:
        ui_display_name: Model Variant
    type:
        ui_display_name: Type
TVMobileNetV3Encoder:
    model_variant:
        ui_display_name: Model Variant
    type:
        ui_display_name: Type
TVRegNetEncoder:
    model_variant:
        ui_display_name: Model Variant
    type:
        ui_display_name: Type
TVResNetEncoder:
    model_variant:
        ui_display_name: Model Variant
    type:
        ui_display_name: Type
TVResNeXtEncoder:
    model_variant:
        ui_display_name: Model Variant
    type:
        ui_display_name: Type
TVShuffleNetV2Encoder:
    model_variant:
        ui_display_name: Model Variant
    type:
        ui_display_name: Type
TVSqueezeNetEncoder:
    model_variant:
        ui_display_name: Model Variant
    type:
        ui_display_name: Type
TVSwinTransformerEncoder:
    model_variant:
        ui_display_name: Model Variant
    type:
        ui_display_name: Type
TVViTEncoder:
    model_variant:
        ui_display_name: Model Variant
    type:
        ui_display_name: Type
TVVGGEncoder:
    model_variant:
        ui_display_name: Model Variant
    type:
        ui_display_name: Type
TVWideResNetEncoder:
    model_variant:
        ui_display_name: Model Variant
    type:
        ui_display_name: Type
ViT:
    type:
        short_description:
            ViT encoder divides images into patches, performs a linear transformation, and then applies a transformer.
        long_description:
            ViT, short for Vision Transformer, divides the image into equal-sized patches, uses a linear
            transformation to encode each flattened patch, then applies a deep transformer architecture to
            the sequence of encoded patches.
    attention_probs_dropout_prob:
        default_value_reasoning: Taken from literature (https://arxiv.org/abs/2010.11929).
        description_implications:
            "Dropout is a computationally cheap regularization\
            \ method where during training, some neurons are randomly ignored or \u201C\
            dropped out\u201D. Increasing dropout has the effect of making the training\
            \ process more noisy and lowering overall network capacity, but it can\
            \ be an effective regularization method to reduce overfitting and improve\
            \ generalization."
        example_value:
            - 0.2
        expected_impact: 3
        literature_references:
            - https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html
        related_parameters:
            - "hidden_dropout_prob,

              attention_probs_dropout_prob"
        suggested_values: 0.05 - 0.8
        suggested_values_reasoning:
            Tuning dropout is really something to be done
            when all of the big choices about architecture have been settled. Consider
            starting with 0.5 and adjusting the dropout depending on observed model
            performance.
        ui_display_name: Attention Dropout
    gradient_checkpointing:
        ui_display_name: null
    height:
        internal_only: true
        ui_display_name: null
    hidden_act:
        default_value_reasoning: Taken from huggingface.
        description_implications:
            Changing this activation function will only affect
            the feed-forward layers of the transformer.
        example_value:
            - relu
        expected_impact: 2
        literature_references:
            - "[Huggingface docs for ViT config](https://huggingface.co/docs/transformers/model_doc/vit#transformers.ViTConfig.hidden_act)


              [Relevant StackOverflow discussion](https://ai.stackexchange.com/questions/30341/why-does-a-transformer-not-use-an-activation-function-following-the-multi-head-a)"
        suggested_values: gelu
        suggested_values_reasoning: Taken from huggingface defaults.
        ui_display_name: Hidden Layer Activation
    hidden_dropout_prob:
        default_value_reasoning: Taken from literature (https://arxiv.org/abs/2010.11929).
        description_implications:
            "Dropout is a computationally cheap regularization\
            \ method where during training, some neurons are randomly ignored or \u201C\
            dropped out\u201D. Increasing dropout has the effect of making the training\
            \ process more noisy and lowering overall network capacity, but it can\
            \ be an effective regularization method to reduce overfitting and improve\
            \ generalization."
        example_value:
            - 0.2
        expected_impact: 3
        literature_references:
            - https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html
        related_parameters:
            - "hidden_dropout_prob,

              attention_probs_dropout_prob"
        suggested_values: 0.05 - 0.8
        suggested_values_reasoning:
            Tuning dropout is really something to be done
            when all of the big choices about architecture have been settled. Consider
            starting with 0.5 and adjusting the dropout depending on observed model
            performance.
        ui_display_name: Hidden Dropout
    hidden_size:
        default_value_reasoning: Huggingface default.
        description_implications:
            Increasing the hidden size makes the model larger
            and slower to train, increases the model's capacity to capture more complexity.
            It also increases the chance of overfitting.
        expected_impact: 2
        suggested_values: 10 - 2048
        suggested_values_reasoning:
            Increasing the hidden size makes sense if the
            model is underfitting. It's useful to train both smaller and larger models
            to see how model capacity affects performance. This should only be explored
            after the architecture of the model has been settled.
        ui_display_name: Hidden Size
    initializer_range:
        description_implications:
            There is an ideal value for this variable that doesn't
            lead to the outputs of these matrices to vanish or explode
        example_value:
            - 0.02
        expected_impact: 3
        other_information: Must be greater than 0
        related_parameters:
            - weights_initializer
        suggested_values: 0.01-0.05
        suggested_values_reasoning:
            Large values will likely lead to very large outputs.
            Small values will lead to vanishing outputs.
        ui_display_name: null
    intermediate_size:
        ui_display_name: null
    layer_norm_eps:
        ui_display_name: null
    num_attention_heads:
        ui_display_name: null
    num_channels:
        ui_display_name: null
    num_hidden_layers:
        ui_display_name: null
    patch_size:
        default_value_reasoning: Taken from ViT paper.
        description_implications:
            "The implications of the image patch size for this\
            \ layer depend on other factors, such as the true resolution of the incoming\
            \ image dataset. If the patch size is kept consistent but a higher resolution\
            \ image is used as input, then the resulting chunked sequence of tokens\
            \ will be longer than it would have been if the input resolution was lower.\
            \ \n\nThe ViT paper notes that decreasing the patch size in this way led\
            \ to robust improvements without introducing other parameters."
        expected_impact: 2
        literature_references:
            - "[Huggingface docs](https://huggingface.co/docs/transformers/model_doc/vit)


              [ViT paper](https://arxiv.org/abs/2010.11929)"
        suggested_values:
            - 16
            - 32
        suggested_values_reasoning:
            16 and 32 are the values used in the original
            ViT paper.
        ui_display_name: Patch Size
    pretrained_model:
        default_value_reasoning:
            The default model is the canonical model for this
            model architecture, and is therefore a good starting point for most use
            cases.
        description_implications:
            "There are two factors to consider when choosing\
            \ a pre-trained model: (1) size, and (2) task similarity. \n\nThe larger\
            \ the model, the more subtle its comprehension of inputs can become. However,\
            \ larger models are also more compute and memory-intensive to train.\n\
            \nModels pretrained on highly-related source tasks are more likely to\
            \ be successful on the target task. Consider searching the HuggingFace\
            \ model repository for models trained on similar tasks."
        expected_impact: 3
        literature_references:
            - https://arxiv.org/abs/2010.11929
        related_parameters:
            - use_pretrained, trainable, pretrained_kwargs
        suggested_values: google/vit-large-patch16-224
        suggested_values_reasoning:
            "If you would like better performance and are
            not compute/memory-constrained, increasing model capacity can potentially
            provide a richer representation than the default. The suggested value
            upsizes the model while maintaining the same model architecture.


            Model trained on internet-scale datasets typically generalize well. Consider
            deviating from the default only if the images in the dataset originate
            from another domain (e.g. medical images, geospatial data)."
        ui_display_name: Pretrained model name
    saved_weights_in_checkpoint:
        default_value_reasoning:
            The weights of the encoder are not necessarily saved
            in the checkpoint. The user has to save them first.
        description_implications:
            The memory footprint for some of these encoders
            can be large.
        expected_impact: 1
        related_parameters:
            - skip_save_model
        suggested_values:
            - false
        suggested_values_reasoning:
            Some of these encoders are large, so it might
            be better to load them as needed, especially if 1. they're not used frequently
            2. the user doesn't have a lot of storage.
        ui_display_name: null
    trainable:
        default_value_reasoning: By default, model components are trainable.
        description_implications:
            The tradeoff when using `trainable` is between speed
            and flexibility. If False, less weights are subject to change and the
            model will therefore train faster. However, the representations output
            by this component are fixed for each input.
        expected_impact: 2
        literature_references:
            - "https://www.ibm.com/cloud/learn/overfitting


              http://d2l.ai/chapter_computer-vision/fine-tuning.html"
        related_parameters:
            - use_pretrained, pretrained_model, saved_weights_in_checkpoint
        suggested_values:
            - false
        suggested_values_reasoning:
            Freezing the weights (i.e. `trainable = False`)
            is only worth trying if you are loading in pretrained weights. In that
            case, check to see if your model is overfitting. If so, freezing the weights
            (and therefore reducing model complexity) may be beneficial.
        ui_display_name: Trainable
    use_pretrained:
        default_value_reasoning:
            By default, the model is initialized as a pretrained
            model.
        description_implications:
            Pretrained models have typically already learned
            features that are difficult to learn from scratch. They are particularly
            beneficial when training on small amounts of data.
        expected_impact: 3
        literature_references:
            - https://machinelearningmastery.com/transfer-learning-for-deep-learning/
        related_parameters:
            - trainable, pretrained_model_name, pretrained_model_name_or_path, pretrained_kwargs
        suggested_values:
            - false
        suggested_values_reasoning:
            If you have a large amount of data and/or you
            have data that differs from the typical distribution, then it might be
            worth training the model from scratch.
        ui_display_name: Use pretrained model
    width:
        internal_only: true
        ui_display_name: null
XLM:
    type:
        short_description:
            XLM is pre-trained by cross-language modeling.
        long_description:
            The xlm encoder loads a pretrained XLM (default xlm-mlm-en-2048) model using the Hugging Face
            transformers package. XLM is pre-trained by cross-language modeling.
    asm:
        ui_display_name: null
    attention_dropout:
        default_value_reasoning: Huggingface default.
        description_implications:
            "Dropout is a computationally cheap regularization\
            \ method where during training, some neurons are randomly ignored or \u201C\
            dropped out\u201D. Increasing dropout has the effect of making the training\
            \ process more noisy and lowering overall network capacity, but it can\
            \ be an effective regularization method to reduce overfitting and improve\
            \ generalization."
        example_value:
            - 0.2
        expected_impact: 3
        literature_references:
            - https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html
        related_parameters:
            - dropout
        suggested_values: 0.05 - 0.8
        suggested_values_reasoning:
            Tuning dropout is really something to be done
            when all of the big choices about architecture have been settled. Consider
            starting with 0.5 and adjusting the dropout depending on observed model
            performance.
        ui_display_name: attention_dropout
    bos_index:
        ui_display_name: null
    bos_token_id:
        default_value_reasoning: Default value used in pre-trained HF encoder.
        ui_display_name: Beginning-of-Sentence Token Id
    causal:
        ui_display_name: null
    dropout:
        default_value_reasoning: Huggingface default.
        description_implications:
            "Dropout is a computationally cheap regularization\
            \ method where during training, some neurons are randomly ignored or \u201C\
            dropped out\u201D. Increasing dropout has the effect of making the training\
            \ process more noisy and lowering overall network capacity, but it can\
            \ be an effective regularization method to reduce overfitting and improve\
            \ generalization."
        example_value:
            - 0.2
        expected_impact: 3
        literature_references:
            - https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html
        related_parameters:
            - attention_dropout
        suggested_values: 0.05 - 0.8
        suggested_values_reasoning:
            Tuning dropout is really something to be done
            when all of the big choices about architecture have been settled. Consider
            starting with 0.5 and adjusting the dropout depending on observed model
            performance.
        ui_display_name: dropout
    emb_dim:
        ui_display_name: null
    embed_init_std:
        ui_display_name: null
    end_n_top:
        ui_display_name: null
    eos_index:
        ui_display_name: null
    gelu_activation:
        ui_display_name: null
    init_std:
        ui_display_name: null
    is_encoder:
        ui_display_name: null
    lang_id:
        ui_display_name: null
    layer_norm_eps:
        ui_display_name: null
    mask_index:
        ui_display_name: null
    mask_token_id:
        default_value_reasoning: Default value used in pre-trained HF encoder.
        ui_display_name: Mask Token ID
    max_position_embeddings:
        default_value_reasoning: Taken from huggingface.
        description_implications:
            "An embedding is a relatively low-dimensional space
            that is used to translate high-dimensional vectors like words or positions,
            which can have a large vocbulary size. Ideally, after an embedding is
            trained, it captures some of the semantics of the input by placing semantically
            similar inputs close together in the embedding space.


            Increasing the embedding size may cause the model to train more slowly,
            but the higher dimensionality can also improve overall quality."
        expected_impact: 2
        suggested_values: 512
        suggested_values_reasoning:
            Out of the box value based on published literature.
            Try models with smaller or larger embedding sizes to observe relative
            impact.
        ui_display_name: Max Position Embeddings
    max_sequence_length:
        default_value_reasoning:
            Sets the maximum sequence length of the expected
            inputs, so input/output shapes are computed accurately.
        expected_impact: 1
        ui_display_name: null
    n_heads:
        ui_display_name: null
    n_langs:
        default_value_reasoning: Default value used in pre-trained HF encoder.
        expected_impact: 1
        ui_display_name: Number of Languages
    n_layers:
        ui_display_name: null
    pad_index:
        ui_display_name: null
    pad_token_id:
        ui_display_name: null
    pretrained_kwargs:
        ui_display_name: null
    pretrained_model_name_or_path:
        ui_display_name: null
    reduce_output:
        ui_display_name: null
    saved_weights_in_checkpoint:
        default_value_reasoning:
            The weights of the encoder are not necessarily saved
            in the checkpoint. The user has to save them first.
        description_implications:
            The memory footprint for some of these encoders
            can be large.
        expected_impact: 1
        related_parameters:
            - skip_save_model
        suggested_values:
            - false
        suggested_values_reasoning:
            Some of these encoders are large, so it might
            be better to load them as needed, especially if 1. they're not used frequently
            2. the user doesn't have a lot of storage.
        ui_display_name: null
    sinusoidal_embeddings:
        ui_display_name: null
    start_n_top:
        ui_display_name: null
    trainable:
        expected_impact: 2
        ui_display_name: null
    unk_index:
        ui_display_name: null
    use_lang_emb:
        ui_display_name: null
    use_pretrained:
        ui_display_name: null
    vocab:
        default_value_reasoning:
            Computed and passed along internally according to
            preprocessing settings.
        example_value:
            - a
            - b
            - c
        expected_impact: 2
        internal_only: true
        ui_display_name: Not Displayed
    vocab_size:
        internal_only: true
        ui_display_name: Not displayed
XLMRoBERTa:
    type:
        short_description:
            XLM-RoBERTa a large multi-lingual language model trained on 2.5TB of filtered CommonCrawl data.
        long_description:
            The xlmroberta encoder loads a pretrained XLM-RoBERTa (default jplu/tf-xlm-reoberta-base) model
            using the Hugging Face transformers package. XLM-RoBERTa is based on Facebook’s RoBERTa model
            released in 2019. It is a large multi-lingual language model, trained on 2.5TB of filtered
            CommonCrawl data.
    add_pooling_layer:
        ui_display_name: null
    bos_token_id:
        default_value_reasoning: Default value used in pre-trained HF encoder.
        ui_display_name: Beginning-of-Sentence Token Id
    eos_token_id:
        default_value_reasoning: Default value used in pre-trained HF encoder.
        ui_display_name: End-of-Sentence Token Id
    max_sequence_length:
        default_value_reasoning:
            Sets the maximum sequence length of the expected
            inputs, so input/output shapes are computed accurately.
        expected_impact: 1
        ui_display_name: null
    pad_token_id:
        ui_display_name: null
    pretrained_kwargs:
        ui_display_name: null
    pretrained_model_name_or_path:
        ui_display_name: null
    reduce_output:
        ui_display_name: null
    saved_weights_in_checkpoint:
        default_value_reasoning:
            The weights of the encoder are not necessarily saved
            in the checkpoint. The user has to save them first.
        description_implications:
            The memory footprint for some of these encoders
            can be large.
        expected_impact: 1
        related_parameters:
            - skip_save_model
        suggested_values:
            - false
        suggested_values_reasoning:
            Some of these encoders are large, so it might
            be better to load them as needed, especially if 1. they're not used frequently
            2. the user doesn't have a lot of storage.
        ui_display_name: null
    trainable:
        expected_impact: 2
        ui_display_name: null
    use_pretrained:
        ui_display_name: null
    vocab:
        default_value_reasoning:
            Computed and passed along internally according to
            preprocessing settings.
        example_value:
            - a
            - b
            - c
        expected_impact: 2
        internal_only: true
        ui_display_name: Not Displayed
    vocab_size:
        internal_only: true
        ui_display_name: Not displayed
XLNet:
    type:
        short_description:
            XLNet is a transformer that outperforms BERT on a variety of benchmarks.
        long_description:
            The xlnet encoder loads a pretrained XLNet (default xlnet-base-cased) model using the Hugging
            Face transformers package. XLnet is an extension of the Transformer-XL model pre-trained using
            an autoregressive method to learn bidirectional contexts by maximizing the expected likelihood
            over all permutations of the input sequence factorization order. XLNet outperforms BERT on a
            variety of benchmarks.
    attn_type:
        ui_display_name: null
    bi_data:
        ui_display_name: null
    bos_token_id:
        default_value_reasoning: Default value used in pre-trained HF encoder.
        ui_display_name: Beginning-of-Sentence Token Id
    clamp_len:
        ui_display_name: null
    d_inner:
        ui_display_name: null
    d_model:
        ui_display_name: null
    dropout:
        default_value_reasoning: Huggingface default.
        description_implications:
            "Dropout is a computationally cheap regularization\
            \ method where during training, some neurons are randomly ignored or \u201C\
            dropped out\u201D. Increasing dropout has the effect of making the training\
            \ process more noisy and lowering overall network capacity, but it can\
            \ be an effective regularization method to reduce overfitting and improve\
            \ generalization."
        example_value:
            - 0.2
        expected_impact: 3
        literature_references:
            - https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html
        related_parameters:
            - summary_last_dropout
        suggested_values: 0.05 - 0.8
        suggested_values_reasoning:
            Tuning dropout is really something to be done
            when all of the big choices about architecture have been settled. Consider
            starting with 0.5 and adjusting the dropout depending on observed model
            performance.
        ui_display_name: dropout
    end_n_top:
        ui_display_name: null
    eos_token_id:
        default_value_reasoning: Default value used in pre-trained HF encoder.
        ui_display_name: End-of-Sequence Token Id
    ff_activation:
        ui_display_name: null
    initializer_range:
        description_implications:
            There is an ideal value for this variable that doesn't
            lead to the outputs of these matrices to vanish or explode
        example_value:
            - 0.02
        expected_impact: 3
        other_information: Must be greater than 0
        related_parameters:
            - weights_initializer
        suggested_values: 0.01-0.05
        suggested_values_reasoning:
            Large values will likely lead to very large outputs.
            Small values will lead to vanishing outputs.
        ui_display_name: null
    layer_norm_eps:
        ui_display_name: null
    max_sequence_length:
        default_value_reasoning:
            Sets the maximum sequence length of the expected
            inputs, so input/output shapes are computed accurately.
        expected_impact: 1
        ui_display_name: null
    mem_len:
        ui_display_name: null
    n_head:
        ui_display_name: null
    n_layer:
        ui_display_name: null
    pad_token_id:
        ui_display_name: null
    pretrained_kwargs:
        ui_display_name: null
    pretrained_model_name_or_path:
        ui_display_name: null
    reduce_output:
        ui_display_name: null
    reuse_len:
        ui_display_name: null
    same_length:
        ui_display_name: null
    saved_weights_in_checkpoint:
        default_value_reasoning:
            The weights of the encoder are not necessarily saved
            in the checkpoint. The user has to save them first.
        description_implications:
            The memory footprint for some of these encoders
            can be large.
        expected_impact: 1
        related_parameters:
            - skip_save_model
        suggested_values:
            - false
        suggested_values_reasoning:
            Some of these encoders are large, so it might
            be better to load them as needed, especially if 1. they're not used frequently
            2. the user doesn't have a lot of storage.
        ui_display_name: null
    start_n_top:
        ui_display_name: null
    summary_activation:
        default_value_reasoning: Default value used in pre-trained HF encoder.
        ui_display_name: Summary Activation Function
    summary_last_dropout:
        default_value_reasoning: Huggingface default.
        description_implications:
            "Dropout is a computationally cheap regularization\
            \ method where during training, some neurons are randomly ignored or \u201C\
            dropped out\u201D. Increasing dropout has the effect of making the training\
            \ process more noisy and lowering overall network capacity, but it can\
            \ be an effective regularization method to reduce overfitting and improve\
            \ generalization."
        example_value:
            - 0.2
        expected_impact: 3
        literature_references:
            - https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html
        related_parameters:
            - dropout
        suggested_values: 0.05 - 0.8
        suggested_values_reasoning:
            Tuning dropout is really something to be done
            when all of the big choices about architecture have been settled. Consider
            starting with 0.5 and adjusting the dropout depending on observed model
            performance.
        ui_display_name: summary_last_dropout
    summary_type:
        ui_display_name: null
    summary_use_proj:
        ui_display_name: null
    trainable:
        expected_impact: 2
        ui_display_name: null
    untie_r:
        ui_display_name: null
    use_mems_eval:
        ui_display_name: null
    use_mems_train:
        ui_display_name: null
    use_pretrained:
        ui_display_name: null
    vocab:
        default_value_reasoning:
            Computed and passed along internally according to
            preprocessing settings.
        example_value:
            - a
            - b
            - c
        expected_impact: 2
        internal_only: true
        ui_display_name: Not Displayed
    vocab_size:
        internal_only: true
        ui_display_name: Not displayed
