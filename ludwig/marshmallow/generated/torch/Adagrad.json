{
    "docstring": "Implements Adagrad algorithm.\n\n.. math::\n   \\begin{aligned}\n        &\\rule{110mm}{0.4pt}                                                                 \\\\\n        &\\textbf{input}      : \\gamma \\text{ (lr)}, \\: \\theta_0 \\text{ (params)}, \\: f(\\theta)\n            \\text{ (objective)}, \\: \\lambda \\text{ (weight decay)},                          \\\\\n        &\\hspace{12mm}    \\tau \\text{ (initial accumulator value)}, \\: \\eta\\text{ (lr decay)}\\\\\n        &\\textbf{initialize} :  state\\_sum_0 \\leftarrow 0                             \\\\[-1.ex]\n        &\\rule{110mm}{0.4pt}                                                                 \\\\\n        &\\textbf{for} \\: t=1 \\: \\textbf{to} \\: \\ldots \\: \\textbf{do}                         \\\\\n        &\\hspace{5mm}g_t           \\leftarrow   \\nabla_{\\theta} f_t (\\theta_{t-1})           \\\\\n        &\\hspace{5mm} \\tilde{\\gamma}    \\leftarrow \\gamma / (1 +(t-1) \\eta)                  \\\\\n        &\\hspace{5mm} \\textbf{if} \\: \\lambda \\neq 0                                          \\\\\n        &\\hspace{10mm} g_t \\leftarrow g_t + \\lambda \\theta_{t-1}                             \\\\\n        &\\hspace{5mm}state\\_sum_t  \\leftarrow  state\\_sum_{t-1} + g^2_t                      \\\\\n        &\\hspace{5mm}\\theta_t \\leftarrow\n            \\theta_{t-1}- \\tilde{\\gamma} \\frac{g_t}{\\sqrt{state\\_sum_t}+\\epsilon}            \\\\\n        &\\rule{110mm}{0.4pt}                                                          \\\\[-1.ex]\n        &\\bf{return} \\:  \\theta_t                                                     \\\\[-1.ex]\n        &\\rule{110mm}{0.4pt}                                                          \\\\[-1.ex]\n   \\end{aligned}\n\nFor further details regarding the algorithm we refer to `Adaptive Subgradient Methods for Online Learning\nand Stochastic Optimization`_.\n\nArgs:\n    params (iterable): iterable of parameters to optimize or dicts defining\n        parameter groups\n    lr (float, optional): learning rate (default: 1e-2)\n    lr_decay (float, optional): learning rate decay (default: 0)\n    weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n    eps (float, optional): term added to the denominator to improve\n        numerical stability (default: 1e-10)\n\n.. _Adaptive Subgradient Methods for Online Learning and Stochastic\n    Optimization: http://jmlr.org/papers/v12/duchi11a.html",
    "docstring_sections": [
        {
            "type": "parameters",
            "value": [
                {
                    "annotation": "iterable",
                    "default": "",
                    "description": "iterable of parameters to optimize or dicts defining\nparameter groups",
                    "is_args": false,
                    "is_kwargs": false,
                    "is_optional": false,
                    "is_required": true,
                    "kind": "POSITIONAL_OR_KEYWORD",
                    "name": "params"
                },
                {
                    "annotation": "float",
                    "default": "0.01",
                    "description": "learning rate (default: 1e-2)",
                    "is_args": false,
                    "is_kwargs": false,
                    "is_optional": true,
                    "is_required": false,
                    "kind": "POSITIONAL_OR_KEYWORD",
                    "name": "lr"
                },
                {
                    "annotation": "float",
                    "default": "0",
                    "description": "learning rate decay (default: 0)",
                    "is_args": false,
                    "is_kwargs": false,
                    "is_optional": true,
                    "is_required": false,
                    "kind": "POSITIONAL_OR_KEYWORD",
                    "name": "lr_decay"
                },
                {
                    "annotation": "float",
                    "default": "0",
                    "description": "weight decay (L2 penalty) (default: 0)",
                    "is_args": false,
                    "is_kwargs": false,
                    "is_optional": true,
                    "is_required": false,
                    "kind": "POSITIONAL_OR_KEYWORD",
                    "name": "weight_decay"
                },
                {
                    "annotation": "float",
                    "default": "1e-10",
                    "description": "term added to the denominator to improve\nnumerical stability (default: 1e-10)",
                    "is_args": false,
                    "is_kwargs": false,
                    "is_optional": true,
                    "is_required": false,
                    "kind": "POSITIONAL_OR_KEYWORD",
                    "name": "eps"
                }
            ]
        }
    ],
    "name": "Adagrad",
    "path": "torch.optim.adagrad.Adagrad"
}
