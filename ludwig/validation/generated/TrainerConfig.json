{
    "attributes": [
        "ludwig.models.trainer.TrainerConfig.optimizer",
        "ludwig.models.trainer.TrainerConfig.epochs",
        "ludwig.models.trainer.TrainerConfig.regularization_lambda",
        "ludwig.models.trainer.TrainerConfig.regularization_type",
        "ludwig.models.trainer.TrainerConfig.should_shuffle",
        "ludwig.models.trainer.TrainerConfig.learning_rate",
        "ludwig.models.trainer.TrainerConfig.batch_size",
        "ludwig.models.trainer.TrainerConfig.eval_batch_size",
        "ludwig.models.trainer.TrainerConfig.early_stop",
        "ludwig.models.trainer.TrainerConfig.steps_per_checkpoint",
        "ludwig.models.trainer.TrainerConfig.checkpoints_per_epoch",
        "ludwig.models.trainer.TrainerConfig.evaluate_training_set",
        "ludwig.models.trainer.TrainerConfig.reduce_learning_rate_on_plateau",
        "ludwig.models.trainer.TrainerConfig.reduce_learning_rate_on_plateau_patience",
        "ludwig.models.trainer.TrainerConfig.reduce_learning_rate_on_plateau_rate",
        "ludwig.models.trainer.TrainerConfig.reduce_learning_rate_eval_metric",
        "ludwig.models.trainer.TrainerConfig.reduce_learning_rate_eval_split",
        "ludwig.models.trainer.TrainerConfig.increase_batch_size_on_plateau",
        "ludwig.models.trainer.TrainerConfig.increase_batch_size_on_plateau_patience",
        "ludwig.models.trainer.TrainerConfig.increase_batch_size_on_plateau_rate",
        "ludwig.models.trainer.TrainerConfig.increase_batch_size_on_plateau_max",
        "ludwig.models.trainer.TrainerConfig.increase_batch_size_eval_metric",
        "ludwig.models.trainer.TrainerConfig.increase_batch_size_eval_split",
        "ludwig.models.trainer.TrainerConfig.decay",
        "ludwig.models.trainer.TrainerConfig.decay_steps",
        "ludwig.models.trainer.TrainerConfig.decay_rate",
        "ludwig.models.trainer.TrainerConfig.staircase",
        "ludwig.models.trainer.TrainerConfig.gradient_clipping",
        "ludwig.models.trainer.TrainerConfig.validation_field",
        "ludwig.models.trainer.TrainerConfig.validation_metric",
        "ludwig.models.trainer.TrainerConfig.learning_rate_warmup_epochs",
        "ludwig.models.trainer.TrainerConfig.learning_rate_scaling"
    ],
    "bases": [
        "ludwig.validation.marshmallow_utils.BaseMarshmallowConfig"
    ],
    "children": {
        "ludwig.models.trainer.TrainerConfig.__init__": {
            "docstring": null,
            "name": "__init__",
            "path": "ludwig.models.trainer.TrainerConfig.__init__",
            "relative_file_path": "ludwig/models/trainer.py",
            "signature": {
                "parameters": [
                    {
                        "kind": "POSITIONAL_OR_KEYWORD",
                        "name": "self"
                    },
                    {
                        "annotation": "BaseOptimizerConfig",
                        "default": "<factory>",
                        "kind": "POSITIONAL_OR_KEYWORD",
                        "name": "optimizer"
                    },
                    {
                        "annotation": "int",
                        "default": "100",
                        "kind": "POSITIONAL_OR_KEYWORD",
                        "name": "epochs"
                    },
                    {
                        "annotation": "float",
                        "default": "0.0",
                        "kind": "POSITIONAL_OR_KEYWORD",
                        "name": "regularization_lambda"
                    },
                    {
                        "annotation": "Optional[str]",
                        "default": "'l2'",
                        "kind": "POSITIONAL_OR_KEYWORD",
                        "name": "regularization_type"
                    },
                    {
                        "annotation": "bool",
                        "default": "True",
                        "kind": "POSITIONAL_OR_KEYWORD",
                        "name": "should_shuffle"
                    },
                    {
                        "annotation": "float",
                        "default": "0.001",
                        "kind": "POSITIONAL_OR_KEYWORD",
                        "name": "learning_rate"
                    },
                    {
                        "annotation": "Union[int, str]",
                        "default": "128",
                        "kind": "POSITIONAL_OR_KEYWORD",
                        "name": "batch_size"
                    },
                    {
                        "annotation": "Union[NoneType, int, str]",
                        "default": "None",
                        "kind": "POSITIONAL_OR_KEYWORD",
                        "name": "eval_batch_size"
                    },
                    {
                        "annotation": "int",
                        "default": "5",
                        "kind": "POSITIONAL_OR_KEYWORD",
                        "name": "early_stop"
                    },
                    {
                        "annotation": "int",
                        "default": "0",
                        "kind": "POSITIONAL_OR_KEYWORD",
                        "name": "steps_per_checkpoint"
                    },
                    {
                        "annotation": "int",
                        "default": "0",
                        "kind": "POSITIONAL_OR_KEYWORD",
                        "name": "checkpoints_per_epoch"
                    },
                    {
                        "annotation": "bool",
                        "default": "True",
                        "kind": "POSITIONAL_OR_KEYWORD",
                        "name": "evaluate_training_set"
                    },
                    {
                        "annotation": "float",
                        "default": "0.0",
                        "kind": "POSITIONAL_OR_KEYWORD",
                        "name": "reduce_learning_rate_on_plateau"
                    },
                    {
                        "annotation": "int",
                        "default": "5",
                        "kind": "POSITIONAL_OR_KEYWORD",
                        "name": "reduce_learning_rate_on_plateau_patience"
                    },
                    {
                        "annotation": "float",
                        "default": "0.5",
                        "kind": "POSITIONAL_OR_KEYWORD",
                        "name": "reduce_learning_rate_on_plateau_rate"
                    },
                    {
                        "annotation": "str",
                        "default": "'loss'",
                        "kind": "POSITIONAL_OR_KEYWORD",
                        "name": "reduce_learning_rate_eval_metric"
                    },
                    {
                        "annotation": "str",
                        "default": "'training'",
                        "kind": "POSITIONAL_OR_KEYWORD",
                        "name": "reduce_learning_rate_eval_split"
                    },
                    {
                        "annotation": "int",
                        "default": "0",
                        "kind": "POSITIONAL_OR_KEYWORD",
                        "name": "increase_batch_size_on_plateau"
                    },
                    {
                        "annotation": "int",
                        "default": "5",
                        "kind": "POSITIONAL_OR_KEYWORD",
                        "name": "increase_batch_size_on_plateau_patience"
                    },
                    {
                        "annotation": "float",
                        "default": "2.0",
                        "kind": "POSITIONAL_OR_KEYWORD",
                        "name": "increase_batch_size_on_plateau_rate"
                    },
                    {
                        "annotation": "int",
                        "default": "512",
                        "kind": "POSITIONAL_OR_KEYWORD",
                        "name": "increase_batch_size_on_plateau_max"
                    },
                    {
                        "annotation": "str",
                        "default": "'loss'",
                        "kind": "POSITIONAL_OR_KEYWORD",
                        "name": "increase_batch_size_eval_metric"
                    },
                    {
                        "annotation": "str",
                        "default": "'training'",
                        "kind": "POSITIONAL_OR_KEYWORD",
                        "name": "increase_batch_size_eval_split"
                    },
                    {
                        "annotation": "bool",
                        "default": "False",
                        "kind": "POSITIONAL_OR_KEYWORD",
                        "name": "decay"
                    },
                    {
                        "annotation": "int",
                        "default": "10000",
                        "kind": "POSITIONAL_OR_KEYWORD",
                        "name": "decay_steps"
                    },
                    {
                        "annotation": "float",
                        "default": "0.96",
                        "kind": "POSITIONAL_OR_KEYWORD",
                        "name": "decay_rate"
                    },
                    {
                        "annotation": "bool",
                        "default": "False",
                        "kind": "POSITIONAL_OR_KEYWORD",
                        "name": "staircase"
                    },
                    {
                        "annotation": "Optional[ludwig.modules.optimization_modules.GradientClippingConfig]",
                        "default": "<factory>",
                        "kind": "POSITIONAL_OR_KEYWORD",
                        "name": "gradient_clipping"
                    },
                    {
                        "annotation": "str",
                        "default": "'combined'",
                        "kind": "POSITIONAL_OR_KEYWORD",
                        "name": "validation_field"
                    },
                    {
                        "annotation": "str",
                        "default": "'loss'",
                        "kind": "POSITIONAL_OR_KEYWORD",
                        "name": "validation_metric"
                    },
                    {
                        "annotation": "float",
                        "default": "1.0",
                        "kind": "POSITIONAL_OR_KEYWORD",
                        "name": "learning_rate_warmup_epochs"
                    },
                    {
                        "annotation": "str",
                        "default": "'linear'",
                        "kind": "POSITIONAL_OR_KEYWORD",
                        "name": "learning_rate_scaling"
                    }
                ],
                "return_annotation": "None"
            }
        },
        "ludwig.models.trainer.TrainerConfig.batch_size": {
            "docstring": "Size of batch to pass to the model for training (default: 128).",
            "name": "batch_size",
            "path": "ludwig.models.trainer.TrainerConfig.batch_size",
            "relative_file_path": "ludwig/models/trainer.py",
            "type": "Union[int, str]"
        },
        "ludwig.models.trainer.TrainerConfig.checkpoints_per_epoch": {
            "docstring": "Number of checkpoints per epoch. For example, 2 -> checkpoints are written every half of an epoch. Note that it\nis invalid to specify both non-zero `steps_per_checkpoint` and non-zero `checkpoints_per_epoch` (default: 0).",
            "name": "checkpoints_per_epoch",
            "path": "ludwig.models.trainer.TrainerConfig.checkpoints_per_epoch",
            "relative_file_path": "ludwig/models/trainer.py",
            "type": "int"
        },
        "ludwig.models.trainer.TrainerConfig.decay": {
            "docstring": "Turn on exponential decay of the learning rate (default: False).",
            "name": "decay",
            "path": "ludwig.models.trainer.TrainerConfig.decay",
            "relative_file_path": "ludwig/models/trainer.py",
            "type": "bool"
        },
        "ludwig.models.trainer.TrainerConfig.decay_rate": {
            "docstring": "TODO: Document parameters. (default: 0.96).",
            "name": "decay_rate",
            "path": "ludwig.models.trainer.TrainerConfig.decay_rate",
            "relative_file_path": "ludwig/models/trainer.py",
            "type": "float"
        },
        "ludwig.models.trainer.TrainerConfig.decay_steps": {
            "docstring": "TODO: Document parameters. (default: 10000).",
            "name": "decay_steps",
            "path": "ludwig.models.trainer.TrainerConfig.decay_steps",
            "relative_file_path": "ludwig/models/trainer.py",
            "type": "int"
        },
        "ludwig.models.trainer.TrainerConfig.early_stop": {
            "docstring": "How many epochs without any improvement in the `validation_metric` triggers the algorithm to stop. Can be set to\n-1, which disables early_stop (default: 5).",
            "name": "early_stop",
            "path": "ludwig.models.trainer.TrainerConfig.early_stop",
            "relative_file_path": "ludwig/models/trainer.py",
            "type": "int"
        },
        "ludwig.models.trainer.TrainerConfig.epochs": {
            "docstring": "Number of epochs the algorithm is intended to be run over (default: 100).",
            "name": "epochs",
            "path": "ludwig.models.trainer.TrainerConfig.epochs",
            "relative_file_path": "ludwig/models/trainer.py",
            "type": "int"
        },
        "ludwig.models.trainer.TrainerConfig.eval_batch_size": {
            "docstring": "Size of batch to pass to the model for evaluation (default: 'auto').",
            "name": "eval_batch_size",
            "path": "ludwig.models.trainer.TrainerConfig.eval_batch_size",
            "relative_file_path": "ludwig/models/trainer.py",
            "type": "Union[NoneType, int, str]"
        },
        "ludwig.models.trainer.TrainerConfig.evaluate_training_set": {
            "docstring": "Whether to include the entire training set during evaluation (default: True).",
            "name": "evaluate_training_set",
            "path": "ludwig.models.trainer.TrainerConfig.evaluate_training_set",
            "relative_file_path": "ludwig/models/trainer.py",
            "type": "bool"
        },
        "ludwig.models.trainer.TrainerConfig.gradient_clipping": {
            "docstring": "Instance of `ludwig.modules.optimization_modules.GradientClippingConfig` that sets gradient clipping params.\n(default: `ludwig.modules.optimization_modules.GradientClippingConfig()`)",
            "name": "gradient_clipping",
            "path": "ludwig.models.trainer.TrainerConfig.gradient_clipping",
            "relative_file_path": "ludwig/models/trainer.py",
            "type": "Optional[ludwig.modules.optimization_modules.GradientClippingConfig]"
        },
        "ludwig.models.trainer.TrainerConfig.increase_batch_size_eval_metric": {
            "docstring": "TODO: Document parameters. (default: 'loss').",
            "name": "increase_batch_size_eval_metric",
            "path": "ludwig.models.trainer.TrainerConfig.increase_batch_size_eval_metric",
            "relative_file_path": "ludwig/models/trainer.py",
            "type": "str"
        },
        "ludwig.models.trainer.TrainerConfig.increase_batch_size_eval_split": {
            "docstring": "TODO: Document parameters. (default: 'training').",
            "name": "increase_batch_size_eval_split",
            "path": "ludwig.models.trainer.TrainerConfig.increase_batch_size_eval_split",
            "relative_file_path": "ludwig/models/trainer.py",
            "type": "str"
        },
        "ludwig.models.trainer.TrainerConfig.increase_batch_size_on_plateau": {
            "docstring": "Number to increase the batch size by on a plateau (default: 0).",
            "name": "increase_batch_size_on_plateau",
            "path": "ludwig.models.trainer.TrainerConfig.increase_batch_size_on_plateau",
            "relative_file_path": "ludwig/models/trainer.py",
            "type": "int"
        },
        "ludwig.models.trainer.TrainerConfig.increase_batch_size_on_plateau_max": {
            "docstring": "Maximum size of the batch (default: 512).",
            "name": "increase_batch_size_on_plateau_max",
            "path": "ludwig.models.trainer.TrainerConfig.increase_batch_size_on_plateau_max",
            "relative_file_path": "ludwig/models/trainer.py",
            "type": "int"
        },
        "ludwig.models.trainer.TrainerConfig.increase_batch_size_on_plateau_patience": {
            "docstring": "How many epochs to wait for before increasing the batch size (default: 5).",
            "name": "increase_batch_size_on_plateau_patience",
            "path": "ludwig.models.trainer.TrainerConfig.increase_batch_size_on_plateau_patience",
            "relative_file_path": "ludwig/models/trainer.py",
            "type": "int"
        },
        "ludwig.models.trainer.TrainerConfig.increase_batch_size_on_plateau_rate": {
            "docstring": "Rate at which the batch size increases (default: 2.0).",
            "name": "increase_batch_size_on_plateau_rate",
            "path": "ludwig.models.trainer.TrainerConfig.increase_batch_size_on_plateau_rate",
            "relative_file_path": "ludwig/models/trainer.py",
            "type": "float"
        },
        "ludwig.models.trainer.TrainerConfig.learning_rate": {
            "docstring": "Learning rate specified in configuration, represents how much to scale the gradients by. If 'auto',\n`tune_learning_rate` must be called before training to estimate the optimal learning rate. (default: 0.001).",
            "name": "learning_rate",
            "path": "ludwig.models.trainer.TrainerConfig.learning_rate",
            "relative_file_path": "ludwig/models/trainer.py",
            "type": "float"
        },
        "ludwig.models.trainer.TrainerConfig.learning_rate_scaling": {
            "docstring": "Scale by which to increase the learning rate as the number of distributed workers increases. Traditionally\nthe learning rate is scaled linearly with the number of workers to reflect the proportion by which\nthe effective batch size is increased. For very large batch sizes, a softer square-root scale can sometimes lead\nto better model performance. If the learning rate is hand-tuned for a given number of workers, setting this value\nto constant can be used to disable scale-up (default: linear).",
            "name": "learning_rate_scaling",
            "path": "ludwig.models.trainer.TrainerConfig.learning_rate_scaling",
            "relative_file_path": "ludwig/models/trainer.py",
            "type": "str"
        },
        "ludwig.models.trainer.TrainerConfig.learning_rate_warmup_epochs": {
            "docstring": "Number of epochs to warmup the learning rate for (default: 1.0).",
            "name": "learning_rate_warmup_epochs",
            "path": "ludwig.models.trainer.TrainerConfig.learning_rate_warmup_epochs",
            "relative_file_path": "ludwig/models/trainer.py",
            "type": "float"
        },
        "ludwig.models.trainer.TrainerConfig.optimizer": {
            "docstring": "Instance of `ludwig.modules.optimization_modules.BaseOptimizerConfig` that specifies a torch-supported optimizer\nand its attributes (default: `ludwig.modules.optimization_modules.AdamOptimizerConfig()`).",
            "name": "optimizer",
            "path": "ludwig.models.trainer.TrainerConfig.optimizer",
            "relative_file_path": "ludwig/models/trainer.py",
            "type": "BaseOptimizerConfig"
        },
        "ludwig.models.trainer.TrainerConfig.reduce_learning_rate_eval_metric": {
            "docstring": "TODO: Document parameters. (default: `ludwig.constants.LOSS`).",
            "name": "reduce_learning_rate_eval_metric",
            "path": "ludwig.models.trainer.TrainerConfig.reduce_learning_rate_eval_metric",
            "relative_file_path": "ludwig/models/trainer.py",
            "type": "str"
        },
        "ludwig.models.trainer.TrainerConfig.reduce_learning_rate_eval_split": {
            "docstring": "TODO: Document parameters. (default: `ludwig.constants.TRAINING`).",
            "name": "reduce_learning_rate_eval_split",
            "path": "ludwig.models.trainer.TrainerConfig.reduce_learning_rate_eval_split",
            "relative_file_path": "ludwig/models/trainer.py",
            "type": "str"
        },
        "ludwig.models.trainer.TrainerConfig.reduce_learning_rate_on_plateau": {
            "docstring": "Reduces the learning rate when the algorithm hits a plateau (i.e. the performance on the validation does not\nimprove) (default: 0.0).",
            "name": "reduce_learning_rate_on_plateau",
            "path": "ludwig.models.trainer.TrainerConfig.reduce_learning_rate_on_plateau",
            "relative_file_path": "ludwig/models/trainer.py",
            "type": "float"
        },
        "ludwig.models.trainer.TrainerConfig.reduce_learning_rate_on_plateau_patience": {
            "docstring": "How many epochs have to pass before the learning rate reduces (default: 5).",
            "name": "reduce_learning_rate_on_plateau_patience",
            "path": "ludwig.models.trainer.TrainerConfig.reduce_learning_rate_on_plateau_patience",
            "relative_file_path": "ludwig/models/trainer.py",
            "type": "int"
        },
        "ludwig.models.trainer.TrainerConfig.reduce_learning_rate_on_plateau_rate": {
            "docstring": "Rate at which we reduce the learning rate (default: 0.5).",
            "name": "reduce_learning_rate_on_plateau_rate",
            "path": "ludwig.models.trainer.TrainerConfig.reduce_learning_rate_on_plateau_rate",
            "relative_file_path": "ludwig/models/trainer.py",
            "type": "float"
        },
        "ludwig.models.trainer.TrainerConfig.regularization_lambda": {
            "docstring": "Strength of the $L2$ regularization (default: 0.0).",
            "name": "regularization_lambda",
            "path": "ludwig.models.trainer.TrainerConfig.regularization_lambda",
            "relative_file_path": "ludwig/models/trainer.py",
            "type": "float"
        },
        "ludwig.models.trainer.TrainerConfig.regularization_type": {
            "docstring": "Type of regularization, one of ('l1', 'l2', 'l1_l2', None) (default: 'l2').",
            "name": "regularization_type",
            "path": "ludwig.models.trainer.TrainerConfig.regularization_type",
            "relative_file_path": "ludwig/models/trainer.py",
            "type": "Optional[str]"
        },
        "ludwig.models.trainer.TrainerConfig.should_shuffle": {
            "docstring": "Whether to shuffle batches during training when true (default: True).",
            "name": "should_shuffle",
            "path": "ludwig.models.trainer.TrainerConfig.should_shuffle",
            "relative_file_path": "ludwig/models/trainer.py",
            "type": "bool"
        },
        "ludwig.models.trainer.TrainerConfig.staircase": {
            "docstring": "Decays the learning rate at discrete intervals (default: False).",
            "name": "staircase",
            "path": "ludwig.models.trainer.TrainerConfig.staircase",
            "relative_file_path": "ludwig/models/trainer.py",
            "type": "bool"
        },
        "ludwig.models.trainer.TrainerConfig.steps_per_checkpoint": {
            "docstring": "How often the model is checkpointed. Also dictates maximum evaluation frequency. If 0 the model is checkpointed\nafter every epoch. (default: 0).",
            "name": "steps_per_checkpoint",
            "path": "ludwig.models.trainer.TrainerConfig.steps_per_checkpoint",
            "relative_file_path": "ludwig/models/trainer.py",
            "type": "int"
        },
        "ludwig.models.trainer.TrainerConfig.validation_field": {
            "docstring": "First output feature, by default it is set as the same field of the first output feature (default:\n`ludwig.constants.COMBINED`).",
            "name": "validation_field",
            "path": "ludwig.models.trainer.TrainerConfig.validation_field",
            "relative_file_path": "ludwig/models/trainer.py",
            "type": "str"
        },
        "ludwig.models.trainer.TrainerConfig.validation_metric": {
            "docstring": "Metric used on `validation_field`, set by default to accuracy (default: `ludwig.constants.LOSS`).",
            "name": "validation_metric",
            "path": "ludwig.models.trainer.TrainerConfig.validation_metric",
            "relative_file_path": "ludwig/models/trainer.py",
            "type": "str"
        }
    },
    "docstring": "TrainerConfig is a dataclass that configures most of the hyperparameters used for model training.",
    "name": "TrainerConfig",
    "path": "ludwig.models.trainer.TrainerConfig"
}
