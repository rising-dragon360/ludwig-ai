{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimization with Hyperopt\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Hyperopt package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hyperopt in /usr/local/lib/python3.6/dist-packages (0.2.2)\r\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from hyperopt) (0.18.2)\r\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from hyperopt) (1.11.0)\r\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from hyperopt) (4.36.1)\r\n",
      "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.6/dist-packages (from hyperopt) (1.2.2)\r\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from hyperopt) (1.3.1)\r\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from hyperopt) (1.17.3)\r\n",
      "Requirement already satisfied: networkx==2.2 in /usr/local/lib/python3.6/dist-packages (from hyperopt) (2.2)\r\n",
      "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx==2.2->hyperopt) (4.4.1)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install hyperopt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "from ludwig.api import LudwigModel\n",
    "from ludwig.utils.data_utils import load_json\n",
    "from ludwig.visualize import learning_curves\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from hyperopt.pyll.stochastic import rng_from_seed\n",
    "import logging\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean out old results\n",
    "try:\n",
    "    shutil.rmtree('./results')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    shutil.rmtree('./visualizations')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    file_list = glob.glob('./data/*.json')\n",
    "    file_list.append(glob.glob('./data/*.hdf5'))\n",
    "    os.remove(file_list)\n",
    "except:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(891, 12)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_df = pd.read_csv('./data/train.csv')\n",
    "raw_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, vald_df = train_test_split(raw_df, test_size = 0.2, random_state=13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define objective function for minimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_model(params):\n",
    "    \n",
    "    print(params)\n",
    "    \n",
    "    model_definition['training']['learning_rate'] = params['learning_rate']\n",
    "    model_definition['training']['batch_size'] = params['batch_size']\n",
    "    model_definition['output_features'][0]['num_fc_layers'] = np.int(params['output_fc_num_layers'])\n",
    "    model_definition['output_features'][0]['fc_size'] = np.int(params['output_fc_size'])\n",
    "    \n",
    "    # Define Ludwig model object that drive model training\n",
    "    model = LudwigModel(model_definition,\n",
    "                        logging_level=logging.WARN)\n",
    "\n",
    "    # initiate model training\n",
    "    train_stats = model.train(data_train_df = train_df,\n",
    "                            data_validation_df = vald_df,\n",
    "                            experiment_name='experiment_name',\n",
    "                            model_name='model_name',\n",
    "                            skip_save_training_description=True, \n",
    "                            skip_save_training_statistics=True, \n",
    "                            skip_save_model=True, \n",
    "                            skip_save_progress=True,                          \n",
    "                            skip_save_log=True,\n",
    "                            skip_save_processed_input=True,\n",
    "                            random_seed=42)\n",
    "\n",
    "\n",
    "\n",
    "    model.close()\n",
    "    \n",
    "    validation_losses = train_stats['validation']['Survived']['loss']\n",
    "    \n",
    "    last_epoch = len(validation_losses)\n",
    "    \n",
    "    return {'loss': validation_losses[last_epoch - 1], 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_definition = {\n",
    "    'input_features': [\n",
    "        {'name': 'Pclass', 'type': 'category'},\n",
    "        {'name': 'Sex', 'type': 'category'},\n",
    "        {'name': 'Age', 'type': 'numerical', \n",
    "            'preprocessing': {'missing_value_strategy': 'fill_with_mean', 'normalization': 'zscore'}},\n",
    "        {'name': 'SibSp', 'type': 'numerical', \n",
    "            'preprocessing': {'missing_value_strategy': 'fill_with_mean', 'normalization': 'minmax'}},\n",
    "        {'name': 'Parch', 'type': 'numerical',\n",
    "            'preprocessing': {'missing_value_strategy': 'fill_with_mean', 'normalization': 'minmax'}},\n",
    "        {'name': 'Fare', 'type': 'numerical',\n",
    "            'preprocessing': {'missing_value_strategy': 'fill_with_mean', 'normalization': 'zscore'}},\n",
    "        {'name': 'Embarked', 'type': 'category'}\n",
    "    ],\n",
    "    \n",
    "    'output_features':[\n",
    "        {'name': 'Survived', 'type': 'binary'}\n",
    "    ],\n",
    "    \n",
    "    'training': {}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define hyperparameter search space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED=13\n",
    "\n",
    "space = {'learning_rate':  hp.uniform('learning_rate', 0.0001, 0.01, rng=rng_from_seed(SEED)),\n",
    "         'batch_size': 2 ** (3 + hp.randint('batch_size_exponent',5, rng=rng_from_seed(SEED))),\n",
    "         'output_fc_num_layers': hp.quniform('output_fc_num_layers',1,4,1, rng=rng_from_seed(SEED)),\n",
    "         'output_fc_size': hp.quniform('output_fc_size',5,100,1, rng=rng_from_seed(SEED))\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 8, 'learning_rate': 0.00010733948578272456, 'output_fc_num_layers': 4.0, 'output_fc_size': 67.0}\n",
      "  0%|          | 0/30 [00:00<?, ?it/s, best loss: ?]WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /opt/project/ludwig/features/binary_feature.py:194: calling weighted_cross_entropy_with_logits (from tensorflow.python.ops.nn_impl) with targets is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "targets is deprecated, use labels instead\n",
      "WARNING:tensorflow:From /opt/project/ludwig/utils/tf_utils.py:78: The name tf.GPUOptions is deprecated. Please use tf.compat.v1.GPUOptions instead.\n",
      "\n",
      "{'batch_size': 128, 'learning_rate': 0.009305337844092676, 'output_fc_num_layers': 2.0, 'output_fc_size': 65.0}\n",
      "{'batch_size': 16, 'learning_rate': 0.0044302621468053326, 'output_fc_num_layers': 2.0, 'output_fc_size': 14.0}\n",
      "{'batch_size': 8, 'learning_rate': 0.008866273911047155, 'output_fc_num_layers': 2.0, 'output_fc_size': 9.0}\n",
      "{'batch_size': 16, 'learning_rate': 0.008015462167976896, 'output_fc_num_layers': 2.0, 'output_fc_size': 66.0}\n",
      "{'batch_size': 16, 'learning_rate': 0.004379969956771687, 'output_fc_num_layers': 2.0, 'output_fc_size': 30.0}\n",
      "{'batch_size': 16, 'learning_rate': 0.0055091670965686365, 'output_fc_num_layers': 4.0, 'output_fc_size': 52.0}\n",
      "{'batch_size': 128, 'learning_rate': 0.004964117767226885, 'output_fc_num_layers': 1.0, 'output_fc_size': 15.0}\n",
      "{'batch_size': 64, 'learning_rate': 0.006991331615014415, 'output_fc_num_layers': 2.0, 'output_fc_size': 70.0}\n",
      "{'batch_size': 8, 'learning_rate': 0.008527319320052915, 'output_fc_num_layers': 1.0, 'output_fc_size': 72.0}\n",
      "{'batch_size': 128, 'learning_rate': 0.0034633652525555315, 'output_fc_num_layers': 1.0, 'output_fc_size': 20.0}\n",
      "{'batch_size': 16, 'learning_rate': 0.007692684813795102, 'output_fc_num_layers': 3.0, 'output_fc_size': 64.0}\n",
      "{'batch_size': 64, 'learning_rate': 0.001196084311709852, 'output_fc_num_layers': 3.0, 'output_fc_size': 28.0}\n",
      "{'batch_size': 32, 'learning_rate': 0.004066459705571373, 'output_fc_num_layers': 2.0, 'output_fc_size': 49.0}\n",
      "{'batch_size': 32, 'learning_rate': 0.003646729009357172, 'output_fc_num_layers': 2.0, 'output_fc_size': 59.0}\n",
      "{'batch_size': 128, 'learning_rate': 0.00517115753570163, 'output_fc_num_layers': 3.0, 'output_fc_size': 47.0}\n",
      "{'batch_size': 16, 'learning_rate': 0.005675458492255646, 'output_fc_num_layers': 1.0, 'output_fc_size': 93.0}\n",
      "{'batch_size': 8, 'learning_rate': 0.0019002526180839155, 'output_fc_num_layers': 3.0, 'output_fc_size': 78.0}\n",
      "{'batch_size': 16, 'learning_rate': 0.009339098645940899, 'output_fc_num_layers': 4.0, 'output_fc_size': 64.0}\n",
      "{'batch_size': 32, 'learning_rate': 0.009786427617716684, 'output_fc_num_layers': 3.0, 'output_fc_size': 71.0}\n",
      "{'batch_size': 128, 'learning_rate': 0.00016190181618999655, 'output_fc_num_layers': 4.0, 'output_fc_size': 88.0}\n",
      "{'batch_size': 8, 'learning_rate': 0.002789367223742025, 'output_fc_num_layers': 4.0, 'output_fc_size': 41.0}\n",
      "{'batch_size': 8, 'learning_rate': 0.00015565976431732365, 'output_fc_num_layers': 1.0, 'output_fc_size': 84.0}\n",
      "{'batch_size': 128, 'learning_rate': 0.0011681841097959424, 'output_fc_num_layers': 1.0, 'output_fc_size': 98.0}\n",
      "{'batch_size': 128, 'learning_rate': 0.0010428166560476904, 'output_fc_num_layers': 1.0, 'output_fc_size': 98.0}\n",
      "{'batch_size': 8, 'learning_rate': 0.00228402142885611, 'output_fc_num_layers': 1.0, 'output_fc_size': 83.0}\n",
      "{'batch_size': 64, 'learning_rate': 0.0007430711810205, 'output_fc_num_layers': 1.0, 'output_fc_size': 97.0}\n",
      "{'batch_size': 64, 'learning_rate': 0.00032340761979764686, 'output_fc_num_layers': 1.0, 'output_fc_size': 90.0}\n",
      "{'batch_size': 64, 'learning_rate': 0.0001653300288409657, 'output_fc_num_layers': 1.0, 'output_fc_size': 80.0}\n",
      "{'batch_size': 64, 'learning_rate': 0.0064577703588307896, 'output_fc_num_layers': 1.0, 'output_fc_size': 100.0}\n",
      "100%|██████████| 30/30 [01:25<00:00,  2.86s/it, best loss: 0.3961795721640134]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "best = fmin(fn=score_model,\n",
    "    space=space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimal hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size: 64 , learning_rate: 0.0007430711810205 , fc_num_layers: 1 , fc_size: 97\n"
     ]
    }
   ],
   "source": [
    "print('batch size:', 2 ** np.int(3 + best['batch_size_exponent']),\n",
    "      ', learning_rate:', best['learning_rate'],\n",
    "      ', fc_num_layers:', np.int(best['output_fc_num_layers']),\n",
    "      ', fc_size:', np.int(best['output_fc_size']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with optimal hyperparameters on the whole data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean out old results\n",
    "try:\n",
    "    shutil.rmtree('./results')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    shutil.rmtree('./visualizations')\n",
    "except:\n",
    "    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set optimal hyperparameters for training    \n",
    "model_definition['training']['learning_rate'] = best['learning_rate']\n",
    "model_definition['training']['batch_size'] = 2** (3 + best['batch_size_exponent'])\n",
    "model_definition['output_features'][0]['num_fc_layers'] = np.int(best['output_fc_num_layers'])\n",
    "model_definition['output_features'][0]['fc_size'] = np.int(best['output_fc_size'])\n",
    "\n",
    "\n",
    "# Define Ludwig model object that drive model training\n",
    "model = LudwigModel(model_definition,\n",
    "                    logging_level=logging.WARN)\n",
    "\n",
    "# initiate model training\n",
    "train_stats = model.train(data_train_df = raw_df,\n",
    "                        experiment_name='hyperparameter_training',\n",
    "                        model_name='optimized_model',                        \n",
    "                        random_seed=42)\n",
    "\n",
    "\n",
    "\n",
    "model.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
