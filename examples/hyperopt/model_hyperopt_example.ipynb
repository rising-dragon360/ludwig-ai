{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimization with Hyperopt\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Hyperopt package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting hyperopt\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0b/4a/79541d4f61e7878f846f68ab31ed709bac6ee99345378c0e02658c3be0d4/hyperopt-0.2.2-py3-none-any.whl (1.9MB)\n",
      "\u001b[K     |████████████████████████████████| 1.9MB 1.7MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: cloudpickle in /usr/local/lib/python3.6/dist-packages (from hyperopt) (1.2.2)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from hyperopt) (1.11.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from hyperopt) (4.36.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from hyperopt) (1.17.3)\n",
      "Collecting future\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/0b/38b06fd9b92dc2b68d58b75f900e97884c45bedd2ff83203d933cf5851c9/future-0.18.2.tar.gz (829kB)\n",
      "\u001b[K     |████████████████████████████████| 829kB 2.9MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting networkx==2.2\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f3/f4/7e20ef40b118478191cec0b58c3192f822cace858c19505c7670961b76b2/networkx-2.2.zip (1.7MB)\n",
      "\u001b[K     |████████████████████████████████| 1.7MB 7.1MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from hyperopt) (1.3.1)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx==2.2->hyperopt) (4.4.1)\n",
      "Building wheels for collected packages: future, networkx\n",
      "  Building wheel for future (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for future: filename=future-0.18.2-cp36-none-any.whl size=493275 sha256=d2427765366dc823355eccffe20d9a3f00f1c05b25a92ff8dae010d88c00d058\n",
      "  Stored in directory: /root/.cache/pip/wheels/8b/99/a0/81daf51dcd359a9377b110a8a886b3895921802d2fc1b2397e\n",
      "  Building wheel for networkx (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for networkx: filename=networkx-2.2-py2.py3-none-any.whl size=1527657 sha256=42a10bea584d7d9b683876e4cc7adeeea5daab040622467e4a5402f9f8daf6bc\n",
      "  Stored in directory: /root/.cache/pip/wheels/68/f8/29/b53346a112a07d30a5a84d53f19aeadaa1a474897c0423af91\n",
      "Successfully built future networkx\n",
      "Installing collected packages: future, networkx, hyperopt\n",
      "  Found existing installation: networkx 2.3\n",
      "    Uninstalling networkx-2.3:\n",
      "      Successfully uninstalled networkx-2.3\n",
      "Successfully installed future-0.18.2 hyperopt-0.2.2 networkx-2.2\n"
     ]
    }
   ],
   "source": [
    "!pip install hyperopt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "from ludwig.api import LudwigModel\n",
    "from ludwig.utils.data_utils import load_json\n",
    "from ludwig.visualize import learning_curves\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from hyperopt.pyll.stochastic import rng_from_seed\n",
    "import logging\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean out old results\n",
    "try:\n",
    "    shutil.rmtree('./results')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    shutil.rmtree('./visualizations')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    file_list = glob.glob('./data/*.json')\n",
    "    file_list.append(glob.glob('./data/*.hdf5'))\n",
    "    os.remove(file_list)\n",
    "except:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(891, 12)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_df = pd.read_csv('./data/train.csv')\n",
    "raw_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, vald_df = train_test_split(raw_df, test_size = 0.2, random_state=13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define objective function for minimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_model(params):\n",
    "    \n",
    "    print(params)\n",
    "    \n",
    "    model_definition['training']['learning_rate'] = params['learning_rate']\n",
    "    model_definition['training']['batch_size'] = params['batch_size']\n",
    "    model_definition['output_features'][0]['num_fc_layers'] = np.int(params['output_fc_num_layers'])\n",
    "    model_definition['output_features'][0]['fc_size'] = np.int(params['output_fc_size'])\n",
    "    \n",
    "    # Define Ludwig model object that drive model training\n",
    "    model = LudwigModel(model_definition,\n",
    "                        logging_level=logging.WARN)\n",
    "\n",
    "    # initiate model training\n",
    "    train_stats = model.train(data_train_df = train_df,\n",
    "                            data_validation_df = vald_df,\n",
    "                            experiment_name='experiment_name',\n",
    "                            model_name='model_name',\n",
    "                            skip_save_training_description=True, \n",
    "                            skip_save_training_statistics=True, \n",
    "                            skip_save_model=True, \n",
    "                            skip_save_progress=True,                          \n",
    "                            skip_save_log=True,\n",
    "                            skip_save_processed_input=True,\n",
    "                            random_seed=42)\n",
    "\n",
    "\n",
    "\n",
    "    model.close()\n",
    "    \n",
    "    validation_losses = train_stats['validation']['Survived']['loss']\n",
    "    \n",
    "    last_epoch = len(validation_losses)\n",
    "    \n",
    "    return {'loss': validation_losses[last_epoch - 1], 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_definition = {\n",
    "    'input_features': [\n",
    "        {'name': 'Pclass', 'type': 'category'},\n",
    "        {'name': 'Sex', 'type': 'category'},\n",
    "        {'name': 'Age', 'type': 'numerical', \n",
    "            'preprocessing': {'missing_value_strategy': 'fill_with_mean', 'normalization': 'zscore'}},\n",
    "        {'name': 'SibSp', 'type': 'numerical', \n",
    "            'preprocessing': {'missing_value_strategy': 'fill_with_mean', 'normalization': 'minmax'}},\n",
    "        {'name': 'Parch', 'type': 'numerical',\n",
    "            'preprocessing': {'missing_value_strategy': 'fill_with_mean', 'normalization': 'minmax'}},\n",
    "        {'name': 'Fare', 'type': 'numerical',\n",
    "            'preprocessing': {'missing_value_strategy': 'fill_with_mean', 'normalization': 'zscore'}},\n",
    "        {'name': 'Embarked', 'type': 'category'}\n",
    "    ],\n",
    "    \n",
    "    'output_features':[\n",
    "        {'name': 'Survived', 'type': 'binary'}\n",
    "    ],\n",
    "    \n",
    "    'training': {}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define hyperparameter search space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED=13\n",
    "\n",
    "space = {'learning_rate':  hp.uniform('learning_rate', 0.0001, 0.01, rng=rng_from_seed(SEED)),\n",
    "         'batch_size': 2 ** (3 + hp.randint('batch_size_exponent',5, rng=rng_from_seed(SEED))),\n",
    "         'output_fc_num_layers': hp.quniform('output_fc_num_layers',1,4,1, rng=rng_from_seed(SEED)),\n",
    "         'output_fc_size': hp.quniform('output_fc_size',5,100,1, rng=rng_from_seed(SEED))\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 64, 'learning_rate': 0.002476612622869838, 'output_fc_num_layers': 2.0, 'output_fc_size': 8.0}\n",
      "  0%|          | 0/30 [00:00<?, ?it/s, best loss: ?]WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /opt/project/ludwig/features/binary_feature.py:194: calling weighted_cross_entropy_with_logits (from tensorflow.python.ops.nn_impl) with targets is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "targets is deprecated, use labels instead\n",
      "WARNING:tensorflow:From /opt/project/ludwig/utils/tf_utils.py:78: The name tf.GPUOptions is deprecated. Please use tf.compat.v1.GPUOptions instead.\n",
      "\n",
      "{'batch_size': 32, 'learning_rate': 0.004832685688730726, 'output_fc_num_layers': 2.0, 'output_fc_size': 91.0}\n",
      "{'batch_size': 8, 'learning_rate': 0.002846487689031914, 'output_fc_num_layers': 4.0, 'output_fc_size': 19.0}\n",
      "{'batch_size': 16, 'learning_rate': 0.002430436589437408, 'output_fc_num_layers': 3.0, 'output_fc_size': 55.0}\n",
      "{'batch_size': 64, 'learning_rate': 0.005678823076109719, 'output_fc_num_layers': 2.0, 'output_fc_size': 66.0}\n",
      "{'batch_size': 8, 'learning_rate': 0.00608247530874953, 'output_fc_num_layers': 4.0, 'output_fc_size': 10.0}\n",
      "{'batch_size': 8, 'learning_rate': 0.00937153269542596, 'output_fc_num_layers': 3.0, 'output_fc_size': 32.0}\n",
      "{'batch_size': 16, 'learning_rate': 0.005376979355944602, 'output_fc_num_layers': 2.0, 'output_fc_size': 18.0}\n",
      "{'batch_size': 32, 'learning_rate': 0.0037062937396301595, 'output_fc_num_layers': 2.0, 'output_fc_size': 97.0}\n",
      "{'batch_size': 16, 'learning_rate': 0.007189724220513468, 'output_fc_num_layers': 3.0, 'output_fc_size': 84.0}\n",
      "{'batch_size': 64, 'learning_rate': 0.0011953784346892453, 'output_fc_num_layers': 2.0, 'output_fc_size': 58.0}\n",
      "{'batch_size': 128, 'learning_rate': 0.005281101163284269, 'output_fc_num_layers': 3.0, 'output_fc_size': 60.0}\n",
      "{'batch_size': 128, 'learning_rate': 0.0011605356349078234, 'output_fc_num_layers': 3.0, 'output_fc_size': 43.0}\n",
      "{'batch_size': 8, 'learning_rate': 0.004893205319106337, 'output_fc_num_layers': 2.0, 'output_fc_size': 37.0}\n",
      "{'batch_size': 128, 'learning_rate': 0.006168029289852262, 'output_fc_num_layers': 2.0, 'output_fc_size': 22.0}\n",
      "{'batch_size': 32, 'learning_rate': 0.0021813016316001785, 'output_fc_num_layers': 3.0, 'output_fc_size': 72.0}\n",
      "{'batch_size': 32, 'learning_rate': 0.0029673416578216627, 'output_fc_num_layers': 3.0, 'output_fc_size': 98.0}\n",
      "{'batch_size': 32, 'learning_rate': 0.0003603016980077343, 'output_fc_num_layers': 3.0, 'output_fc_size': 94.0}\n",
      "{'batch_size': 128, 'learning_rate': 0.005307170836797155, 'output_fc_num_layers': 4.0, 'output_fc_size': 33.0}\n",
      "{'batch_size': 32, 'learning_rate': 0.007445953715142756, 'output_fc_num_layers': 4.0, 'output_fc_size': 93.0}\n",
      "{'batch_size': 16, 'learning_rate': 0.004244407027335028, 'output_fc_num_layers': 1.0, 'output_fc_size': 6.0}\n",
      "{'batch_size': 64, 'learning_rate': 0.009382463627210864, 'output_fc_num_layers': 1.0, 'output_fc_size': 16.0}\n",
      "{'batch_size': 16, 'learning_rate': 0.00824635803549801, 'output_fc_num_layers': 1.0, 'output_fc_size': 5.0}\n",
      "{'batch_size': 64, 'learning_rate': 0.008210516990466793, 'output_fc_num_layers': 1.0, 'output_fc_size': 5.0}\n",
      "{'batch_size': 16, 'learning_rate': 0.008616733565439077, 'output_fc_num_layers': 1.0, 'output_fc_size': 27.0}\n",
      "{'batch_size': 64, 'learning_rate': 0.0067493804623423165, 'output_fc_num_layers': 1.0, 'output_fc_size': 49.0}\n",
      "{'batch_size': 64, 'learning_rate': 0.001519509184670381, 'output_fc_num_layers': 1.0, 'output_fc_size': 11.0}\n",
      "{'batch_size': 16, 'learning_rate': 0.00011656851862334528, 'output_fc_num_layers': 1.0, 'output_fc_size': 13.0}\n",
      "{'batch_size': 64, 'learning_rate': 0.008356065385100586, 'output_fc_num_layers': 1.0, 'output_fc_size': 24.0}\n",
      "{'batch_size': 16, 'learning_rate': 0.009871163270730832, 'output_fc_num_layers': 1.0, 'output_fc_size': 43.0}\n",
      "100%|██████████| 30/30 [01:20<00:00,  2.67s/it, best loss: 0.3943763751557419]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "best = fmin(fn=score_model,\n",
    "    space=space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimal hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size: 16 , learning_rate: 0.00824635803549801 , fc_num_layers: 1 , fc_size: 5\n"
     ]
    }
   ],
   "source": [
    "print('batch size:', 2 ** np.int(3 + best['batch_size_exponent']),\n",
    "      ', learning_rate:', best['learning_rate'],\n",
    "      ', fc_num_layers:', np.int(best['output_fc_num_layers']),\n",
    "      ', fc_size:', np.int(best['output_fc_size']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with optimal hyperparameters on the whole data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean out old results\n",
    "try:\n",
    "    shutil.rmtree('./results')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    shutil.rmtree('./visualizations')\n",
    "except:\n",
    "    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set optimal hyperparameters for training    \n",
    "model_definition['training']['learning_rate'] = best['learning_rate']\n",
    "model_definition['training']['batch_size'] = 2** (3 + best['batch_size_exponent'])\n",
    "model_definition['output_features'][0]['num_fc_layers'] = np.int(best['output_fc_num_layers'])\n",
    "model_definition['output_features'][0]['fc_size'] = np.int(best['output_fc_size'])\n",
    "\n",
    "\n",
    "# Define Ludwig model object that drive model training\n",
    "model = LudwigModel(model_definition,\n",
    "                    logging_level=logging.WARN)\n",
    "\n",
    "# initiate model training\n",
    "train_stats = model.train(data_train_df = raw_df,\n",
    "                        experiment_name='hyperparameter_training',\n",
    "                        model_name='optimized_model',                        \n",
    "                        random_seed=42)\n",
    "\n",
    "\n",
    "\n",
    "model.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
